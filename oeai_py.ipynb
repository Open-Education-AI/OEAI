{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import re\n",
        "from delta.tables import DeltaTable\n",
        "import urllib\n",
        "from urllib.parse import urlparse\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import concat, lit, monotonically_increasing_id, expr,input_file_name, trim, lower\n",
        "from pyspark.sql.functions import coalesce, col, when, expr, format_number, avg, count, sum\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DecimalType, ArrayType\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "import warnings\n",
        "import traceback\n",
        "\n",
        "class OEAI:\n",
        "    \"\"\"\n",
        "    The Open Education AI (OEAI) class provides a suite of utilities for handling and manipulating \n",
        "    data within a lakehouse architecture. This class simplifies complex data processing tasks, \n",
        "    including handling of nested JSON structures, DataFrame schema alignment, and secure access \n",
        "    to sensitive information.\n",
        "\n",
        "    The class offers methods for:\n",
        "    - Flattening nested JSON data into a simpler tabular format.\n",
        "    - Matching and aligning DataFrame column types across different DataFrames.\n",
        "    - Securely retrieving secrets from Azure Key Vault.\n",
        "    - Adding missing columns to DataFrames and creating structures with null values.\n",
        "    - Dynamically modifying DataFrame schemas based on specified mappings.\n",
        "    - Working with date ranges and generating date chunks for batch processing.\n",
        "    - Handling nested and complex data types within DataFrames.\n",
        "\n",
        "    This class is designed to be flexible and robust, suitable for various data processing \n",
        "    and transformation needs in educational data analysis and other domains.\n",
        "\n",
        "    Usage:\n",
        "        The OEAI class is instantiated and its methods are called with the necessary \n",
        "        parameters, typically involving Spark DataFrames and other PySpark constructs.\n",
        "\n",
        "    Example:\n",
        "        ```\n",
        "        oeai = OEAI()\n",
        "        flattened_data = oeai.flatten_nested_json(json_data)\n",
        "        updated_df = oeai.match_column_types(df1, df2)\n",
        "        secret_value = oeai.get_secret(spark, \"mySecretName\", \"myKeyvaultLinkedService\", \"myKeyvault\")\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def __init__(self, timezone=\"UTC\", platform=\"Synapse\"):\n",
        "        # Initialize Spark Session\n",
        "        self.spark = SparkSession.builder.appName(\"oeaiSpark\").getOrCreate()\n",
        "        \n",
        "        # Set the timezone\n",
        "        self.spark.conf.set(\"spark.sql.session.timeZone\", timezone)\n",
        "\n",
        "        # Suppress future warnings\n",
        "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "        # Selects relevant code blocks. Defaults to \"Synapse\"\n",
        "        self.platform = platform\n",
        "    \n",
        "    def load_audit_log(self, spark, audit_log_file):\n",
        "        \"\"\"\n",
        "        Loads the audit log from a specified JSON file if it exists, otherwise returns an empty list.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): Active Spark session for file system operations.\n",
        "            audit_log_file (str): Path to the audit log file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of dictionaries representing the audit log entries.\n",
        "        \"\"\"     \n",
        "        schema = StructType([\n",
        "            StructField(\"school_id\", StringType(), True),\n",
        "            StructField(\"endpoint\", StringType(), True),\n",
        "            StructField(\"query\", StringType(), True),\n",
        "            StructField(\"start_time\", StringType(), True),\n",
        "            StructField(\"end_time\", StringType(), True),\n",
        "            StructField(\"duration\", StringType(), True),\n",
        "            StructField(\"records_returned\", StringType(), True)\n",
        "        ])\n",
        "        \n",
        "        try:\n",
        "            if self.platform == \"Fabric\":\n",
        "                if mssparkutils.fs.exists(audit_log_file):\n",
        "                    # Use this schema when reading the audit log\n",
        "                    audit_log_df = spark.read.schema(schema).json(audit_log_file)\n",
        "                    # Converting to a list of dictionaries while avoiding collecting large data sets\n",
        "                    return [row.asDict() for row in audit_log_df.collect()]\n",
        "                else:\n",
        "                    return []\n",
        "            else:\n",
        "                fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
        "                path = spark._jvm.org.apache.hadoop.fs.Path(audit_log_file)\n",
        "            \n",
        "                if fs.exists(path):\n",
        "                    # Use this schema when reading the audit log\n",
        "                    audit_log_df = spark.read.schema(schema).json(audit_log_file)\n",
        "                    # Converting to a list of dictionaries while avoiding collecting large data sets\n",
        "                    return [row.asDict() for row in audit_log_df.collect()]\n",
        "                else:\n",
        "                    return []\n",
        "        except Exception as e:\n",
        "            # Log the exception\n",
        "            print(f\"An error occurred while loading the audit log: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_audit_log(self, spark, audit_log, audit_log_file):\n",
        "        \"\"\"\n",
        "        Saves the audit log to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): Active Spark session for file operations.\n",
        "            audit_log (list): List of audit log entries, each as a dictionary.\n",
        "            audit_log_file (str): Path to save the audit log file.\n",
        "\n",
        "        Note:\n",
        "            This function overwrites the existing file at audit_log_file path.\n",
        "        \"\"\"\n",
        "        # Define the schema for the audit log DataFrame\n",
        "        schema = StructType([\n",
        "            StructField(\"school_id\", StringType(), True),\n",
        "            StructField(\"endpoint\", StringType(), True),\n",
        "            StructField(\"query\", StringType(), True),\n",
        "            StructField(\"start_time\", StringType(), True),\n",
        "            StructField(\"end_time\", StringType(), True),\n",
        "            StructField(\"duration\", StringType(), True),\n",
        "            StructField(\"records_returned\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Convert the list of dictionaries to a DataFrame\n",
        "            audit_log_df = spark.createDataFrame([Row(**record) for record in audit_log], schema)\n",
        "\n",
        "            # Write the DataFrame to a JSON file, overwriting any existing file\n",
        "            audit_log_df.write.mode(\"overwrite\").json(audit_log_file)\n",
        "        except Exception as e:\n",
        "            # Handle potential exceptions (consider using a logging framework)\n",
        "            print(f\"An error occurred while saving the audit log: {e}\")\n",
        "\n",
        "\n",
        "    def save_empty_json(self, spark, file_path):\n",
        "        \"\"\"\n",
        "        Creates and saves an empty JSON file with a predefined schema.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): Active Spark session for file operations.\n",
        "            file_path (str): The file path where the empty JSON file will be saved.\n",
        "\n",
        "        Note:\n",
        "            This function creates a JSON file with a single empty record.\n",
        "        \"\"\"\n",
        "        # Define an empty schema with at least one field\n",
        "        schema = StructType([\n",
        "            StructField(\"message\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Create an empty DataFrame with the defined schema\n",
        "            empty_df = spark.createDataFrame([], schema)\n",
        "\n",
        "            # Write the empty DataFrame to a JSON file, overwriting any existing file\n",
        "            empty_df.write.mode(\"overwrite\").json(file_path)\n",
        "        except Exception as e:\n",
        "            # Handle potential exceptions (consider using a logging framework)\n",
        "            print(f\"An error occurred while saving the empty JSON file: {e}\")\n",
        "\n",
        "\n",
        "    def parse_date(self, date_dict):\n",
        "        \"\"\"\n",
        "        Parses a date from a dictionary if the 'date' key exists.\n",
        "\n",
        "        Args:\n",
        "            date_dict (dict): A dictionary potentially containing a 'date' key.\n",
        "\n",
        "        Returns:\n",
        "            str: The value associated with the 'date' key, if it exists; otherwise, None.\n",
        "        \"\"\"\n",
        "        # Check if the input is a dictionary and contains the 'date' key\n",
        "        if isinstance(date_dict, dict) and 'date' in date_dict:\n",
        "            return date_dict['date']\n",
        "\n",
        "        # Return None if the 'date' key does not exist or the input is not a dictionary\n",
        "        return None\n",
        "\n",
        "\n",
        "    def flatten_json(self, y):\n",
        "        \"\"\"\n",
        "        Flattens a nested JSON structure into a flat dictionary. Special handling for date fields.\n",
        "\n",
        "        Args:\n",
        "            y (dict or list): The JSON object (nested dictionary or list) to be flattened.\n",
        "\n",
        "        Returns:\n",
        "            dict: A flat dictionary with all nested keys concatenated by underscores.\n",
        "        \"\"\"\n",
        "        out = {}\n",
        "\n",
        "        def flatten(x, name=''):\n",
        "            # Recursively flatten the dictionary\n",
        "            if isinstance(x, dict):\n",
        "                for a in x:\n",
        "                    # Special handling for date fields\n",
        "                    if a in ['achievement_date', 'recorded_date', 'created_at', 'updated_at']:\n",
        "                        out[f\"{name}{a}\"] = self.parse_date(x[a])\n",
        "                    else:\n",
        "                        flatten(x[a], f\"{name}{a}_\")\n",
        "            # Flatten each item in the list separately\n",
        "            elif isinstance(x, list):\n",
        "                for i, a in enumerate(x):\n",
        "                    flatten(a, f\"{name}{i}_\")\n",
        "            else:\n",
        "                # Handle the base case\n",
        "                out[name[:-1]] = x\n",
        "\n",
        "        flatten(y)\n",
        "        return out\n",
        "\n",
        "    def generate_date_chunks(self, start_date, end_date, chunk_size=timedelta(weeks=2)):\n",
        "        \"\"\"\n",
        "        Generates date ranges from start_date to end_date in specified chunk sizes.\n",
        "\n",
        "        Args:\n",
        "            start_date (datetime): The start date of the range.\n",
        "            end_date (datetime): The end date of the range.\n",
        "            chunk_size (timedelta, optional): The size of each date chunk. Defaults to two weeks.\n",
        "\n",
        "        Yields:\n",
        "            tuple: A tuple containing the start and end date of each chunk.\n",
        "        \"\"\"\n",
        "        while start_date < end_date:\n",
        "            chunk_end_date = min(start_date + chunk_size, end_date)\n",
        "            yield (start_date, chunk_end_date)\n",
        "            start_date = chunk_end_date\n",
        "\n",
        "    def update_query_with_chunks(self, original_query, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Updates a query string by replacing or adding 'updated_after' and 'updated_before' parameters \n",
        "        with the provided start and end dates.\n",
        "\n",
        "        Args:\n",
        "            original_query (str): The original query string.\n",
        "            start_date (datetime): The start date for the 'updated_after' parameter.\n",
        "            end_date (datetime): The end date for the 'updated_before' parameter.\n",
        "\n",
        "        Returns:\n",
        "            str: The updated query string.\n",
        "        \"\"\"\n",
        "        # Remove existing 'updated_after' parameter using regex\n",
        "        query_without_updated_after = re.sub(r'updated_after=[^&]*', '', original_query)\n",
        "\n",
        "        # Trim any trailing '&' characters\n",
        "        query_without_updated_after = query_without_updated_after.rstrip('&')\n",
        "\n",
        "        # Ensure the query starts correctly with '?' or '&' based on existing content\n",
        "        if query_without_updated_after and not query_without_updated_after.startswith('?'):\n",
        "            query_prefix = '&' if '?' in query_without_updated_after else '?'\n",
        "            query_without_updated_after = query_prefix + query_without_updated_after\n",
        "\n",
        "        # Format the new 'updated_after' and 'updated_before' parameters\n",
        "        formatted_start_date = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        formatted_end_date = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        chunk_query = f\"{query_without_updated_after}updated_after={formatted_start_date}&updated_before={formatted_end_date}\"\n",
        "        \n",
        "        return chunk_query    \n",
        "\n",
        "    def get_secret(self, spark, secret_name, keyvault_linked_service, keyvault):\n",
        "        \"\"\"\n",
        "        Retrieves a specified secret from Azure Key Vault.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): The SparkSession object.\n",
        "            secret_name (str): The name of the secret to retrieve.\n",
        "            keyvault_linked_service (str): The name of the Azure Synapse Analytics linked service.\n",
        "            keyvault (str): The name of the Azure Key Vault.\n",
        "\n",
        "        Returns:\n",
        "            str: The value of the retrieved secret.\n",
        "        \"\"\"\n",
        "        if self.platform == \"Fabric\":\n",
        "            value = mssparkutils.credentials.getSecret(keyvault, secret_name)\n",
        "        else:\n",
        "            # Access the TokenLibrary from Azure Synapse's Spark pool\n",
        "            token_library = spark._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "            # Retrieve the secret value\n",
        "            value = token_library.getSecret(keyvault, secret_name, keyvault_linked_service)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def row_to_dict(self, row_obj):\n",
        "        \"\"\"\n",
        "        Converts a PySpark Row object into a Python dictionary.\n",
        "\n",
        "        Args:\n",
        "            row_obj (Row): The Row object to be converted.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary representation of the Row object.\n",
        "        \"\"\"\n",
        "        # Convert the Row object to a dictionary\n",
        "        return {k: v for k, v in row_obj.asDict().items()}\n",
        "\n",
        "    def safe_get(self, dct, *keys):\n",
        "        \"\"\"\n",
        "        Safely retrieves a value from a nested dictionary using a sequence of keys.\n",
        "\n",
        "        Args:\n",
        "            dct (dict): The dictionary from which to retrieve the value.\n",
        "            *keys: A sequence of keys to traverse through the nested dictionary.\n",
        "\n",
        "        Returns:\n",
        "            The value found at the nested key path, or None if any key is missing or an error occurs.\n",
        "        \"\"\"\n",
        "        for key in keys:\n",
        "            try:\n",
        "                dct = dct[key]\n",
        "            except (TypeError, KeyError):\n",
        "                # Return None if the key is not found or if not a dictionary\n",
        "                return None\n",
        "        return dct\n",
        "\n",
        "    \n",
        "    def safe_get_or_create(self, dct, default_value, *keys):\n",
        "        \"\"\"\n",
        "        Safely retrieves or sets a value in a nested dictionary using a sequence of keys. \n",
        "        If the key path does not exist, it is created and set to a default value.\n",
        "\n",
        "        Args:\n",
        "            dct (dict): The dictionary to traverse or modify.\n",
        "            default_value: The default value to set if the key path does not exist.\n",
        "            *keys: A sequence of keys representing the path in the nested dictionary.\n",
        "\n",
        "        Returns:\n",
        "            The value found or set at the nested key path.\n",
        "        \"\"\"\n",
        "        for key in keys[:-1]:\n",
        "            # Ensure each key in the path exists and is a dictionary\n",
        "            if key not in dct or not isinstance(dct[key], dict):\n",
        "                dct[key] = {}\n",
        "            dct = dct[key]\n",
        "\n",
        "        # Set the final key to default value if it does not exist\n",
        "        final_key = keys[-1]\n",
        "        if final_key not in dct:\n",
        "            dct[final_key] = default_value\n",
        "\n",
        "        return dct[final_key]\n",
        "\n",
        "    import json\n",
        "\n",
        "    def flatten_nested_json(self, data):\n",
        "        \"\"\"\n",
        "        Flattens a nested JSON structure. Special handling is implemented for 'students' data.\n",
        "\n",
        "        Args:\n",
        "            data (str): A string representation of the JSON data.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of flattened dictionary objects.\n",
        "        \"\"\"\n",
        "        data = json.loads(data)\n",
        "        output_data = []\n",
        "\n",
        "        for item in data:\n",
        "            base_info = self._flatten_dict(item, exclude_keys=['students'])\n",
        "\n",
        "            if 'students' in item:\n",
        "                students_info = self._flatten_students_data(item['students'])\n",
        "\n",
        "                for student in students_info:\n",
        "                    # Combine base info with each student's info\n",
        "                    combined_info = {**base_info, **student}\n",
        "                    output_data.append(combined_info)\n",
        "            else:\n",
        "                # If there are no students, append base_info\n",
        "                output_data.append(base_info)\n",
        "\n",
        "        return output_data\n",
        "\n",
        "    def _flatten_dict(self, dct, exclude_keys=None, prefix=None):\n",
        "        \"\"\"\n",
        "        Flattens a dictionary, optionally excluding specified keys and adding a prefix to keys.\n",
        "\n",
        "        Args:\n",
        "            dct (dict): The dictionary to flatten.\n",
        "            exclude_keys (list, optional): Keys to exclude from flattening.\n",
        "            prefix (str, optional): A prefix to prepend to each key in the flattened dictionary.\n",
        "\n",
        "        Returns:\n",
        "            dict: A flattened dictionary.\n",
        "        \"\"\"\n",
        "        exclude_keys = exclude_keys or []\n",
        "        flattened = {}\n",
        "        for key, value in dct.items():\n",
        "            if key in exclude_keys:\n",
        "                continue\n",
        "            if isinstance(value, dict):\n",
        "                for subkey, subvalue in value.items():\n",
        "                    new_key = f'{prefix}_{subkey}' if prefix else f'{key}_{subkey}'\n",
        "                    flattened[new_key] = subvalue\n",
        "            else:\n",
        "                new_key = f'{prefix}_{key}' if prefix else key\n",
        "                flattened[new_key] = value\n",
        "        return flattened\n",
        "\n",
        "    def _flatten_students_data(self, students_data):\n",
        "        \"\"\"\n",
        "        Specifically flattens the 'students' data within the nested JSON.\n",
        "\n",
        "        Args:\n",
        "            students_data (dict): The 'students' section of the data.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of flattened student data dictionaries.\n",
        "        \"\"\"\n",
        "        flattened_students = []\n",
        "        for student_data in students_data['data']:\n",
        "            flattened_student = self._flatten_dict(student_data, prefix='student_data')\n",
        "            flattened_students.append(flattened_student)\n",
        "        return flattened_students\n",
        "\n",
        "    def apply_column_mappings(self, df, mappings):\n",
        "        \"\"\"\n",
        "        Applies various column mappings to a DataFrame such as dropping, renaming, \n",
        "        and adding columns with default values.\n",
        "\n",
        "        Args:\n",
        "            df (DataFrame): The DataFrame to be modified.\n",
        "            mappings (dict): A dictionary containing the mapping instructions. \n",
        "                             Keys are column names and values are actions or new names.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: The modified DataFrame after applying the mappings.\n",
        "        \"\"\"\n",
        "        # Drop columns\n",
        "        drop_cols = [col for col, action in mappings.items() if action == \"drop\"]\n",
        "        df = df.drop(*drop_cols)\n",
        "\n",
        "        # Rename columns\n",
        "        rename_mappings = {col: details['new_name'] for col, details in mappings.items()\n",
        "                           if isinstance(details, dict) and 'new_name' in details}\n",
        "        for old_col, new_col in rename_mappings.items():\n",
        "            df = df.withColumnRenamed(old_col, new_col)\n",
        "\n",
        "        # Add new columns with default values\n",
        "        add_columns = mappings.get(\"add_columns\", {})\n",
        "        for new_col, default_value in add_columns.items():\n",
        "            df = df.withColumn(new_col, lit(default_value))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_missing_columns(self, df, columns):\n",
        "        \"\"\"\n",
        "        Adds missing columns to the DataFrame as null columns of StringType.\n",
        "\n",
        "        Args:\n",
        "            df (DataFrame): The DataFrame to which columns will be added.\n",
        "            columns (list): A list of column names to be added if they are missing.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: The DataFrame with the missing columns added.\n",
        "        \"\"\"\n",
        "        # Special case: If the DataFrame is empty (except for 'school_id'), add all specified columns\n",
        "        if len(df.columns) == 1 and 'school_id' in df.columns:\n",
        "            missing_columns = [col for col in columns if col != 'school_id']\n",
        "        else:\n",
        "            # General case: Identify columns that are missing from the DataFrame\n",
        "            missing_columns = [col for col in columns if col not in df.columns]\n",
        "\n",
        "        # Add each missing column as a null column\n",
        "        for col in missing_columns:\n",
        "            df = df.withColumn(col, lit(None).cast(StringType()))\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_null_struct(fields):\n",
        "        \"\"\"\n",
        "        Creates a Spark SQL struct with null values for each specified field.\n",
        "\n",
        "        Args:\n",
        "            fields (list of StructField): A list of StructField objects defining the schema of the struct.\n",
        "\n",
        "        Returns:\n",
        "            Column: A Spark SQL Column representing a struct with null values for each field.\n",
        "        \"\"\"\n",
        "        return lit(None).cast(StructType(fields))\n",
        "\n",
        "    def get_uuid_column_name(self, delta_table_name):\n",
        "        \"\"\"\n",
        "        Determines the UUID column name based on the given delta table name.\n",
        "\n",
        "        Args:\n",
        "            delta_table_name (str): The name of the delta table.\n",
        "\n",
        "        Returns:\n",
        "            str: The standardized UUID column name.\n",
        "        \"\"\"\n",
        "        # Remove prefixes and convert to lowercase to get the base name\n",
        "        base_name = delta_table_name.replace(\"dim_\", \"\").replace(\"fact_\", \"\").lower()\n",
        "        \n",
        "        # Form the UUID column name by appending 'key' to the base name\n",
        "        uuid_column_name = f\"{base_name}key\"\n",
        "\n",
        "        return uuid_column_name\n",
        "\n",
        "    def match_column_types(self, df1, df2):\n",
        "        \"\"\"\n",
        "        Matches the column data types of two DataFrames.\n",
        "\n",
        "        The method iterates through the columns of df1 and updates df2 to ensure\n",
        "        matching data types, casting them as necessary. If df2 lacks a column present in df1,\n",
        "        it is added with null values of the appropriate type.\n",
        "\n",
        "        Args:\n",
        "            df1 (DataFrame): The DataFrame with the desired column types.\n",
        "            df2 (DataFrame): The DataFrame to be modified to match df1's column types.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: The updated df2 with matching column types to df1.\n",
        "        \"\"\"\n",
        "        for col_name in df1.columns:\n",
        "            if col_name in df2.columns:\n",
        "                df1_col_type = df1.schema[col_name].dataType\n",
        "                df2_col_type = df2.schema[col_name].dataType\n",
        "\n",
        "                if isinstance(df1_col_type, StructType) and isinstance(df2_col_type, StructType):\n",
        "                    # Additional logic for StructType columns\n",
        "                    pass\n",
        "                elif isinstance(df1_col_type, ArrayType) and isinstance(df2_col_type, ArrayType):\n",
        "                    # Additional logic for ArrayType columns\n",
        "                    pass\n",
        "                else:\n",
        "                    # Cast to the same type as in df1\n",
        "                    df2 = df2.withColumn(col_name, col(col_name).cast(df1_col_type))\n",
        "            else:\n",
        "                # Add missing column in df2 as nulls of the same type as in df1\n",
        "                df2 = df2.withColumn(col_name, lit(None).cast(df1.schema[col_name].dataType))\n",
        "\n",
        "        return df2\n",
        "\n",
        "    def log_error(self, spark, message, error_log_path):\n",
        "        \"\"\"\n",
        "        Logs an error message to the specified error log file in ABFS. \n",
        "        Creates the file if it doesn't exist.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): Active Spark session for file system operations.\n",
        "            message (str): The error message to log.\n",
        "            error_log_path (str): Path to the error log file in ABFS.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get the current date and time\n",
        "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # Create a new Row with the error message and timestamp\n",
        "            error_entry = Row(timestamp=current_time, message=message)\n",
        "            error_df = spark.createDataFrame([error_entry])\n",
        "\n",
        "            # Writing the error message to the error log file\n",
        "            error_df.repartition(1).write.mode(\"append\").json(error_log_path)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while logging the error: {e}\")\n",
        "\n",
        "    def load_audit_log_silver(self, spark, audit_log_file):\n",
        "        \"\"\"\n",
        "        Loads the audit log from a specified JSON file if it exists, otherwise returns an empty list.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): Active Spark session for file system operations.\n",
        "            audit_log_file (str): Path to the audit log file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of dictionaries representing the audit log entries.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
        "            path = spark._jvm.org.apache.hadoop.fs.Path(audit_log_file)\n",
        "            \n",
        "            schema = StructType([\n",
        "                StructField(\"school_id\", StringType(), True),\n",
        "                StructField(\"endpoint\", StringType(), True),\n",
        "                StructField(\"query\", StringType(), True),\n",
        "                StructField(\"start_time\", StringType(), True),\n",
        "                StructField(\"end_time\", StringType(), True),\n",
        "                StructField(\"duration\", StringType(), True),\n",
        "                StructField(\"records_returned\", StringType(), True)\n",
        "            ])\n",
        "\n",
        "            if fs.exists(path):\n",
        "                # Use this schema when reading the audit log\n",
        "                audit_log_df = spark.read.schema(schema).json(audit_log_file)\n",
        "                # Converting to a list of dictionaries while avoiding collecting large data sets\n",
        "                return [row.asDict() for row in audit_log_df.collect()]\n",
        "            else:\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            # Log the exception\n",
        "            print(f\"An error occurred while loading the audit log: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_audit_log_silver(self, spark, audit_log, audit_log_file):\n",
        "        \"\"\"\n",
        "        Saves the audit log to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            spark (SparkSession): Active Spark session for file operations.\n",
        "            audit_log (list): List of audit log entries, each as a dictionary.\n",
        "            audit_log_file (str): Path to save the audit log file.\n",
        "\n",
        "        Note:\n",
        "            This function overwrites the existing file at audit_log_file path.\n",
        "        \"\"\"\n",
        "        # Define the schema for the audit log DataFrame\n",
        "        schema = StructType([\n",
        "            StructField(\"school_id\", StringType(), True),\n",
        "            StructField(\"endpoint\", StringType(), True),\n",
        "            StructField(\"query\", StringType(), True),\n",
        "            StructField(\"start_time\", StringType(), True),\n",
        "            StructField(\"end_time\", StringType(), True),\n",
        "            StructField(\"duration\", StringType(), True),\n",
        "            StructField(\"records_returned\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Convert the list of dictionaries to a DataFrame\n",
        "            audit_log_df = spark.createDataFrame([Row(**record) for record in audit_log], schema)\n",
        "\n",
        "            # Write the DataFrame to a JSON file, overwriting any existing file\n",
        "            audit_log_df.write.mode(\"overwrite\").json(audit_log_file)\n",
        "        except Exception as e:\n",
        "            # Handle potential exceptions (consider using a logging framework)\n",
        "            print(f\"An error occurred while saving the audit log: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
