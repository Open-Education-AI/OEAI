{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\" # fully qualified for Fabric  \n",
        "keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME_HERE\" # linked service name for Synapse\n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"bromcom-bronze\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"bromcom-ids\", keyvault_linked_service, keyvault)\n",
        "school_ids = school_ids_secret.split(\",\")\n",
        "appid = oeai.get_secret(spark, \"bromcom-appid\", keyvault_linked_service, keyvault)\n",
        "token = oeai.get_secret(spark, \"bromcom-appsecret\", keyvault_linked_service, keyvault)\n",
        "\n",
        "# Set up date parameters\n",
        "today = datetime.today()\n",
        "last_year = today - timedelta(days=365)\n",
        "DateFrom = last_year.strftime('%Y-%m-%d')\n",
        "DateTo = today.strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialise the audit log\n",
        "audit_log = oeai.load_audit_log(spark, bronze_path + \"audit_log.json\")\n",
        "audit_logs = []\n",
        "error_log_path = bronze_path + \"error_log.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_school_data(endpoint: str, appid: str, token: str, school_id: str, query: str) -> dict:\n",
        "    \n",
        "    url = f\"https://api.bromcomcloud.com/{endpoint}?applicationId={appid}&applicationSecret={token}&schoolId={school_id}{query}\"\n",
        "    all_data = []\n",
        "    next_url = url\n",
        "\n",
        "    while next_url:\n",
        "        #print(next_url)\n",
        "        response = requests.get(next_url)\n",
        "        # Check if the request was successful\n",
        "        if response.status_code != 200:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            break\n",
        "\n",
        "        response_data = response.json()\n",
        "        # Check if data is a list or dictionary\n",
        "        data_from_response = response_data.get(\"data\", [])\n",
        "        \n",
        "        if isinstance(data_from_response, dict):\n",
        "            all_data.append(data_from_response)\n",
        "        else:\n",
        "            all_data.extend(data_from_response)\n",
        "        \n",
        "        # Check if there are more pages\n",
        "        next_url = response_data.get(\"meta\", {}).get(\"pagination\", {}).get(\"next\")\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_bronze(spark, endpoint: str, appid: str, token: str, school_id: str, query: str, use_date_chunk: str, limit=None, audit_log_file=\"audit_log.json\"):\n",
        "    \"\"\"\n",
        "    Loads data from an API into a Bronze layer, handling pagination, date chunking, and audit logging.\n",
        "\n",
        "    Args:\n",
        "        spark (SparkSession): Active SparkSession for DataFrame operations.\n",
        "        endpoint (str): API endpoint to retrieve data from.\n",
        "        subkey (str): Subkey for identifying the specific data.\n",
        "        school_id (str): Unique identifier for the school.\n",
        "        token (str): Authentication token for API access.\n",
        "        pagination_type (str): Type of pagination used by the API ('cursor' or 'offset').\n",
        "        limit (int, optional): Limit for the number of records to retrieve. Defaults to None.\n",
        "        query (str, optional): Additional query parameters for the API call. Defaults to None.\n",
        "        audit_log_file (str, optional): Filename for the audit log. Defaults to \"audit_log.json\".\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: A PySpark DataFrame with the loaded data.\n",
        "    \"\"\"\n",
        "    global audit_log\n",
        "    df = pd.DataFrame()\n",
        "    data_list = []  # Default empty list\n",
        "    full_data_list = []  # Default empty list\n",
        "    \n",
        "    # Calculate the current time and the time from which we should update\n",
        "    now = datetime.now()\n",
        "    last_updated_time = \"2023-08-01 00:00:00\"\n",
        "    if isinstance(last_updated_time, str):\n",
        "        last_updated_time = datetime.strptime(last_updated_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "    else:\n",
        "        last_updated_time = last_updated_str\n",
        "\n",
        "\n",
        "    # Use 'safe_get' to retrieve the last updated date for the specific school_id and endpoint (subkey)\n",
        "    #print(\"about to call safe_get(LastUpdated, school_id, subkey) as: \", LastUpdated, \" \", school_id, \" \", subkey)\n",
        "    #last_updated_str = oeai.safe_get_or_create(LastUpdated, \"2018-09-01 00:00:00\", school_id, subkey)\n",
        "\n",
        "    # if \n",
        "    if use_date_chunk:\n",
        "            for start_date, end_date in oeai.generate_date_chunks(last_updated_time, now, chunk_size=timedelta(weeks=1)):\n",
        "                chunk_query = oeai.update_query_with_chunks_bromcom(query, start_date, end_date)\n",
        "                r = get_school_data(endpoint, appid, token, school_id, chunk_query)\n",
        "                #print(chunk_query)\n",
        "                # Check if the response is not None and not empty before processing\n",
        "                if r:\n",
        "                    if isinstance(r, dict) and 'data' in r:\n",
        "                        data_list.append(r['data'])\n",
        "                    elif isinstance(r, list):\n",
        "                        data_list.extend(r)\n",
        "                else:\n",
        "                    error_message = f\"Empty response, not adding to data_list: {traceback.format_exc()}\"\n",
        "                    oeai.log_error(spark, error_message, error_log_path)\n",
        "    else:\n",
        "        r = get_school_data(endpoint, appid, token, school_id, query)\n",
        "        # Ensure the data is always a list\n",
        "        if isinstance(r, dict) and 'data' in r:\n",
        "            data_list = [r['data']]\n",
        "        elif isinstance(r, list):\n",
        "            data_list = r\n",
        "\n",
        "    # Construct the directory path\n",
        "    school_folder = os.path.join(bronze_path, school_id)\n",
        "\n",
        "    # Check and create directory if it doesn't exist\n",
        "    if not os.path.exists(school_folder):\n",
        "        os.makedirs(school_folder)\n",
        "\n",
        "    # Calculate the duration of the API call\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    if not data_list:\n",
        "        oeai.save_empty_json(spark, school_folder + \"/\" + endpoint + \".json\")\n",
        "        #print(\"Just saved empty json\")\n",
        "    else:\n",
        "        try:\n",
        "            # Update the last called date in the JSON file\n",
        "            #LastUpdated[subkey] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            r_df = pd.DataFrame.from_records(data_list)\n",
        "            flattened_data_list = [oeai.flatten_json(item) for item in data_list]\n",
        "\n",
        "            # Convert the list of dictionaries to a Pandas DataFrame\n",
        "            pandas_df = pd.DataFrame(flattened_data_list)\n",
        "\n",
        "            # Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "            r_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "            # Add school_id and unique_key to the DataFrame\n",
        "            r_df = r_df.withColumn(\"school_id\", lit(school_id))\n",
        "            if \"studentID\" in r_df.columns:\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(school_id),r_df[\"studentID\"].cast(\"string\").cast(\"string\")))\n",
        "            else:\n",
        "                r_df = r_df.withColumn(\"unique_key\", lit(school_id).cast(\"string\"))\n",
        "\n",
        "            # Save the DataFrame to a JSON file\n",
        "            r_df.write.mode(\"overwrite\").json(school_folder + \"/\" + endpoint + \".json\")\n",
        "\n",
        "            #with open(bronze_path+'last_run.json', 'w') as f:\n",
        "            #    json.dump(LastUpdated, f)\n",
        "            \n",
        "        # if the key doesn't exist, skip it    \n",
        "        except Exception as e:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            pass\n",
        "\n",
        "    # Update the audit log\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    duration_str = str(duration)\n",
        "    audit_data = {\n",
        "        \"school_id\": school_id,\n",
        "        \"endpoint\": endpoint,\n",
        "        \"query\": query,\n",
        "        \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"duration\": duration_str,\n",
        "        \"records_returned\": str(len(data_list)),\n",
        "    }\n",
        "    audit_log.append(audit_data)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "  BRONZE PROCESS\n",
        "'''\n",
        "# introduce a limit for testing or leave as None for Live\n",
        "Limit = None\n",
        "query = \"\"\n",
        "# Get today's date and format it as 'YYYY-MM-DDT00:00:00'\n",
        "datetoday = datetime.now().strftime('%Y-%m-%dT00:00:00')\n",
        "\n",
        "# Set up the daily Jobs list with required endpoints\n",
        "for school_id in school_ids:\n",
        "    daily_jobs = [\n",
        "        (\"Schools\", appid, token, school_id, \"&entityFilter=schoolID=1\", False),\n",
        "        (\"Students\", appid, token, school_id, query, False),\n",
        "        (\"StudentFlatView\", appid, token, school_id, query, False),\n",
        "        (\"AttendanceSessions\", appid, token, school_id, \"&entityFilter=year=2023\", False),\n",
        "        (\"CalendarModels\", appid, token, school_id, query, False),\n",
        "        #(\"Attendances\", appid, token, school_id, query, True),\n",
        "        #(\"Attendances\", appid, token, school_id, \"&entityFilter=calendarStartDate>'2023-08-01T00:00:00'\"),       \n",
        "        ]\n",
        "\n",
        "    # call load bronze for each of the daily jobs\n",
        "    for job in daily_jobs:\n",
        "        load_bronze(spark, job[0], job[1], job[2], job[3], job[4], job[5])\n",
        "\n",
        "    # Save the audit log\n",
        "    oeai.save_audit_log(spark, audit_log, bronze_path + \"audit_log.json\")\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
