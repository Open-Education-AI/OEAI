{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-01-14T14:38:07.4960367Z",
              "execution_start_time": "2024-01-14T14:38:07.4958159Z",
              "livy_statement_state": "available",
              "parent_msg_id": "1b0fd94c-b0cc-4fda-9f0d-e4894d8f32eb",
              "queued_time": "2024-01-14T14:37:29.3102157Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, , -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-01-14T14:38:09.8516491Z",
              "execution_start_time": "2024-01-14T14:38:09.2759352Z",
              "livy_statement_state": "available",
              "parent_msg_id": "b72497c0-da21-4cd0-b889-563b26b2e98d",
              "queued_time": "2024-01-14T14:38:09.1443514Z",
              "session_id": "44",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 44, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_KV_NAME\"  \n",
        "keyvault_linked_service = \"INSERT_LS_NAME\"  \n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"bromcom-bronze\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"bromcom-ids\", keyvault_linked_service, keyvault)\n",
        "school_ids = school_ids_secret.split(\",\")\n",
        "appid = oeai.get_secret(spark, \"bromcom-appid\", keyvault_linked_service, keyvault)\n",
        "token = oeai.get_secret(spark, \"bromcom-appsecret\", keyvault_linked_service, keyvault)\n",
        "\n",
        "# Set up date parameters\n",
        "today = datetime.today()\n",
        "last_year = today - timedelta(days=365)\n",
        "DateFrom = last_year.strftime('%Y-%m-%d')\n",
        "DateTo = today.strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-01-14T14:38:17.9152493Z",
              "execution_start_time": "2024-01-14T14:38:10.8432395Z",
              "livy_statement_state": "available",
              "parent_msg_id": "e901e263-f937-464c-8fe1-4ab8f0a9deea",
              "queued_time": "2024-01-14T14:38:10.7120388Z",
              "session_id": "44",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 44, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# initialise the audit log\n",
        "audit_log = oeai.load_audit_log(spark, bronze_path + \"audit_log.json\")\n",
        "audit_logs = []\n",
        "error_log_path = bronze_path + \"error_log.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-01-14T14:38:18.2448066Z",
              "execution_start_time": "2024-01-14T14:38:18.0602482Z",
              "livy_statement_state": "available",
              "parent_msg_id": "87797486-8baf-4e78-9dca-0b7cc08d499a",
              "queued_time": "2024-01-14T14:38:13.301229Z",
              "session_id": "44",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 44, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_school_data(endpoint: str, appid: str, token: str, school_id: str, query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves school-related data from a specified API endpoint.\n",
        "\n",
        "    This function makes HTTP GET requests to a given API endpoint to fetch data related to a specific school. It handles pagination by iterating through pages of data until no further pages are available. If an error occurs during a request, it logs the error and stops fetching data.\n",
        "\n",
        "    Parameters:\n",
        "    endpoint (str): The API endpoint to make the request to. Example: 'v1/schooldata'.\n",
        "    appid (str): The application ID used for API authentication.\n",
        "    token (str): The token used for API authentication.\n",
        "    school_id (str): The unique identifier of the school for which data is being fetched.\n",
        "    query (str): Additional query parameters to be appended to the URL. Example: '&year=2021'.\n",
        "\n",
        "    Returns:\n",
        "    dict: A list of dictionaries, each dictionary containing data for one page of results. \n",
        "          The structure of each dictionary depends on the API's response format.\n",
        "\n",
        "    Raises:\n",
        "    Logs an error message if the request returns a status code other than 200.\n",
        "\n",
        "    Example usage:\n",
        "    data = get_school_data('v1/schooldata', 'app123', 'token456', 'school789', '&year=2021')\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"https://api.bromcomcloud.com/{endpoint}?applicationId={appid}&applicationSecret={token}&schoolId={school_id}{query}\"\n",
        "    all_data = []\n",
        "    next_url = url\n",
        "\n",
        "    while next_url:\n",
        "        response = requests.get(next_url)\n",
        "        if response.status_code != 200:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            break\n",
        "\n",
        "        response_data = response.json()\n",
        "        data_from_response = response_data.get(\"data\", [])\n",
        "        \n",
        "        if isinstance(data_from_response, dict):\n",
        "            all_data.append(data_from_response)\n",
        "        else:\n",
        "            all_data.extend(data_from_response)\n",
        "        \n",
        "        next_url = response_data.get(\"meta\", {}).get(\"pagination\", {}).get(\"next\")\n",
        "\n",
        "    return all_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-01-14T14:38:18.5787271Z",
              "execution_start_time": "2024-01-14T14:38:18.3908288Z",
              "livy_statement_state": "available",
              "parent_msg_id": "2a12a500-afb0-4b4d-ba59-ba59e8378ae4",
              "queued_time": "2024-01-14T14:38:15.4174042Z",
              "session_id": "44",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 44, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def load_bronze(spark, endpoint: str, appid: str, token: str, school_id: str, query: str, limit=None, audit_log_file=\"audit_log.json\"):\n",
        "    \"\"\"\n",
        "    Loads data from a specified API endpoint into a Bronze layer in a data lake architecture, handling API pagination and audit logging.\n",
        "\n",
        "    This function fetches data using the 'get_school_data' function, then processes and stores the data in a JSON format. It also handles logging for audit purposes, including start and end times, duration, and the number of records returned.\n",
        "\n",
        "    Args:\n",
        "        spark (SparkSession): The active SparkSession for DataFrame operations.\n",
        "        endpoint (str): The API endpoint to retrieve data from.\n",
        "        appid (str): The application ID for API authentication.\n",
        "        token (str): The token used for API authentication.\n",
        "        school_id (str): The unique identifier of the school for which data is being fetched.\n",
        "        query (str): Additional query parameters to be appended to the API call.\n",
        "        limit (int, optional): The limit for the number of records to retrieve. Defaults to None.\n",
        "        audit_log_file (str, optional): The filename for the audit log. Defaults to \"audit_log.json\".\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A PySpark DataFrame with the loaded data, although the function currently returns an empty DataFrame.\n",
        "\n",
        "    Note:\n",
        "        The function currently does not use the 'limit' and 'audit_log_file' arguments. \n",
        "        Additionally, the final returned DataFrame is empty and does not contain the fetched data.\n",
        "\n",
        "    Example usage:\n",
        "        df = load_bronze(spark, 'v1/schooldata', 'app123', 'token456', 'school789', '&year=2021')\n",
        "    \"\"\"\n",
        "    global audit_log\n",
        "    df = pd.DataFrame()\n",
        "    data_list = []  \n",
        "    full_data_list = []  \n",
        "    \n",
        "    # Calculate the current time and the time from which we should update\n",
        "    now = datetime.now()\n",
        "\n",
        "    r = get_school_data(endpoint, appid, token, school_id, query)\n",
        "    # Ensure the data is always a list\n",
        "    if isinstance(r, dict) and 'data' in r:\n",
        "        data_list = [r['data']]\n",
        "    elif isinstance(r, list):\n",
        "        data_list = r\n",
        "\n",
        "    # Construct the directory path\n",
        "    school_folder = os.path.join(bronze_path, school_id)\n",
        "\n",
        "    # Check and create directory if it doesn't exist\n",
        "    if not os.path.exists(school_folder):\n",
        "        os.makedirs(school_folder)\n",
        "\n",
        "    # Calculate the duration of the API call\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    if not data_list:\n",
        "        oeai.save_empty_json(spark, school_folder + \"/\" + endpoint + \".json\")\n",
        "    else:\n",
        "        try:\n",
        "            r_df = pd.DataFrame.from_records(data_list)\n",
        "            flattened_data_list = [oeai.flatten_json(item) for item in data_list]\n",
        "\n",
        "            # Convert the list of dictionaries to a Pandas DataFrame\n",
        "            pandas_df = pd.DataFrame(flattened_data_list)\n",
        "\n",
        "            # Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "            r_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "            # Add school_id and unique_key to the DataFrame\n",
        "            r_df = r_df.withColumn(\"school_id\", lit(school_id))\n",
        "            if \"studentID\" in r_df.columns:\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(school_id),r_df[\"studentID\"].cast(\"string\").cast(\"string\")))\n",
        "            else:\n",
        "                r_df = r_df.withColumn(\"unique_key\", lit(school_id).cast(\"string\"))\n",
        "\n",
        "            # Save the DataFrame to a JSON file\n",
        "            r_df.write.mode(\"overwrite\").json(school_folder + \"/\" + endpoint + \".json\")\n",
        "            \n",
        "        # if the key doesn't exist, skip it    \n",
        "        except Exception as e:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            pass\n",
        "\n",
        "    # Update the audit log\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    duration_str = str(duration)\n",
        "    audit_data = {\n",
        "        \"school_id\": school_id,\n",
        "        \"endpoint\": endpoint,\n",
        "        \"query\": query,\n",
        "        \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"duration\": duration_str,\n",
        "        \"records_returned\": str(len(data_list)),\n",
        "    }\n",
        "    audit_log.append(audit_data)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2024-01-14T14:38:48.2566044Z",
              "execution_start_time": "2024-01-14T14:38:22.7638442Z",
              "livy_statement_state": "available",
              "parent_msg_id": "1f26c7cf-7583-4624-9e64-56f1a2fd4d54",
              "queued_time": "2024-01-14T14:38:22.6304048Z",
              "session_id": "44",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 44, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''\n",
        "  BRONZE PROCESS\n",
        "'''\n",
        "# This section of code is dedicated to the 'Bronze Process' in a data pipeline.\n",
        "# It is responsible for fetching and storing raw data from various endpoints.\n",
        "\n",
        "# Initialize an empty query string\n",
        "query = \"\"\n",
        "\n",
        "# Get the current date and format it as 'YYYY-MM-DDT00:00:00'.\n",
        "# This formatted date can be used in query parameters where date filters are required.\n",
        "datetoday = datetime.now().strftime('%Y-%m-%dT00:00:00')\n",
        "\n",
        "# Loop over each school ID to set up and process daily jobs.\n",
        "# 'daily_jobs' is a list of tuples, each representing an API endpoint and its associated parameters.\n",
        "for school_id in school_ids:\n",
        "    daily_jobs = [\n",
        "        # Each tuple contains the endpoint, appid, token, school_id, and query string.\n",
        "        # The query strings are set up to filter the data as per the requirements.\n",
        "        (\"Schools\", appid, token, school_id, \"&entityFilter=schoolID=1\"),\n",
        "        (\"Students\", appid, token, school_id, query),\n",
        "        (\"StudentFlatView\", appid, token, school_id, query),\n",
        "        (\"AttendanceSessions\", appid, token, school_id, \"&entityFilter=year=2023\"),\n",
        "        (\"CalendarModels\", appid, token, school_id, query),\n",
        "        (\"Attendances\", appid, token, school_id, \"&entityFilter=calendarStartDate>'2023-09-01T00:00:00' AND calendarStartDate<'2024-01-13T00:00:00' AND calendarName='PM'\"),\n",
        "        ]\n",
        "\n",
        "    # Process each job in 'daily_jobs' using the 'load_bronze' function.\n",
        "    # This function will load and process the data from each endpoint.\n",
        "    for job in daily_jobs:\n",
        "        load_bronze(spark, job[0], job[1], job[2], job[3], job[4])\n",
        "\n",
        "    # Save the updated audit log to a JSON file at the specified path.\n",
        "    # This log tracks the execution and outcomes of the data loading processes.\n",
        "    oeai.save_audit_log(spark, audit_log, bronze_path + \"audit_log.json\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
