{"cells":[{"cell_type":"code","execution_count":null,"id":"4c343a90-3647-4aa9-a3f4-b4a5e4a85c15","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["%run oeai_py"]},{"cell_type":"code","execution_count":null,"id":"5aaf45ea-cafe-4782-bef5-0e162e58647f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n","oeai = OEAI(\"Fabric\")"]},{"cell_type":"code","execution_count":null,"id":"6afb802d-e32b-4a6b-98b5-1d83aeaca24e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# CHANGE VALUES FOR YOUR KEY VAULT\n","keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\" # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n","keyvault_linked_service = \"INSERT_YOUR_LINKED_SERVICE_NAME_HERE\"  # Not required for Fabric.\n","\n","# Synapse OEA environment path & secrets\n","bronze_path = oeai.get_secret(spark, \"msgraph-rp-bronze\", keyvault_linked_service, keyvault)\n","msgraph_tenantid = oeai.get_secret(spark, \"msgraph-tenantid\", keyvault_linked_service, keyvault)\n","msgraph_clientid = oeai.get_secret(spark, \"msgraph-clientid\", keyvault_linked_service, keyvault)\n","msgraph_secret = oeai.get_secret(spark, \"msgraph-secret\", keyvault_linked_service, keyvault)"]},{"cell_type":"code","execution_count":null,"id":"a823752f-eea7-4f78-95dd-f11330999007","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import functions as F\n","# Get today's date\n","today = spark.sql(\"SELECT current_date() AS today\").collect()[0][\"today\"]\n","\n","# Get yesterday's date\n","yesterday = spark.sql(\"SELECT date_sub(current_date(), 1) AS yesterday\").collect()[0][\"yesterday\"]\n","\n","# Output today's and yesterday's dates\n","print(\"Today's Date:\", today)\n","print(\"Yesterday's Date:\", yesterday)"]},{"cell_type":"code","execution_count":null,"id":"aa6db091-144b-47dc-81a6-22e4ac496b8f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from datetime import datetime, timedelta\n","from pyspark.sql import Row"]},{"cell_type":"code","execution_count":null,"id":"11387c6c-bf0d-4341-841e-177a4de6c06c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# initialise the audit log\n","audit_log = oeai.load_audit_log(spark, bronze_path + \"audit_log.json\")\n","audit_logs = []\n","error_log_path = bronze_path + \"error_log.txt\""]},{"cell_type":"code","execution_count":null,"id":"0b882ca4-62ea-4e60-851e-7af878d413d4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# URL for obtaining the bearer token\n","token_url = f\"https://login.windows.net/{msgraph_tenantid}/oauth2/v2.0/token\"\n","\n","# Prepare the body for the token request\n","token_body = {\n","    'grant_type': 'client_credentials',\n","    'client_id': msgraph_clientid,\n","    'client_secret': msgraph_secret,\n","    'scope': 'https://graph.microsoft.com/.default'\n","}"]},{"cell_type":"code","execution_count":null,"id":"b5f53991-daab-4c95-8b8e-cdf9b87e585a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Function to call the API for a token\n","def get_bearer_token(token_url, body):\n","    try:\n","        response = requests.post(token_url, data=body)\n","        # If the response was successful, no Exception will be raised\n","        response.raise_for_status()\n","        print(\"Token request successful.\")\n","        return response.json()['access_token']\n","    except Exception as e:\n","        print(f\"Error obtaining token: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"id":"e0e12b8e-a577-4214-828f-55606a243ea1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import time\n","import requests\n","\n","def call_api_with_token(url, params, token):\n","    headers = {\n","        'Authorization': f'Bearer {token}',\n","        'Content-Type': 'application/json',\n","        'Accept': 'application/json',\n","    }\n","    all_data = []  # Store all records from all pages\n","    while url:  # Loop as long as there's a URL to call\n","        try:\n","            print(f\"Making API call to: {url}\")\n","            print(f\"With params: {params}\")\n","            response = requests.get(url, params=params, headers=headers)\n","            print(f\"Response Status: {response.status_code} - {response.reason}\")\n","            if response.status_code != 200:\n","                print(f\"Non-success response received: {response.text}\")\n","                return None\n","\n","            data = response.json()\n","            if 'value' in data:\n","                all_data.extend(data['value'])  # Append the current page of records\n","                print(f\"Records received: {len(data['value'])}\")\n","            else:\n","                print(\"No 'value' key in response.\")\n","                break\n","\n","            # Handle pagination\n","            url = data.get('@odata.nextLink')  # Update the URL for the next call\n","            if url:\n","                print(f\"Pagination link found: {url}\")\n","                params = {}  # Clear params because nextLink includes required parameters\n","            else:\n","                print(\"No pagination link found.\")\n","        except Exception as e:\n","            print(f\"Error in API call: {e}\")\n","            return None\n","    print(\"API call successful. Total records received: \", len(all_data))\n","    return {'value': all_data}\n","\n"]},{"cell_type":"code","execution_count":null,"id":"082a942e-cbc3-472f-a07f-da14927ba837","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def fetch_assignments_for_class(class_id, gt_date, access_token, spark, max_records=None):\n","    assignments_api_url = f\"https://graph.microsoft.com/v1.0/education/classes/{class_id}/assignments\"\n","    return fetch_graph_data(gt_date, assignments_api_url, access_token, spark, {}, max_records)"]},{"cell_type":"code","execution_count":null,"id":"1ca5f574-290f-4044-bef8-910d33035cf5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def fetch_graph_data(gt_date, api_url, access_token, spark, params, max_records=None):\n","    df = None\n","    items = []\n","\n","    while api_url and (max_records is None or len(items) < max_records):\n","        response = call_api_with_token(api_url, params, access_token)\n","\n","        if response and 'value' in response:\n","            items.extend(response['value'])\n","\n","            if max_records is not None and len(items) >= max_records:\n","                items = items[:max_records]\n","                break\n","\n","            api_url = response.get('@odata.nextLink', None)\n","            params = {}  # Clear params as the nextLink URL will have them\n","\n","    if items:\n","        schema_fields = [StructField(k, StringType(), True) for k in items[0].keys()]\n","        schema = StructType(schema_fields)\n","        \n","        rdd = spark.sparkContext.parallelize(items)\n","        row_rdd = rdd.map(lambda x: Row(**{k: str(v) if v is not None else None for k, v in x.items()}))\n","        \n","        df = spark.createDataFrame(row_rdd, schema=schema)\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"id":"069f3f56-de40-4bd7-b243-623567617b7f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Function to call the API for each day within the date range and aggregate results\n","def fetch_rp_data_for_date_range(start_date, end_date, api_url, access_token, spark):\n","    current_date = start_date\n","    aggregated_df = None\n","    \n","    while current_date <= end_date:\n","        date_filter = current_date.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n","        params = {\n","            \"$filter\": f\"submissionDateTime gt {date_filter}\"\n","        }\n","        \n","        # Make the API call\n","        daily_response = call_api_with_token(api_url, params, access_token)\n","        \n","        # Process the response and create a DataFrame\n","        if daily_response and 'value' in daily_response:\n","            daily_items = daily_response['value']\n","            if daily_items:\n","                daily_df = spark.createDataFrame([Row(**item) for item in daily_items])\n","                if aggregated_df:\n","                    aggregated_df = aggregated_df.union(daily_df)\n","                else:\n","                    aggregated_df = daily_df\n","        \n","        current_date += timedelta(days=1)\n","    \n","    return aggregated_df\n"]},{"cell_type":"code","execution_count":null,"id":"90e2cefc-4e1b-4d76-9f3f-df0ad52f4aed","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def execute_api_calls(api_endpoints_with_params, gt_date, token_url, token_body, spark, bronze_path, max_records):\n","    access_token = get_bearer_token(token_url, token_body)\n","    all_assignments_df = None  # Initialize a DataFrame to hold all assignments\n","\n","    for endpoint_name, (api_url, params) in api_endpoints_with_params.items():\n","        if endpoint_name == \"classes\":\n","            print(f\"Fetching data for: {endpoint_name}\")\n","            classes_df = fetch_graph_data(gt_date, api_url, access_token, spark, params, max_records)\n","            \n","            if classes_df:\n","                classes_path = f\"{bronze_path}/{endpoint_name}/\"\n","                classes_df.coalesce(1).write.mode('overwrite').json(classes_path)\n","                print(f\"Class data saved to: {classes_path}\")\n","\n","                # Extract class IDs to fetch assignments\n","                class_ids = [row.id for row in classes_df.collect()]\n","                \n","                for class_id in class_ids:\n","                    print(f\"Fetching assignments for class ID: {class_id}\")\n","                    assignments_df = fetch_assignments_for_class(class_id, gt_date, access_token, spark, max_records)\n","                    \n","                    # Aggregate assignments into a single DataFrame\n","                    if assignments_df:\n","                        if all_assignments_df is None:\n","                            all_assignments_df = assignments_df\n","                        else:\n","                            all_assignments_df = all_assignments_df.union(assignments_df)\n","        else:\n","            # Generic handling for other endpoints\n","            print(f\"Fetching data for: {endpoint_name}\")\n","            df = fetch_graph_data(gt_date, api_url, access_token, spark, params, max_records)\n","            if df:\n","                path = f\"{bronze_path}/{endpoint_name}/\"\n","                df.coalesce(1).write.mode('overwrite').json(path)\n","                print(f\"Data for {endpoint_name} saved to: {path}\")\n","    \n","    # Save the aggregated assignments DataFrame to a single JSON file\n","    if all_assignments_df:\n","        assignments_path = f\"{bronze_path}/assignments/\"\n","        all_assignments_df.coalesce(1).write.mode('overwrite').json(assignments_path)\n","        print(f\"All assignments data saved to: {assignments_path}\")\n"]},{"cell_type":"code","execution_count":null,"id":"58efdbb9-cf2f-41cf-a6c3-eb3dabae7d73","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["gt_date = datetime.strptime(\"2018-09-01\", \"%Y-%m-%d\").date()\n","max_records = 15000\n","\n","date_filter = gt_date.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n","\n","api_endpoints_with_params = {\n","    #\"assignments\": (\"https://graph.microsoft.com/v1.0/education/assignments\", {\"$filter\": f\"createdDateTime ge {date_filter}\"}),\n","    #Response Status: 400 - Bad Request\n","    #Non-success response received: {\"error\":{\"code\":\"BadRequest\",\"message\":\"Resource not found for the segment 'assignments'.\",\"innerError\":{\"date\":\"2024-04-08T14:39:55\",\"request-id\":\"450714eb-9239-41fa-af6f-fcb7d44d0571\",\"client-request-id\":\"450714eb-9239-41fa-af6f-fcb7d44d0571\"}}}\n","    \"schools\": (\"https://graph.microsoft.com/v1.0/education/schools\", {}),\n","    \"users\": (\"https://graph.microsoft.com/v1.0/users\", {\"$filter\": f\"createdDateTime ge {date_filter}\"}),    \n","    \"classes\": (\"https://graph.microsoft.com/v1.0/education/classes\", {}),\n","    #\"reflect\": (\"https://graph.microsoft.com/beta/education/reports/reflectCheckInResponses\", {}),\n","}\n","\n","#  Assignment has a lastModifiedDateTime  \n","\n","execute_api_calls(api_endpoints_with_params, gt_date, token_url, token_body, spark, bronze_path, max_records)"]},{"cell_type":"code","execution_count":null,"id":"d3b72e19-0ee9-40bf-a929-92775498687a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["access_token = get_bearer_token(token_url, token_body)\n","# Define the date range\n","start_date = datetime.strptime(\"2024-04-01\", \"%Y-%m-%d\").date()\n","end_date = yesterday #datetime.strptime(\"yesterday\", \"%Y-%m-%d\").date()\n","\n","# API endpoint\n","api_url = \"https://graph.microsoft.com/beta/education/reports/readingassignmentsubmissions\"\n","\n","# Fetch data for the date range\n","df = fetch_rp_data_for_date_range(start_date, end_date, api_url, access_token, spark)\n","\n","# Save the DataFrame to a JSON file\n","if df:\n","    path = f\"{bronze_path}/readingprogress/\"\n","    df.coalesce(1).write.mode(\"overwrite\").json(path)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.9"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
