{"cells":[{"cell_type":"code","execution_count":null,"id":"48ac0692-9b40-4471-a764-03ffb9a36db2","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["%run oeai_py"]},{"cell_type":"code","execution_count":null,"id":"20c40bd1-edd2-4448-be3b-9036ae85f422","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n","oeai = OEAI(\"Fabric\")"]},{"cell_type":"code","execution_count":null,"id":"eeae86f6-38de-4773-a932-b706302648b8","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# CHANGE VALUES FOR YOUR KEY VAULT\n","keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\" # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n","keyvault_linked_service = \"INSERT_YOUR_LINKED_SERVICE_NAME_HERE\"  # Not required for Fabric.\n","\n","# Synapse OEA environment paths\n","bronze_path = oeai.get_secret(spark, \"msgraph-rp-bronze\", keyvault_linked_service, keyvault)\n","silver_path = oeai.get_secret(spark, \"msgraph-silver\", keyvault_linked_service, keyvault)\n","wonde_silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)"]},{"cell_type":"code","execution_count":null,"id":"e1e6eb92-0f9a-4dd5-a8da-072c5cdca86b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Define the mapping between JSON files and desired Delta table names\n","delta_table_name_mapping = {\n","    \"readingprogress\": \"fact_graph_ReadingProgress\",\n","    \"users\": \"dim_graph_User\",\n","    \"classes\": \"dim_graph_Class\",\n","    \"assignments\": \"dim_graph_Assignment\",\n","}"]},{"cell_type":"code","execution_count":null,"id":"55eddb4d-6652-4859-9e48-74d9cca3a0a2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["column_mappings = {\n","    \"readingprogress\": {\n","        # drops\n","        \"\": \"drop\", \n","        # Renames\n","        #\"id\": {\"new_name\": \"external_id\"}, \n","        # adds\n","        \"add_columns\": {\n","            \"organisationkey\": \"\",  \n","            \"studentkey\": \"\",  \n","            \"readingprogresskey\": \"\",  \n","            \"external_id\": \"\",\n","        }\n","    },\n","    \"users\": {\n","        # drops\n","        \"\": \"drop\", \n","        # Renames\n","        \"id\": {\"new_name\": \"external_id\"}, \n","        # adds\n","        \"add_columns\": {\n","            \"organisationkey\": \"\",  \n","            \"studentkey\": \"\",  \n","            \"userkey\": \"\",  \n","        }\n","    },\n","    \"classes\": {\n","        # drops\n","        \"\": \"drop\", \n","        # Renames\n","        \"id\": {\"new_name\": \"external_id\"}, \n","        # adds\n","        \"add_columns\": {\n","            \"organisationkey\": \"\",  \n","            \"classkey\": \"\",  \n","        }\n","    },\n","    \"assignments\": {\n","        # drops\n","        \"\": \"drop\", \n","        # Renames\n","        \"id\": {\"new_name\": \"external_id\"}, \n","        # adds\n","        \"add_columns\": {\n","            \"organisationkey\": \"\",  \n","            \"assignmentkey\": \"\",  \n","        }\n","    },\n","}"]},{"cell_type":"code","execution_count":null,"id":"09b0b325-505d-4b2d-b4b9-1b1f3c64bcdc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Dictionary to hold dataframes for each json file\n","json_dfs = {}\n","temp_dfs = {}\n","all_columns = {}\n","'''\n","    This code is to loop through each directory and compile all the individual schools jsons into\n","    a single json per endpoint.\n","\n","    It creates json_dfs - a dictionary of the aggregated json files\n","'''\n","\n","json_dirs = list(delta_table_name_mapping.keys())\n","\n","    #print(list(delta_table_name_mapping.keys()))\n","for json_dir in json_dirs:\n","    json_dir_path = f\"{bronze_path}{json_dir}/\"\n","    try:\n","        temp_df = spark.read.json(json_dir_path)\n","                \n","        # Update the set of columns for the json_dir\n","        all_columns.setdefault(json_dir, set()).update(temp_df.columns)\n","\n","        # Check if json_dir already exists in temp_dfs dictionary\n","        if json_dir in temp_dfs:\n","            # Align the schema of temp_df with existing DataFrame in temp_dfs\n","            existing_columns = all_columns[json_dir]\n","            temp_df = oeai.add_missing_columns(temp_df, existing_columns)\n","            existing_df = oeai.add_missing_columns(temp_dfs[json_dir], temp_df.columns)\n","            # Perform the union operation\n","            try:\n","                temp_df = oeai.match_column_types(existing_df, temp_df)\n","                temp_dfs[json_dir] = existing_df[sorted(existing_df.columns)].unionByName(temp_df[sorted(temp_df.columns)])\n","            except Exception as e:\n","                print(\"An unexpected error occurred:\", e)\n","        else:\n","            # If not, simply assign temp_df to temp_dfs[json_dir]\n","            temp_dfs[json_dir] = temp_df\n","    except AnalysisException as e:\n","        print(f\"Path does not exist: {json_dir_path}, skipping...\")\n","        continue\n","    except Exception as e:\n","        print(f\"An unexpected error occurred while processing {json_dir_path}: {e}\")\n","        continue\n","# Assign the final json_dfs outside the loops\n","json_dfs = temp_dfs"]},{"cell_type":"code","execution_count":null,"id":"9ac876bf-537f-4a90-9e07-a797ab974a7c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def apply_column_mappings(df, mappings):\n","        \"\"\"\n","        Applies various column mappings to a DataFrame such as dropping, renaming, \n","        and adding columns with default values.\n","\n","        Args:\n","            df (DataFrame): The DataFrame to be modified.\n","            mappings (dict): A dictionary containing the mapping instructions. \n","                             Keys are column names and values are actions or new names.\n","\n","        Returns:\n","            DataFrame: The modified DataFrame after applying the mappings.\n","        \"\"\"\n","        # Drop columns\n","        drop_cols = [col for col, action in mappings.items() if action == \"drop\"]\n","        df = df.drop(*drop_cols)\n","\n","        # Rename columns or add new ones if they don't exist\n","        #print(\"Existing columns before renaming:\", df.columns)\n","        rename_mappings = {col: details['new_name'] for col, details in mappings.items()\n","                        if isinstance(details, dict) and 'new_name' in details}\n","        existing_columns = df.columns\n","        for old_col, new_col in rename_mappings.items():\n","            if old_col in existing_columns:\n","                #print(f\"Renaming {old_col} to {new_col}\")\n","                df = df.withColumnRenamed(old_col, new_col)\n","            else:\n","                #print(f\"Column {old_col} not found, adding {new_col} with None values\")\n","                df = df.withColumn(new_col, lit(None))\n","\n","        # Add new columns with default values\n","        add_columns = mappings.get(\"add_columns\", {})\n","        for new_col, default_value in add_columns.items():\n","            df = df.withColumn(new_col, lit(default_value))\n","\n","        return df"]},{"cell_type":"code","execution_count":null,"id":"a9c53bdd-d5cf-4566-8e25-624c98189122","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["for json_name, df in json_dfs.items():\n","    if json_name in column_mappings:\n","        df = apply_column_mappings(df, column_mappings[json_name])\n","        json_dfs[json_name] = df  "]},{"cell_type":"code","execution_count":null,"id":"bdb99efa-89f9-46e5-9dfb-f02f77fab6da","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# List of jobs next to get the dimensions in the correct schema.\n","\n","try:\n","    if json_dfs['readingprogress'].count() > 0:\n","        try:\n","            df = json_dfs['readingprogress']\n","            df = df.withColumn(\"external_id\", concat(col(\"submissionId\"), col(\"submissionDateTime\")))\n","            json_dfs['readingprogress'] = df\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")  \n","    else:\n","        print(\"DataFrame is empty, skipping the operation.\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")  "]},{"cell_type":"code","execution_count":null,"id":"5e31ed80-77b8-4d46-b0f6-29ef7a4a0d34","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def add_missing_columns(df_to_adjust, df_reference):\n","    missing_columns = set(df_reference.columns) - set(df_to_adjust.columns)\n","    for column in missing_columns:\n","        df_to_adjust = df_to_adjust.withColumn(column, lit(None).cast(df_reference.schema[column].dataType))\n","    return df_to_adjust"]},{"cell_type":"code","execution_count":null,"id":"62718ce2-e976-419b-8c82-be47ddb6bd13","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Process each DataFrame and upsert it to the silver_path\n","for json_name, df in json_dfs.items():\n","    \n","    if json_name in delta_table_name_mapping and delta_table_name_mapping[json_name] != \"\":\n","        #print(json_name)\n","        if df.count() > 0:\n","            # Get the Delta table name from the mapping\n","            delta_table_name = delta_table_name_mapping[json_name]\n","            silver_table_path = f\"{silver_path}/{delta_table_name}\"\n","            uuid_column_name = oeai.get_uuid_column_name(delta_table_name)\n","\n","            # Define the unique key column name\n","            unique_key_column = \"unique_key\"  \n","    \n","            if delta_table_name == \"dim_graph_User\":\n","\n","                dim_student_df = spark.read.format(\"delta\").load(f\"{wonde_silver_path}/dim_StudentExtended\").select(\"Email\", \"organisationkey\", \"studentkey\")\n","                # Rename the 'studentkey' column from dim_student_df to avoid ambiguity\n","                dim_student_df = dim_student_df.withColumnRenamed(\"studentkey\", \"dim_studentkey\")\n","                \n","                # Perform a left join\n","                df_studjoined = df.alias(\"source\").join(\n","                    dim_student_df.alias(\"dim\"),\n","                    (trim(lower(col(\"source.mail\"))) == trim(lower(col(\"dim.Email\")))),\n","                    \"left\"\n","                )\n","                \n","                # Use when() to decide which studentkey to keep\n","                df_both_keys = df_studjoined.withColumn(\"studentkey\", \n","                                        when(col(\"dim.dim_studentkey\").isNull(), col(\"source.studentkey\"))\n","                                        .otherwise(col(\"dim.dim_studentkey\"))\n","                                        ) \\\n","                            .drop(\"dim.dim_studentkey\") \\\n","                            .select(\"source.*\", \"studentkey\")\n","\n","                df = df_both_keys\n","                # if the studentkey lookup has failed to find a student record then remove the related record for referential integrity\n","                df = df.filter((col(\"studentkey\").isNotNull()) & (col(\"studentkey\") != \"\"))\n","\n","               \n","                \n","            # Set the update columns to update everything other than organisationkey and the unique_key\n","            update_columns = {col: f\"source.{col}\" for col in df.columns if col not in ['organisationkey', uuid_column_name]}\n","            # print(update_columns)\n","\n","            if DeltaTable.isDeltaTable(spark, silver_table_path):\n","                delta_table = DeltaTable.forPath(spark, silver_table_path)\n","                target_df = delta_table.toDF()\n","                \n","                # Identify columns in source not in target\n","                new_columns = set(df.columns) - set(target_df.columns)\n","                \n","                if new_columns:\n","                    # Add new columns with nulls to the target DataFrame\n","                    for new_col in new_columns:\n","                        target_df = target_df.withColumn(new_col, lit(None).cast(df.schema[new_col].dataType))\n","                    \n","                    # Create a new Delta table with the updated schema from the target DataFrame\n","                    new_table_path = silver_table_path + \"_new\"\n","                    \n","                    if DeltaTable.isDeltaTable(spark, new_table_path):\n","                        print(f\"Table at {new_table_path} exists. Deleting...\")\n","                        deltaTable = DeltaTable.forPath(spark, new_table_path)\n","                        deltaTable.delete()\n","                    \n","                    target_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(new_table_path)\n","                    \n","                    # Prepare the new Delta table for merging\n","                    new_delta_table = DeltaTable.forPath(spark, new_table_path)\n","                    \n","                    # Adjust the source DataFrame to match the target schema, including any new columns\n","                    df_adjusted = add_missing_columns(df, target_df)\n","                    \n","                    # Perform the merge operation\n","                    new_delta_table.alias(\"target\").merge(\n","                        df_adjusted.alias(\"source\"),\n","                        f\"target.external_id = source.external_id\"\n","                    ).whenMatchedUpdate(set=update_columns\n","                    ).whenNotMatchedInsertAll().execute()\n","                    \n","                    # Overwrite the old table with the new table's data\n","                    spark.read.format(\"delta\").load(new_table_path).write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(silver_table_path)\n","                    \n","                    # Consider cleaning up the new_table_path if necessary\n","\n","                else:\n","                    df_adjusted = add_missing_columns(df, target_df)\n","                    #df_adjusted.show(n=20, truncate=False)\n","                    # Perform the merge operation with the adjusted source DataFrame\n","                          \n","                    # Remove duplicates based on 'external_id' - note this is due to scholarpack holding multiple records for students that have left and rejoined with the same email address\n","                    df_adjusted = df_adjusted.dropDuplicates([\"external_id\"])\n","\n","                   \n","                    delta_table.alias(\"target\").merge(\n","                        df_adjusted.alias(\"source\"),\n","                        \"target.external_id = source.external_id\"\n","                    ).whenMatchedUpdate(set=update_columns\n","                    ).whenNotMatchedInsertAll().execute()\n","\n","            else:\n","                # If the table does not exist, create it by writing the current DataFrame\n","                # First, generate a UUID for all records in the new UUID column\n","                df = df.withColumn(uuid_column_name, expr(\"uuid()\"))\n","                # debug, show 20 records\n","                #df.show(n=20, truncate=False)\n","\n","                df.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)\n","         "]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
