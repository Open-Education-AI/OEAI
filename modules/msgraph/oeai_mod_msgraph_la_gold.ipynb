{"cells":[{"cell_type":"code","execution_count":null,"id":"ffaa03e0-cdbc-480a-a5ce-45cd23348907","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%run oeai_py"]},{"cell_type":"code","execution_count":null,"id":"7263aebe-e061-48a8-892c-d2b6c7710bd8","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n","oeai = OEAI(\"Fabric\")"]},{"cell_type":"code","execution_count":null,"id":"3a534790-ef28-4d6d-932a-34407e206ca6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# CHANGE VALUES FOR YOUR KEY VAULT\n","keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\" # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n","keyvault_linked_service = \"INSERT_YOUR_LINKED_SERVICE_NAME_HERE\"  # Not required for Fabric.\n","\n","# Synapse OEA environment paths\n","silver_path = oeai.get_secret(spark, \"msgraph-silver\", keyvault_linked_service, keyvault)\n","silver_path_wonde = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\n","gold_path = oeai.get_secret(spark, \"gold-path\", keyvault_linked_service, keyvault)\n","storage_account_name = oeai.get_secret(spark, \"storage-account\", keyvault_linked_service, keyvault)\n","storage_account_access_key = oeai.get_secret(spark, \"storage-accesskey\", keyvault_linked_service, keyvault)"]},{"cell_type":"code","execution_count":null,"id":"5f7f80e1-ad3f-4bed-88cb-a9a8c0d35f76","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, gold_path):\n","    \"\"\"\n","    Sets up configuration for Azure storage access, lists subdirectories in the silver path, and processes\n","    each Delta Lake table by converting and saving it in Parquet format in the gold path.\n","\n","    Args:\n","        spark (SparkSession): Active Spark session.\n","        storage_account_name (str): Azure storage account name.\n","        storage_account_access_key (str): Access key for the Azure storage account.\n","        silver_path (str): Path to the silver layer directory (source Delta tables).\n","        gold_path (str): Path to the gold layer directory (destination for Parquet files).\n","\n","    This function will process each Delta Lake table found in the silver layer, partition the data by \n","    'organisationkey', and write it to the gold layer as Parquet files.\n","    \"\"\"\n","    # Set up the configuration for accessing the storage account\n","    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_access_key)\n","\n","    sc = spark.sparkContext\n","    hadoop_conf = sc._jsc.hadoopConfiguration()\n","    hadoop_conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n","    hadoop_conf.set(\"fs.azure.account.key.\" + storage_account_name + \".dfs.core.windows.net\", storage_account_access_key)\n","\n","    # URI for the parent directory\n","    parent_dir_uri = sc._gateway.jvm.java.net.URI(silver_path)\n","\n","    # Hadoop Path of the parent directory\n","    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n","    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n","\n","    # Get the FileSystem for the given URI and configuration\n","    fs = FileSystem.get(parent_dir_uri, hadoop_conf)\n","\n","    # List the subdirectories at the given URI\n","    status = fs.listStatus(Path(silver_path))\n","    delta_table_paths = [file.getPath().toString() for file in status if file.isDirectory()]\n","\n","    for table_path in delta_table_paths:\n","        try:\n","            df = spark.read.format(\"delta\").load(table_path)\n","            table_name = os.path.basename(urlparse(table_path).path)\n","            parquet_output_folder_path = os.path.join(gold_path, table_name)\n","            df.write.mode(\"overwrite\").format(\"parquet\").save(parquet_output_folder_path)\n","            \n","            \n","        except AnalysisException as e:\n","            print(f\"Error reading Delta table at {table_path}: \", e)"]},{"cell_type":"code","execution_count":null,"id":"12f82654-a191-4e56-8613-d69372d97ffc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Initialize Spark session\n","process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, gold_path)"]},{"cell_type":"code","execution_count":null,"id":"0d95bfee-1979-40dc-9b93-5367b29cbf20","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["fact_graph_ReadingProgress_path = f\"{silver_path}/fact_graph_ReadingProgress\"\n","dim_graph_Assignment_path = f\"{silver_path}/dim_graph_Assignment\"\n","dim_graph_User_path = f\"{silver_path}/dim_graph_User\"\n","dim_Student_path = f\"{silver_path_wonde}/dim_Student\"\n","dim_Organisation_path = f\"{silver_path_wonde}/dim_Organisation\"\n","\n","# Load Delta tables into DataFrames\n","df_fact_graph_ReadingProgress = spark.read.format(\"delta\").load(fact_graph_ReadingProgress_path)\n","df_dim_graph_Assignment = spark.read.format(\"delta\").load(dim_graph_Assignment_path)\n","df_dim_graph_User = spark.read.format(\"delta\").load(dim_graph_User_path)\n","df_dim_Student = spark.read.format(\"delta\").load(dim_Student_path)\n","df_dim_Organisation = spark.read.format(\"delta\").load(dim_Organisation_path)"]},{"cell_type":"code","execution_count":null,"id":"52cf3c4e-5a5d-469f-890c-65e08255a745","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Join fact_graph_ReadingProgress with dim_graph_Assignment\n","df_enriched = df_fact_graph_ReadingProgress.join(\n","    df_dim_graph_Assignment,\n","    df_fact_graph_ReadingProgress['assignmentid'] == df_dim_graph_Assignment['external_id'],\n","    how='left'\n",")\n","\n","# Join the result with dim_graph_User\n","df_enriched = df_enriched.join(\n","    df_dim_graph_User,\n","    df_enriched['studentid'] == df_dim_graph_User['external_id'],\n","    how='left'\n",")\n","\n","# Join the result with dim_Student using studentkey from dim_graph_User\n","df_enriched = df_enriched.join(\n","    df_dim_Student,\n","    df_dim_graph_User['studentkey'] == df_dim_Student['studentkey'],\n","    how='left'\n",")\n","\n","# Join the result with dim_Organisation using organisationkey from dim_Student\n","df_enriched = df_enriched.join(\n","    df_dim_Organisation,\n","    df_dim_Student['organisationkey'] == df_dim_Organisation['organisationkey'],\n","    how='left'\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"07282f49-6945-4af7-874f-beb68da5feb9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.functions import col\n","\n","# Select and rename the columns\n","df_enriched = df_enriched.select(\n","    df_fact_graph_ReadingProgress['*'],\n","    df_dim_graph_Assignment['displayName'],\n","    df_dim_Organisation['Organisation_Name']\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"2ff1a185-2c6c-40d3-a80b-d8eb3c14f21f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Write the result to Parquet\n","df_enriched.write.format(\"parquet\").mode(\"overwrite\").save(gold_path + 'fact_graph_ReadingProgress')\n","\n","print(f\"Enriched fact_graph_ReadingProgress has been saved to Parquet at {gold_path}\")\n"]}],"metadata":{"description":null,"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
