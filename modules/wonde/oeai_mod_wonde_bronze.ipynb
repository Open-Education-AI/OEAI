{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT KEY VAULT NAME HERE\"\n",
        "keyvault_linked_service = \"INSERT LINKED SERVICE NAME FOR KEY VAULT HERE\" \n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"wonde-bronze\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"school-ids\", keyvault_linked_service, keyvault)\n",
        "school_ids = school_ids_secret.split(\",\")\n",
        "\n",
        "# Set up date parameters\n",
        "today = datetime.today()\n",
        "last_year = today - timedelta(days=365)\n",
        "DateFrom = last_year.strftime('%Y-%m-%d')\n",
        "DateTo = today.strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# initialise the audit log\n",
        "audit_log = oeai.load_audit_log(spark, bronze_path + \"audit_log.json\")\n",
        "audit_logs = []\n",
        "error_log_path = bronze_path + \"error_log.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_school_data(token: str, school_id: str, endpoint: str, query: str, pagination_type: str = \"cursor\") -> dict:\n",
        "    \"\"\"\n",
        "    Fetches data for a specific school using the Wonde API.\n",
        "\n",
        "    Args:\n",
        "        token (str): The authentication token for the Wonde API.\n",
        "        school_id (str): The unique identifier of the school.\n",
        "        endpoint (str): The specific endpoint of the API to query.\n",
        "        query (str): Additional query parameters for the API call.\n",
        "        pagination_type (str, optional): Type of pagination to use. Defaults to 'cursor'.\n",
        "            Supported values: 'cursor', 'offset'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A collection of data for the specified school.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the endpoint URL\n",
        "    endpoint = endpoint.rstrip('/') + '/'\n",
        "\n",
        "    # Construct the base URL\n",
        "    base_url = f\"https://api.wonde.com/v1.0/schools/{school_id}{endpoint}\"\n",
        "    \n",
        "    # Format the query string\n",
        "    query = f\"?{query.lstrip('&?')}\"\n",
        "\n",
        "    url = base_url + query\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "\n",
        "    all_data = []\n",
        "    next_url = url\n",
        "    page = 1\n",
        "    per_page_limit = 50  # Limit for number of items per page\n",
        "\n",
        "    while next_url:\n",
        "        # Adjust URL based on pagination type\n",
        "        if pagination_type == \"offset\":\n",
        "            paginated_url = f\"{url}&page={page}\"\n",
        "        else:\n",
        "            paginated_url = next_url\n",
        "\n",
        "        response = requests.get(paginated_url, headers=headers)\n",
        "\n",
        "        # Handle unsuccessful requests\n",
        "        if response.status_code != 200:\n",
        "            error_message = f\"Error fetching data from {paginated_url}: {response.status_code} {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            break\n",
        "\n",
        "        response_data = response.json()\n",
        "\n",
        "        # Extract data from response\n",
        "        data_from_response = response_data.get(\"data\", [])\n",
        "        \n",
        "        # Append data based on its type (list or dict)\n",
        "        if isinstance(data_from_response, dict):\n",
        "            all_data.append(data_from_response)\n",
        "        else:\n",
        "            all_data.extend(data_from_response)\n",
        "\n",
        "        # Handle pagination logic\n",
        "        if pagination_type == \"cursor\":\n",
        "            next_url = response_data.get(\"meta\", {}).get(\"pagination\", {}).get(\"next\")\n",
        "        elif pagination_type == \"offset\":\n",
        "            if len(data_from_response) < per_page_limit:\n",
        "                break  # End loop if fewer items than per_page_limit\n",
        "            page += 1\n",
        "            next_url = f\"{url}&page={page}\"\n",
        "\n",
        "    return all_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def load_bronze(spark, endpoint: str, subkey: str, school_id: str, token: str, pagination_type, limit=None, query=None, use_date_chunk=False, has_students_array:bool=False, audit_log_file=\"audit_log.json\", override_date=None):\n",
        "    \"\"\"\n",
        "    Loads data from an API into a Bronze layer, handling pagination, date chunking, and audit logging.\n",
        "\n",
        "    Args:\n",
        "        spark (SparkSession): Active SparkSession for DataFrame operations.\n",
        "        endpoint (str): API endpoint to retrieve data from.\n",
        "        subkey (str): Subkey for identifying the specific data.\n",
        "        school_id (str): Unique identifier for the school.\n",
        "        token (str): Authentication token for API access.\n",
        "        pagination_type (str): Type of pagination used by the API ('cursor' or 'offset').\n",
        "        limit (int, optional): Limit for the number of records to retrieve. Defaults to None.\n",
        "        query (str, optional): Additional query parameters for the API call. Defaults to None.\n",
        "        use_date_chunk (bool, optional): Flag to indicate if date chunking is to be used. Defaults to False.\n",
        "        has_students_array (bool, optional): Flag to indicate if the data contains a students array. Defaults to False.\n",
        "        audit_log_file (str, optional): Filename for the audit log. Defaults to \"audit_log.json\".\n",
        "        override_date (str, optional): Date string in 'YYYY-MM-DD HH:MM:SS' format to override the last updated logic.\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: A PySpark DataFrame with the loaded data.\n",
        "    \"\"\"\n",
        "    global audit_log\n",
        "    df = pd.DataFrame()\n",
        "    data_list = [] \n",
        "    full_data_list = []\n",
        "    last_updated_str = None\n",
        "\n",
        "    # Calculate the duration of the API call\n",
        "    start_time = datetime.now()\n",
        "    now = datetime.now()\n",
        "    \n",
        "    if override_date:\n",
        "        last_updated_time = datetime.strptime(override_date, \"%Y-%m-%d %H:%M:%S\")\n",
        "    else:\n",
        "        last_updated_str = oeai.safe_get_or_create(LastUpdated, \"2018-09-01 00:00:00\", school_id, subkey)\n",
        "        if last_updated_str is None:\n",
        "            last_updated_time = now - timedelta(weeks=2)\n",
        "        elif isinstance(last_updated_str, str):\n",
        "            last_updated_time = datetime.strptime(last_updated_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "        elif isinstance(last_updated_str, datetime):\n",
        "            last_updated_time = last_updated_str\n",
        "        else:\n",
        "            last_updated_time = now - timedelta(weeks=2)\n",
        "\n",
        "    # If last_updated_time is more than two weeks ago, chunk the requests\n",
        "    if use_date_chunk and (now - last_updated_time).days > 14:\n",
        "        for start_date, end_date in oeai.generate_date_chunks(last_updated_time, now, chunk_size=timedelta(weeks=2)):\n",
        "            chunk_query = oeai.update_query_with_chunks(query, start_date, end_date)\n",
        "            r = get_school_data(token, school_id, endpoint, chunk_query, pagination_type)\n",
        "\n",
        "            # Check if the response is not None and not empty before processing\n",
        "            if r:\n",
        "                if isinstance(r, dict) and 'data' in r:\n",
        "                    data_list.append(r['data'])\n",
        "                elif isinstance(r, list):\n",
        "                    data_list.extend(r)\n",
        "            else:\n",
        "                error_message = f\"Empty response, not adding to data_list: {traceback.format_exc()}\"\n",
        "                oeai.log_error(spark, error_message, error_log_path)\n",
        "    else:\n",
        "        if not override_date and last_updated_str is not None:\n",
        "            query += \"&updated_after=\" + last_updated_str\n",
        "        \n",
        "        r = get_school_data(token, school_id, endpoint, query, pagination_type)\n",
        "\n",
        "        # Ensure the data is always a list\n",
        "        if isinstance(r, dict) and 'data' in r:\n",
        "            data_list = [r['data']]\n",
        "        elif isinstance(r, list):\n",
        "            data_list = r\n",
        "\n",
        "    # Construct the directory path\n",
        "    school_folder = os.path.join(bronze_path, school_id)\n",
        "\n",
        "    # Check and create directory if it doesn't exist\n",
        "    if not os.path.exists(school_folder):\n",
        "        os.makedirs(school_folder)\n",
        "\n",
        "    LastUpdated[school_id][subkey] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    json_str = json.dumps(LastUpdated)\n",
        "    last_updated_df = spark.createDataFrame([LastUpdated])\n",
        "    last_updated_df.repartition(1).write.mode(\"overwrite\").json(bronze_path + 'last_run')\n",
        "\n",
        "    if not data_list:\n",
        "        oeai.save_empty_json(spark, school_folder + \"/\" + subkey + \".json\")\n",
        "    else:\n",
        "        try:\n",
        "            # Flatten each item in data_list\n",
        "            if has_students_array:\n",
        "                flattened_data_list = oeai.flatten_nested_json(json.dumps(data_list))\n",
        "            else:\n",
        "                flattened_data_list = [oeai.flatten_json(item) for item in data_list]\n",
        "\n",
        "            # Convert the list of dictionaries to a Pandas DataFrame\n",
        "            pandas_df = pd.DataFrame(flattened_data_list)\n",
        "\n",
        "            # Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "            r_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "            # Add school_id and unique_key to the DataFrame\n",
        "            r_df = r_df.withColumn(\"school_id\", lit(school_id))\n",
        "            if \"student_data_id\" in r_df.columns:\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(school_id),r_df[\"student_data_id\"].cast(\"string\"), r_df[\"id\"].cast(\"string\")))\n",
        "            else:\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(school_id), r_df[\"id\"].cast(\"string\")))\n",
        "\n",
        "            # Save the DataFrame to a JSON file\n",
        "            r_df.write.mode(\"overwrite\").json(school_folder + \"/\" + subkey + \".json\")\n",
        "      \n",
        "        # if the key doesn't exist, skip it    \n",
        "        except Exception as e:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            pass\n",
        "\n",
        "    # Update the audit log\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    duration_str = str(duration)\n",
        "    audit_data = {\n",
        "        \"school_id\": school_id,\n",
        "        \"endpoint\": endpoint,\n",
        "        \"query\": query,\n",
        "        \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"duration\": duration_str,\n",
        "        \"records_returned\": str(len(data_list)),\n",
        "    }\n",
        "    audit_log.append(audit_data)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  BRONZE PROCESS\n",
        "'''\n",
        "# introduce a limit for testing or leave as None for Live\n",
        "Limit = None\n",
        "\n",
        "# Populate the schools list with the Wonde school_id and secret\n",
        "schools_list = []\n",
        "for school_id in school_ids:\n",
        "    secret_name = f\"wonde-{school_id}\"\n",
        "    try:\n",
        "        token = oeai.get_secret(spark, secret_name, keyvault_linked_service, keyvault)\n",
        "        schools_list.append({\"school_id\": school_id, \"token\": token})\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error: {traceback.format_exc()}\"\n",
        "        oeai.log_error(spark, error_message, error_log_path)\n",
        "\n",
        "# Reset LastUpdated at the beginning of your read or function block\n",
        "LastUpdated = {}\n",
        "\n",
        "# Read the JSON file using PySpark\n",
        "df = spark.read.json(bronze_path + 'last_run')\n",
        "rows = df.collect()\n",
        "if rows:\n",
        "    LastUpdated = rows[0].asDict()\n",
        "else:\n",
        "    # Handle the case where the JSON file might be empty or not read correctly\n",
        "    LastUpdated = {}\n",
        "\n",
        "# Convert 'LastUpdated' Row objects to dictionaries\n",
        "for key, value in LastUpdated.items():\n",
        "    if isinstance(value, Row):\n",
        "        LastUpdated[key] = oeai.row_to_dict(value)\n",
        "\n",
        "for school in schools_list:\n",
        "    school_id = school[\"school_id\"]\n",
        "    token = school[\"token\"]  \n",
        "\n",
        "    daily_jobs = [\n",
        "        (\"\", \"schools\", \"cursor\", Limit, \"\", False, False), \n",
        "        (\"/students\", \"students\", \"cursor\", Limit, \"\", False, False), \n",
        "        (\"/students\", \"students_education\", \"cursor\", Limit, \"&include=education_details\", False, False), \n",
        "        (\"/students\", \"students_extended\", \"cursor\", Limit, \"&include=extended_details\", False, False), \n",
        "        (\"/attendance-summaries\", \"attendance-summaries\", \"cursor\", Limit, \"\", False, False),\n",
        "        ]\n",
        "\n",
        "    # call load bronze for each of the daily jobs\n",
        "    for job in daily_jobs:\n",
        "        load_bronze(spark, job[0], job[1], school[\"school_id\"], school[\"token\"], job[2], job[3], job[4], job[5], job[6])\n",
        "        # to override the lastupdated:\n",
        "        #load_bronze(spark, job[0], job[1], school_id, token, job[2], job[3], job[4], job[5], job[6], override_date=\"2024-01-01 00:00:00\")\n",
        "\n",
        "\n",
        "    # Save the audit log\n",
        "    oeai.save_audit_log(spark, audit_log, bronze_path + \"audit_log.json\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
