{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\"  # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n",
        "keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME_HERE\"  # Not required for Fabric.\n",
        "\n",
        "# Synapse OEA environment paths\n",
        "silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\n",
        "gold_path = oeai.get_secret(spark, \"gold-path\", keyvault_linked_service, keyvault)\n",
        "storage_account_name = oeai.get_secret(spark, \"storage-account\", keyvault_linked_service, keyvault)\n",
        "storage_account_access_key = oeai.get_secret(spark, \"storage-accesskey\", keyvault_linked_service, keyvault)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, gold_path):\n",
        "    \"\"\"\n",
        "    Sets up configuration for Azure storage access, lists subdirectories in the silver path, and processes\n",
        "    each Delta Lake table by converting and saving it in Parquet format in the gold path.\n",
        "\n",
        "    Args:\n",
        "        spark (SparkSession): Active Spark session.\n",
        "        storage_account_name (str): Azure storage account name.\n",
        "        storage_account_access_key (str): Access key for the Azure storage account.\n",
        "        silver_path (str): Path to the silver layer directory (source Delta tables).\n",
        "        gold_path (str): Path to the gold layer directory (destination for Parquet files).\n",
        "\n",
        "    This function will process each Delta Lake table found in the silver layer, partition the data by \n",
        "    'organisationkey', and write it to the gold layer as Parquet files.\n",
        "    \"\"\"\n",
        "    # Set up the configuration for accessing the storage account\n",
        "    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_access_key)\n",
        "\n",
        "    sc = spark.sparkContext\n",
        "    hadoop_conf = sc._jsc.hadoopConfiguration()\n",
        "    hadoop_conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
        "    hadoop_conf.set(\"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\", storage_account_access_key)\n",
        "\n",
        "    # URI for the parent directory\n",
        "    parent_dir_uri = sc._gateway.jvm.java.net.URI(silver_path)\n",
        "\n",
        "    # Hadoop Path of the parent directory\n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
        "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
        "\n",
        "    # Get the FileSystem for the given URI and configuration\n",
        "    fs = FileSystem.get(parent_dir_uri, hadoop_conf)\n",
        "\n",
        "    # List the subdirectories at the given URI\n",
        "    status = fs.listStatus(Path(silver_path))\n",
        "    delta_table_paths = [file.getPath().toString() for file in status if file.isDirectory()]\n",
        "\n",
        "    for table_path in delta_table_paths:\n",
        "        try:\n",
        "            df = spark.read.format(\"delta\").load(table_path)\n",
        "            table_name = os.path.basename(urlparse(table_path).path)\n",
        "            parquet_output_folder_path = os.path.join(gold_path, table_name)\n",
        "            \n",
        "            df = df.withColumn(\"partitionkey\", col(\"organisationkey\"))\n",
        "            df.write.partitionBy(\"partitionkey\").mode(\"overwrite\").format(\"parquet\").save(parquet_output_folder_path)\n",
        "            \n",
        "        except AnalysisException as e:\n",
        "            print(f\"Error reading Delta table at {table_path}: \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, gold_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
