{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "import itertools\n",
        "replace_counter = itertools.count(start=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_KEYVAULT_NAME\"  # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n",
        "keyvault_linked_service = \"INSERT_LINKED_SERVICE_NAME\"  # Not required for Fabric.\n",
        "\n",
        "# Synapse OEA environment paths\n",
        "silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\n",
        "pseudo_path = oeai.get_secret(spark, \"pseudo-path\", keyvault_linked_service, keyvault)\n",
        "storage_account_name = oeai.get_secret(spark, \"storage-account\", keyvault_linked_service, keyvault)\n",
        "storage_account_access_key = oeai.get_secret(spark, \"storage-accesskey\", keyvault_linked_service, keyvault)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pseudo_mapping = {\n",
        "    \"dim_Organisation\": {\n",
        "        # pseudoID\n",
        "        \"URN\": \"pseudoID\",\n",
        "        # mask\n",
        "        \"Establishment_Number\": \"mask\",\n",
        "        \"LA_Code\": \"mask\",\n",
        "        # replace\n",
        "        \"Organisation_Name\": {\"string\": \"Academy \"}\n",
        "        # hash\n",
        "    },\n",
        "    \"dim_Student\": {\n",
        "        # pseudoID\n",
        "        \"UPN\": \"pseudoID\",\n",
        "        # mask string\n",
        "        \n",
        "        \"Forename\": \"mask\",\n",
        "        \"Legal_Forename\": \"mask\",\n",
        "        \"Legal_Surname\": \"mask\",\n",
        "        \"Middle_Names\": \"mask\",\n",
        "        # replace\n",
        "        \"Surname\": {\"string\": \"Student \"},\n",
        "        # hash\n",
        "        # mask date\n",
        "        \"Date_Of_Birth\": \"mask_date\",\n",
        "    },\n",
        "    \"dim_StudentExtended\": {\n",
        "        # pseudoID\n",
        "        # mask\n",
        "        \"First_Language\": \"mask\",\n",
        "        # replace\n",
        "        # hash\n",
        "    },\n",
        "    \"fact_Achievement\": {\n",
        "        # pseudoID\n",
        "        # mask\n",
        "        \"Comment\": \"mask\",\n",
        "        # replace\n",
        "        # hash\n",
        "    },\n",
        "    \"fact_Behaviour\": {\n",
        "        # pseudoID\n",
        "        # mask\n",
        "        \"Comment\": \"mask\",\n",
        "        # replace\n",
        "        # hash\n",
        "    },\n",
        "    \"fact_Exclusion\": {\n",
        "        # pseudoID\n",
        "        # mask\n",
        "        \"Comments\": \"mask\",\n",
        "        # replace\n",
        "        # hash\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Pseudonymization functions\n",
        "def mask_function(col_value):\n",
        "    return \"XXXXXX\"  # Simple mask\n",
        "\n",
        "def get_replace_function(prefix):\n",
        "    def replace_function(col_value):\n",
        "        \n",
        "        return f\"{prefix}{next(replace_counter)}\"\n",
        "    return replace_function\n",
        "\n",
        "mask_udf = udf(mask_function, StringType())\n",
        "\n",
        "fixed_date_str = \"2000-01-01\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def pseudonymize_df(df, table_name, config, pseudo_path, spark, replace_counter):\n",
        "    for col_name, pseudo_type in config.items():\n",
        "        if pseudo_type == \"pseudoID\":\n",
        "            # call save_mapping_for_pseudoID\n",
        "            df = save_mapping_for_pseudoID(df, table_name, col_name, pseudo_path, spark)\n",
        "        elif pseudo_type == \"mask\":\n",
        "            # Apply fixed mask\n",
        "            df = df.withColumn(col_name, mask_udf(col_name))\n",
        "        elif isinstance(pseudo_type, dict) and \"string\" in pseudo_type:\n",
        "            # Dynamically create and apply a UDF for replacement with the specific prefix\n",
        "            prefix = pseudo_type[\"string\"]\n",
        "            replace_udf = udf(get_replace_function(prefix), StringType())\n",
        "            df = df.withColumn(col_name, replace_udf(col(col_name)))\n",
        "        elif pseudo_type == \"mask_date\":\n",
        "            # Apply fixed date masking\n",
        "            df = df.withColumn(col_name, lit(fixed_date_str))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_table_name_from_path(table_path):\n",
        "    # Extracts the table name from the full path\n",
        "    return table_path.split('/')[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def save_mapping_for_pseudoID(df, table_name, col_name, gold_path, spark):\n",
        "    # Generate a new ID column\n",
        "    df_with_new_id = df.withColumn('new_id', monotonically_increasing_id())\n",
        "    \n",
        "    # Create a mapping DataFrame with original values and new pseudo IDs\n",
        "    mapping_df = df_with_new_id.select(col(col_name).alias('original_value'), 'new_id')\n",
        "    \n",
        "    # Define the path for saving the mapping file\n",
        "    mapping_file_path = f\"{pseudo_path}/pseudo_map_{table_name}_{col_name}.parquet\"\n",
        "    \n",
        "    # Save the mapping DataFrame to a Parquet file\n",
        "    mapping_df.write.mode('overwrite').parquet(mapping_file_path)\n",
        "    \n",
        "    # Replace the original column with the 'new_id' column in the main DataFrame\n",
        "    df_final = df_with_new_id.drop(col_name).withColumnRenamed('new_id', col_name)\n",
        "    \n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, pseudo_path):\n",
        "    \"\"\"\n",
        "    Sets up configuration for Azure storage access, lists subdirectories in the silver path, and processes\n",
        "    each Delta Lake table by converting and saving it in Parquet format in the gold path.\n",
        "\n",
        "    Args:\n",
        "        spark (SparkSession): Active Spark session.\n",
        "        storage_account_name (str): Azure storage account name.\n",
        "        storage_account_access_key (str): Access key for the Azure storage account.\n",
        "        silver_path (str): Path to the silver layer directory (source Delta tables).\n",
        "        gold_path (str): Path to the gold layer directory (destination for Parquet files).\n",
        "\n",
        "    This function will process each Delta Lake table found in the silver layer, partition the data by \n",
        "    'organisationkey', and write it to the gold layer as Parquet files.\n",
        "    \"\"\"\n",
        "    # Set up the configuration for accessing the storage account\n",
        "    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_access_key)\n",
        "\n",
        "    sc = spark.sparkContext\n",
        "    hadoop_conf = sc._jsc.hadoopConfiguration()\n",
        "    hadoop_conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
        "    hadoop_conf.set(\"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\", storage_account_access_key)\n",
        "\n",
        "    # URI for the parent directory\n",
        "    parent_dir_uri = sc._gateway.jvm.java.net.URI(silver_path)\n",
        "\n",
        "    # Hadoop Path of the parent directory\n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
        "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
        "\n",
        "    # Get the FileSystem for the given URI and configuration\n",
        "    fs = FileSystem.get(parent_dir_uri, hadoop_conf)\n",
        "\n",
        "    # List the subdirectories at the given URI\n",
        "    status = fs.listStatus(Path(silver_path))\n",
        "    delta_table_paths = [file.getPath().toString() for file in status if file.isDirectory()]\n",
        "\n",
        "    for table_path in delta_table_paths:\n",
        "        try:\n",
        "            df = spark.read.format(\"delta\").load(table_path)\n",
        "            table_name = os.path.basename(urlparse(table_path).path)\n",
        "\n",
        "            # Retrieve pseudonymization config for the table\n",
        "            config = pseudo_mapping.get(table_name, {})\n",
        "                            \n",
        "            # Apply pseudonymization\n",
        "            if config:  \n",
        "                df = pseudonymize_df(df, table_name, pseudo_mapping[table_name], pseudo_path, spark, replace_counter)\n",
        "            else:\n",
        "                print(f\"No pseudonymization configuration found for {table_name}\")\n",
        "\n",
        "            parquet_output_folder_path = os.path.join(pseudo_path, table_name)\n",
        "            df = df.withColumn(\"partitionkey\", col(\"organisationkey\"))\n",
        "            df.write.partitionBy(\"partitionkey\").mode(\"overwrite\").format(\"parquet\").save(parquet_output_folder_path)\n",
        "\n",
        "        except AnalysisException as e:\n",
        "            print(f\"Error reading Delta table at {table_path}: \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Process Delta tables to Parquet\n",
        "process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, pseudo_path)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
