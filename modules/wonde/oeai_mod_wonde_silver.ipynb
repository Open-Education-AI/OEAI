{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\"  \n",
        "keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME_HERE\" # not required for Fabric  \n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"wonde-bronze\", keyvault_linked_service, keyvault)\n",
        "silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\n",
        "gold_path = oeai.get_secret(spark, \"gold-path\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"school-ids\", keyvault_linked_service, keyvault)\n",
        "subdirectories = school_ids_secret.split(\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the mapping between JSON files and desired Delta table names\n",
        "delta_table_name_mapping = {\n",
        "    #\"schools.json\": \"dim_Organisation\",\n",
        "    \"students.json\": \"dim_Student\",\n",
        "    \"students_extended.json\": \"dim_StudentExtended\",\n",
        "    \"students_education.json\": \"\",\n",
        "    \"students_contact_details.json\": \"dim_Address\",\n",
        "    \"students_leaver.json\": \"\",\n",
        "    \"students_leaver_extended.json\": \"\",\n",
        "    \"students_leaver_education.json\": \"\",\n",
        "    \"attendance_summaries.json\": \"fact_AttendanceSummary\",\n",
        "    #\"attendance_session.json\": \"fact_AttendanceSession\",\n",
        "    \"attendance_codes.json\": \"\",\n",
        "    \"behaviours_students.json\": \"fact_Behaviour\",\n",
        "    \"exclusions.json\": \"fact_Exclusion\",\n",
        "    \"achievements_students.json\": \"fact_Achievement\",\n",
        "    #\"subjects.json\":\"dim_Subject\",\n",
        "    #\"classes.json\":\"\",\n",
        "    \"groups.json\":\"dim_Group\",\n",
        "    \"group_membership.json\":\"dim_GroupMembership\",\n",
        "    \"aspects.json\":\"dim_Assessment\",\n",
        "    \"resultsets.json\":\"dim_ResultSet\",\n",
        "    \"results.json\":\"fact_Result\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_mappings = {\n",
        "    \"schools.json\": {\n",
        "        # drops\n",
        "        \"timezone\": \"drop\", \n",
        "        \"mis\": \"drop\",\n",
        "        \"address_address_line_1\": \"drop\",\n",
        "        \"address_address_line_2\": \"drop\",\n",
        "        \"address_address_town\": \"drop\",\n",
        "        \"address_address_postcode\": \"drop\",\n",
        "        \"address_address_country_code\": \"drop\",\n",
        "        \"address_address_country_name\": \"drop\",\n",
        "        \"extended_allows_writeback\": \"drop\",\n",
        "        \"extended_has_timetables\": \"drop\",\n",
        "        \"extended_has_lesson_attendance\": \"drop\",\n",
        "        \"extended_audit_approved_at_date\": \"drop\",\n",
        "        \"extended_audit_approved_at_timezone_type\": \"drop\",\n",
        "        \"extended_audit_approved_at_timezone\": \"drop\",\n",
        "        \"region_code\": \"drop\",\n",
        "        \"region_domain\": \"drop\",\n",
        "        \"region_school_url\": \"drop\",\n",
        "        \"region_identifiers_la_code\": \"drop\",\n",
        "        \"region_identifiers_establishment_number\": \"drop\",\n",
        "        \"region_identifiers_urn\": \"drop\",\n",
        "        \"school_id\": \"drop\",\n",
        "        # Renames\n",
        "        \"id\": {\"new_name\": \"external_id\"}, \n",
        "        \"name\": {\"new_name\": \"Organisation_Name\"},  \n",
        "        \"establishment_number\": {\"new_name\": \"Establishment_Number\"},  \n",
        "        \"urn\": {\"new_name\": \"URN\"},\n",
        "        \"la_code\": {\"new_name\": \"LA_Code\"},\n",
        "        \"phase_of_education\": {\"new_name\": \"Organisation_Type\"},\n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",  \n",
        "            \"addresskey\": \"\",  \n",
        "            \"UKPRN\": \"\",\n",
        "            \"Organisation_Status\": \"Active\",\n",
        "            \"last_updated\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"students.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"created_at_date\": \"drop\",\n",
        "        \"created_at_timezone\": \"drop\",\n",
        "        \"created_at_timezone_type\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"updated_at_date\": \"drop\",\n",
        "        \"updated_at_timezone\": \"drop\",\n",
        "        \"updated_at_timezone_type\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        # Renames\n",
        "        \"date_of_birth_date\": {\"new_name\": \"Date_Of_Birth\"}, \n",
        "        \"forename\": {\"new_name\": \"Forename\"}, \n",
        "        \"gender\": {\"new_name\": \"Gender\"}, \n",
        "        \"id\": {\"new_name\": \"student_id\"}, \n",
        "        \"legal_forename\": {\"new_name\": \"Legal_Forename\"}, \n",
        "        \"legal_surname\": {\"new_name\": \"Legal_Surname\"}, \n",
        "        \"middle_names\": {\"new_name\": \"Middle_Names\"}, \n",
        "        \"surname\": {\"new_name\": \"Surname\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"students_extended.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"created_at_date\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"date_of_birth_date\": \"drop\",\n",
        "        \"forename\": \"drop\",\n",
        "        \"gender\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"legal_forename\": \"drop\",\n",
        "        \"legal_surname\": \"drop\",\n",
        "        \"middle_names\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"surname\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        \"mis_id\": \"drop\",\n",
        "        \"extended_details_data_fsm_review_date\": \"drop\",\n",
        "        \"extended_details_data_premium_pupil_notes\": \"drop\",\n",
        "        \"extended_details_data_boarding_status\": \"drop\",\n",
        "        \"extended_details_data_application_status\": \"drop\",\n",
        "        \"extended_details_data_birth_place\": \"drop\",\n",
        "        \"extended_details_data_custody_details\": \"drop\",\n",
        "        \"extended_details_data_dietary_needs\": \"drop\",\n",
        "        \"extended_details_data_general_notes\": \"drop\",\n",
        "        \"extended_details_data_marital_status\": \"drop\",\n",
        "        \"extended_details_data_in_care_details\": \"drop\",\n",
        "        \"extended_details_data_national_insurance_number\": \"drop\",\n",
        "        \"extended_details_data_paramedical_support\": \"drop\",\n",
        "        \"extended_details_data_permanent_resident\": \"drop\",\n",
        "        \"extended_details_data_religion\": \"drop\",\n",
        "        \"extended_details_data_responsible_care_authority\": \"drop\",\n",
        "        \"extended_details_data_youth_support_services_agreement\": \"drop\",\n",
        "        \"extended_details_data_custody_alert\": \"drop\",\n",
        "        \"extended_details_data_next_of_kin\": \"drop\",\n",
        "        # Renames\n",
        "        \"extended_details_data_english_as_additional_language\": {\"new_name\": \"English_As_Additional_Language\"}, \n",
        "        \"extended_details_data_english_as_additional_language_status\": {\"new_name\": \"English_As_Additional_Language_Status\"}, \n",
        "        \"extended_details_data_enrolment_status\": {\"new_name\": \"Enrolment_Status\"}, \n",
        "        \"extended_details_data_ethnicity\": {\"new_name\": \"Ethnicity\"}, \n",
        "        \"extended_details_data_ethnicity_code\": {\"new_name\": \"Ethnicity_Code\"}, \n",
        "        \"extended_details_data_ever_in_care\": {\"new_name\": \"Ever_In_Care\"}, \n",
        "        \"extended_details_data_first_language\": {\"new_name\": \"First_Language\"}, \n",
        "        \"extended_details_data_free_school_meals\": {\"new_name\": \"Free_School_Meals\"}, \n",
        "        \"extended_details_data_free_school_meals_6\": {\"new_name\": \"Free_School_Meals_6\"}, \n",
        "        \"extended_details_data_gifted_and_talented_status\": {\"new_name\": \"Gifted_And_Talented_Status\"}, \n",
        "        \"extended_details_data_in_lea_care\": {\"new_name\": \"In_LEA_Care\"}, \n",
        "        \"extended_details_data_premium_pupil_indicator\": {\"new_name\": \"Pupil_Premium_Indicator\"}, \n",
        "        \"extended_details_data_sen_status\": {\"new_name\": \"SEN_Status\"}, \n",
        "        \"extended_details_data_service_children_indicator\": {\"new_name\": \"Service_Child_Indicator\"}, \n",
        "        \"extended_details_data_child_in_need\": {\"new_name\": \"Child_In_Need\"}, \n",
        "        \"extended_details_data_child_protection_plan\": {\"new_name\": \"Child_Protection_Plan\"}, \n",
        "        \"extended_details_data_nationality\": {\"new_name\": \"Nationality\"}, \n",
        "        \"extended_details_data_premium_pupil_eligible\": {\"new_name\": \"Pupil_Premium_Eligible\"}, \n",
        "        \"id\": {\"new_name\": \"student_id\"},    \n",
        "        \"extended_details_data_leaver_destination\": {\"new_name\": \"Leaver_Destination\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"studentextendedkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"students_education.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"date_of_birth_date\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"education_details_data_admission_date_timezone\": \"drop\",\n",
        "        \"education_details_data_admission_date_timezone_type\": \"drop\",\n",
        "        \"education_details_data_learner_number\": \"drop\",\n",
        "        \"education_details_data_part_time\": \"drop\",\n",
        "        \"education_details_data_leaving_date_timezone\": \"drop\",\n",
        "        \"education_details_data_leaving_date_timezone_type\": \"drop\",\n",
        "        \"education_details_data_former_upn\": \"drop\",\n",
        "        \"education_details_data_local_upn\": \"drop\",\n",
        "        \"education_details_data_part_time_rate\": \"drop\",\n",
        "        \"forename\": \"drop\",\n",
        "        \"gender\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"legal_forename\": \"drop\",\n",
        "        \"legal_surname\": \"drop\",\n",
        "        \"middle_names\": \"drop\",\n",
        "        \"mis_id\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"surname\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        # Renames\n",
        "        \"education_details_data_admission_number\": {\"new_name\": \"Admission_Number\"}, \n",
        "        \"education_details_data_admission_date_date\": {\"new_name\": \"Admission_Date\"}, \n",
        "        \"education_details_data_current_nc_year\": {\"new_name\": \"Current_Year\"}, \n",
        "        \"education_details_data_upn\": {\"new_name\": \"UPN\"}, \n",
        "        \"education_details_data_leaving_date_date\": {\"new_name\": \"Leaving_Date\"}, \n",
        "        \"id\": {\"new_name\": \"student_id\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"studenteducationkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },    \n",
        "    \"students_leaver.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"created_at_date\": \"drop\",\n",
        "        \"created_at_timezone\": \"drop\",\n",
        "        \"created_at_timezone_type\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"updated_at_date\": \"drop\",\n",
        "        \"updated_at_timezone\": \"drop\",\n",
        "        \"updated_at_timezone_type\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        # Renames\n",
        "        \"date_of_birth_date\": {\"new_name\": \"Date_Of_Birth\"}, \n",
        "        \"forename\": {\"new_name\": \"Forename\"}, \n",
        "        \"gender\": {\"new_name\": \"Gender\"}, \n",
        "        \"id\": {\"new_name\": \"student_id\"}, \n",
        "        \"legal_forename\": {\"new_name\": \"Legal_Forename\"}, \n",
        "        \"legal_surname\": {\"new_name\": \"Legal_Surname\"}, \n",
        "        \"middle_names\": {\"new_name\": \"Middle_Names\"}, \n",
        "        \"surname\": {\"new_name\": \"Surname\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"students_leaver_extended.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"created_at_date\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"date_of_birth_date\": \"drop\",\n",
        "        \"forename\": \"drop\",\n",
        "        \"gender\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"legal_forename\": \"drop\",\n",
        "        \"legal_surname\": \"drop\",\n",
        "        \"middle_names\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"surname\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        \"mis_id\": \"drop\",\n",
        "        \"extended_details_data_fsm_review_date\": \"drop\",\n",
        "        \"extended_details_data_premium_pupil_notes\": \"drop\",\n",
        "        \"extended_details_data_boarding_status\": \"drop\",\n",
        "        \"extended_details_data_application_status\": \"drop\",\n",
        "        \"extended_details_data_birth_place\": \"drop\",\n",
        "        \"extended_details_data_custody_details\": \"drop\",\n",
        "        \"extended_details_data_dietary_needs\": \"drop\",\n",
        "        \"extended_details_data_general_notes\": \"drop\",\n",
        "        \"extended_details_data_marital_status\": \"drop\",\n",
        "        \"extended_details_data_in_care_details\": \"drop\",\n",
        "        \"extended_details_data_national_insurance_number\": \"drop\",\n",
        "        \"extended_details_data_paramedical_support\": \"drop\",\n",
        "        \"extended_details_data_permanent_resident\": \"drop\",\n",
        "        \"extended_details_data_religion\": \"drop\",\n",
        "        \"extended_details_data_responsible_care_authority\": \"drop\",\n",
        "        \"extended_details_data_youth_support_services_agreement\": \"drop\",\n",
        "        \"extended_details_data_custody_alert\": \"drop\",\n",
        "        \"extended_details_data_next_of_kin\": \"drop\",\n",
        "        # Renames\n",
        "        \"extended_details_data_english_as_additional_language\": {\"new_name\": \"English_As_Additional_Language\"}, \n",
        "        \"extended_details_data_english_as_additional_language_status\": {\"new_name\": \"English_As_Additional_Language_Status\"}, \n",
        "        \"extended_details_data_enrolment_status\": {\"new_name\": \"Enrolment_Status\"}, \n",
        "        \"extended_details_data_ethnicity\": {\"new_name\": \"Ethnicity\"}, \n",
        "        \"extended_details_data_ethnicity_code\": {\"new_name\": \"Ethnicity_Code\"}, \n",
        "        \"extended_details_data_ever_in_care\": {\"new_name\": \"Ever_In_Care\"}, \n",
        "        \"extended_details_data_first_language\": {\"new_name\": \"First_Language\"}, \n",
        "        \"extended_details_data_free_school_meals\": {\"new_name\": \"Free_School_Meals\"}, \n",
        "        \"extended_details_data_free_school_meals_6\": {\"new_name\": \"Free_School_Meals_6\"}, \n",
        "        \"extended_details_data_gifted_and_talented_status\": {\"new_name\": \"Gifted_And_Talented_Status\"}, \n",
        "        \"extended_details_data_in_lea_care\": {\"new_name\": \"In_LEA_Care\"}, \n",
        "        \"extended_details_data_premium_pupil_indicator\": {\"new_name\": \"Pupil_Premium_Indicator\"}, \n",
        "        \"extended_details_data_sen_status\": {\"new_name\": \"SEN_Status\"}, \n",
        "        \"extended_details_data_service_children_indicator\": {\"new_name\": \"Service_Child_Indicator\"}, \n",
        "        \"extended_details_data_child_in_need\": {\"new_name\": \"Child_In_Need\"}, \n",
        "        \"extended_details_data_child_protection_plan\": {\"new_name\": \"Child_Protection_Plan\"}, \n",
        "        \"extended_details_data_nationality\": {\"new_name\": \"Nationality\"}, \n",
        "        \"extended_details_data_premium_pupil_eligible\": {\"new_name\": \"Pupil_Premium_Eligible\"}, \n",
        "        \"id\": {\"new_name\": \"student_id\"},    \n",
        "        \"extended_details_data_leaver_destination\": {\"new_name\": \"Leaver_Destination\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"studentextendedkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"students_leaver_education.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"date_of_birth_date\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"education_details_data_admission_date_timezone\": \"drop\",\n",
        "        \"education_details_data_admission_date_timezone_type\": \"drop\",\n",
        "        \"education_details_data_learner_number\": \"drop\",\n",
        "        \"education_details_data_part_time\": \"drop\",\n",
        "        \"education_details_data_leaving_date_timezone\": \"drop\",\n",
        "        \"education_details_data_leaving_date_timezone_type\": \"drop\",\n",
        "        \"education_details_data_former_upn\": \"drop\",\n",
        "        \"education_details_data_local_upn\": \"drop\",\n",
        "        \"education_details_data_part_time_rate\": \"drop\",\n",
        "        \"forename\": \"drop\",\n",
        "        \"gender\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"legal_forename\": \"drop\",\n",
        "        \"legal_surname\": \"drop\",\n",
        "        \"middle_names\": \"drop\",\n",
        "        \"mis_id\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"surname\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        # Renames\n",
        "        \"education_details_data_admission_number\": {\"new_name\": \"Admission_Number\"}, \n",
        "        \"education_details_data_admission_date_date\": {\"new_name\": \"Admission_Date\"}, \n",
        "        \"education_details_data_current_nc_year\": {\"new_name\": \"Current_Year\"}, \n",
        "        \"education_details_data_upn\": {\"new_name\": \"UPN\"}, \n",
        "        \"education_details_data_leaving_date_date\": {\"new_name\": \"Leaving_Date\"}, \n",
        "        \"id\": {\"new_name\": \"student_id\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"studenteducationkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },    \n",
        "    \"students_contact_details.json\": {\n",
        "        # drops\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"contact_details_data_salutation\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_county\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_town\": \"drop\",     \n",
        "        \"contact_details_data_addresses_postal_country\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_district\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_street\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_apartment\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_house_name\": \"drop\",\n",
        "        \"contact_details_data_addresses_postal_house_number\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_postcode\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_country\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_county\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_town\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_district\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_street\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_apartment\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_house_name\": \"drop\",\n",
        "        \"contact_details_data_addresses_work_house_number\": \"drop\",\n",
        "        \"contact_details_data_emails_work\": \"drop\",\n",
        "        \"contact_details_data_emails_home\": \"drop\",\n",
        "        \"contact_details_data_emails_primary\": \"drop\",\n",
        "        \"contact_details_data_phones_mobile\": \"drop\",\n",
        "        \"contact_details_data_phones_work\": \"drop\",\n",
        "        \"contact_details_data_phones_home\": \"drop\",\n",
        "        \"contact_details_data_phones_primary\": \"drop\",\n",
        "        \"contact_details_data_phones_phone\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        \"created_at\": \"drop\",\n",
        "        \"restored_at\": \"drop\",\n",
        "        \"date_of_birth_timezone\": \"drop\",\n",
        "        \"date_of_birth_timezone_type\": \"drop\",\n",
        "        \"date_of_birth_date\": \"drop\",\n",
        "        \"gender_identity\": \"drop\",\n",
        "        \"gender\": \"drop\",\n",
        "        \"legal_forename\": \"drop\",\n",
        "        \"legal_surname\": \"drop\",\n",
        "        \"middle_names\": \"drop\",\n",
        "        \"forename\": \"drop\",\n",
        "        \"surname\": \"drop\",\n",
        "        \"title\": \"drop\",\n",
        "        \"initials\": \"drop\",\n",
        "        \"upi\": \"drop\",\n",
        "        # Renames\n",
        "        \"id\": {\"new_name\": \"student_id\"}, \n",
        "        \"contact_details_data_addresses_postal_postcode\": {\"new_name\": \"Postcode_Postal\"}, \n",
        "        \"contact_details_data_addresses_home_postcode\": {\"new_name\": \"Postcode_Home\"}, \n",
        "        \"contact_details_data_emails_email\": {\"new_name\": \"Email\"}, \n",
        "        \"contact_details_data_addresses_home_uprn\": {\"new_name\": \"UPRN\"}, \n",
        "        \"contact_details_data_addresses_home_country\": {\"new_name\": \"Country\"},\n",
        "        \"contact_details_data_addresses_home_county\": {\"new_name\": \"County\"},\n",
        "        \"contact_details_data_addresses_home_town\": {\"new_name\": \"Town\"},\n",
        "        \"contact_details_data_addresses_home_district\": {\"new_name\": \"District\"},\n",
        "        \"contact_details_data_addresses_home_street\": {\"new_name\": \"Street\"},\n",
        "        \"contact_details_data_addresses_home_apartment\": {\"new_name\": \"Apartment\"},\n",
        "        \"contact_details_data_addresses_home_house_name\": {\"new_name\": \"House_Name\"},\n",
        "        \"contact_details_data_addresses_home_house_number\": {\"new_name\": \"House_Number\"},\n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"addresskey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "            \"Address_Type\": \"Primary\",\n",
        "        }\n",
        "    },    \n",
        "    \"attendance_summaries.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"attendance_codes_#\": \"drop\",\n",
        "        \"attendance_codes_/\": \"drop\",\n",
        "        \"attendance_codes_B\": \"drop\",\n",
        "        \"attendance_codes_C\": \"drop\",\n",
        "        \"attendance_codes_D\": \"drop\",\n",
        "        \"attendance_codes_E\": \"drop\",\n",
        "        \"attendance_codes_F\": \"drop\",\n",
        "        \"attendance_codes_G\": \"drop\",\n",
        "        \"attendance_codes_H\": \"drop\",\n",
        "        \"attendance_codes_I\": \"drop\",\n",
        "        \"attendance_codes_J\": \"drop\",\n",
        "        \"attendance_codes_L\": \"drop\",\n",
        "        \"attendance_codes_M\": \"drop\",\n",
        "        \"attendance_codes_N\": \"drop\",\n",
        "        \"attendance_codes_O\": \"drop\",\n",
        "        \"attendance_codes_P\": \"drop\",\n",
        "        \"attendance_codes_R\": \"drop\",\n",
        "        \"attendance_codes_S\": \"drop\",\n",
        "        \"attendance_codes_T\": \"drop\",\n",
        "        \"attendance_codes_U\": \"drop\",\n",
        "        \"attendance_codes_V\": \"drop\",\n",
        "        \"attendance_codes_W\": \"drop\",\n",
        "        \"attendance_codes_X\": \"drop\",\n",
        "        \"attendance_codes_Y\": \"drop\",\n",
        "        \"attendance_codes_Z\": \"drop\",\n",
        "        \"attendance_codes_\\\\\": \"drop\",\n",
        "        # Renames\n",
        "        \"approved_education_activity\": {\"new_name\": \"Approved_Education_Activity\"}, \n",
        "        \"attendance_not_required\": {\"new_name\": \"Attendance_Not_Required\"}, \n",
        "        \"authorised_absences\": {\"new_name\": \"Authorised_Absences\"}, \n",
        "        \"id\": {\"new_name\": \"external_id\"}, \n",
        "        \"late_after_registration\": {\"new_name\": \"Late_After_Registration\"}, \n",
        "        \"late_before_registration\": {\"new_name\": \"Late_Before_Registration\"}, \n",
        "        \"missing_marks\": {\"new_name\": \"Missing_marks\"}, \n",
        "        \"possible_marks\": {\"new_name\": \"Possible_marks\"}, \n",
        "        \"present\": {\"new_name\": \"Present\"}, \n",
        "        \"unauthorized_absences\": {\"new_name\": \"Unauthorised_Absences\"}, \n",
        "        \"unexplained_absences\": {\"new_name\": \"Unexplained_Absences\"}, \n",
        "        \"updated_at\": {\"new_name\": \"last_updated\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"attendancesummarykey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "            \"Attendance_Mark_String\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"behaviours_students.json\": {\n",
        "        # drops\n",
        "        \"created_at_timezone\": \"drop\",\n",
        "        \"created_at_timezone_type\": \"drop\",\n",
        "        \"incident_date_timezone\": \"drop\",\n",
        "        \"incident_date_timezone_type\": \"drop\",\n",
        "        \"recorded_date_timezone\": \"drop\",\n",
        "        \"recorded_date_timezone_type\": \"drop\",\n",
        "        \"student_data_created_at_date\": \"drop\",\n",
        "        \"student_data_created_at_timezone\": \"drop\",\n",
        "        \"student_data_created_at_timezone_type\": \"drop\",\n",
        "        \"student_data_date_of_birth_date\": \"drop\",\n",
        "        \"student_data_date_of_birth_timezone\": \"drop\",\n",
        "        \"student_data_date_of_birth_timezone_type\": \"drop\",\n",
        "        \"student_data_forename\": \"drop\",\n",
        "        \"student_data_gender\": \"drop\",\n",
        "        \"student_data_initials\": \"drop\",\n",
        "        \"student_data_legal_forename\": \"drop\",\n",
        "        \"student_data_legal_surname\": \"drop\",\n",
        "        \"student_data_middle_names\": \"drop\",\n",
        "        \"student_data_restored_at_date\": \"drop\",\n",
        "        \"student_data_restored_at_timezone\": \"drop\",\n",
        "        \"student_data_restored_at_timezone_type\": \"drop\",\n",
        "        \"student_data_surname\": \"drop\",\n",
        "        \"student_data_updated_at_timezone\": \"drop\",\n",
        "        \"student_data_updated_at_timezone_type\": \"drop\",\n",
        "        \"student_data_meta_action\": \"drop\",\n",
        "        \"action\": \"drop\",\n",
        "        \"action_date_date\": \"drop\",\n",
        "        \"action_date_timezone\": \"drop\",\n",
        "        \"action_date_timezone_type\": \"drop\",\n",
        "        \"updated_at_timezone\": \"drop\",\n",
        "        \"updated_at_timezone_type\": \"drop\",\n",
        "        \"created_at_date\": \"drop\",\n",
        "        \"mis_id\": \"drop\",\n",
        "        \"parents_notified\": \"drop\",\n",
        "        \"recorded_date_date\": \"drop\",\n",
        "        \"student_data_meta_points\": \"drop\",\n",
        "        \"student_data_meta_role\": \"drop\",\n",
        "        \"student_data_mis_id\": \"drop\",\n",
        "        \"student_data_updated_at_date\": \"drop\",\n",
        "        \"student_data_upi\": \"drop\",\n",
        "        # Renames\n",
        "        \"student_data_id\": {\"new_name\": \"student_id\"}, \n",
        "        \"updated_at_date\": {\"new_name\": \"last_updated\"}, \n",
        "        \"class\": {\"new_name\": \"Class\"}, \n",
        "        \"comment\": {\"new_name\": \"Comment\"}, \n",
        "        \"id\": {\"new_name\": \"external_id\"}, \n",
        "        \"incident_date_date\": {\"new_name\": \"Incident_Date\"}, \n",
        "        \"location\": {\"new_name\": \"Location\"}, \n",
        "        \"total_points\": {\"new_name\": \"Total_Points\"}, \n",
        "        \"points\": {\"new_name\": \"Points\"}, \n",
        "        \"subject\": {\"new_name\": \"Subject\"}, \n",
        "        \"type\": {\"new_name\": \"Type\"}, \n",
        "        \"status\": {\"new_name\": \"Status\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"behaviourkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"exclusions.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"discipline_committee_date_date\": \"drop\",\n",
        "        \"discipline_committee_date_timezone\": \"drop\",\n",
        "        \"end_date_timezone\": \"drop\",\n",
        "        \"end_date_timezone_type\": \"drop\",\n",
        "        \"mis_id\": \"drop\",\n",
        "        \"start_date_timezone\": \"drop\",\n",
        "        \"start_date_timezone_type\": \"drop\",\n",
        "        \"student_data_created_at\": \"drop\",\n",
        "        \"student_data_date_of_birth_date\": \"drop\",\n",
        "        \"student_data_date_of_birth_timezone\": \"drop\",\n",
        "        \"student_data_date_of_birth_timezone_type\": \"drop\",\n",
        "        \"student_data_forename\": \"drop\",\n",
        "        \"student_data_gender\": \"drop\",\n",
        "        \"student_data_initials\": \"drop\",\n",
        "        \"student_data_legal_forename\": \"drop\",\n",
        "        \"student_data_legal_surname\": \"drop\",\n",
        "        \"student_data_mis_id\": \"drop\",\n",
        "        \"student_data_surname\": \"drop\",\n",
        "        \"student_data_updated_at\": \"drop\",\n",
        "        \"student_data_upi\": \"drop\",\n",
        "        \"student_data_restored_at_date\": \"drop\",\n",
        "        \"student_data_restored_at_timezone\": \"drop\",\n",
        "        \"student_data_restored_at_timezone_type\": \"drop\",\n",
        "        \"discipline_committee_date_timezone_type\": \"drop\",\n",
        "        \"discipline_committee_representation_made\": \"drop\",\n",
        "        \"student_data_middle_names\": \"drop\",\n",
        "        \"end_date\": \"drop\",\n",
        "        \"appeal_result_date_date\": \"drop\",\n",
        "        \"appeal_result_date_timezone\": \"drop\",\n",
        "        \"appeal_result_date_timezone_type\": \"drop\",\n",
        "        \"appeal_reinstatement_date\": \"drop\",\n",
        "        \"appeal_reinstatement_date_date\": \"drop\",\n",
        "        \"appeal_reinstatement_date_timezone\": \"drop\",\n",
        "        \"appeal_reinstatement_date_timezone_type\": \"drop\",\n",
        "        \"discipline_committee_date\": \"drop\",\n",
        "        \"discipline_committee_reinstatement_date\": \"drop\",\n",
        "        \"discipline_committee_reinstatement_date_date\": \"drop\",\n",
        "        \"discipline_committee_reinstatement_date_timezone\": \"drop\",\n",
        "        \"discipline_committee_reinstatement_date_timezone_type\": \"drop\",\n",
        "        \"discipline_committee_result\": \"drop\",\n",
        "        \"student_data_gender_identity\": \"drop\",\n",
        "        \"student_data_restored_at\": \"drop\",\n",
        "        \"student_data_title\": \"drop\",\n",
        "        #\"\": \"drop\",\n",
        "        # Renames\n",
        "        \"academic_year\": {\"new_name\": \"Academic_Year\"}, \n",
        "        \"agencies_involved\": {\"new_name\": \"Agencies_Involved\"}, \n",
        "        \"appeal_received\": {\"new_name\": \"Appeal_Received\"}, \n",
        "        \"appeal_result\": {\"new_name\": \"Appeal_Result\"},\n",
        "        \"appeal_result_date\": {\"new_name\": \"Appeal_Result_Date\"},\n",
        "        \"comments\": {\"new_name\": \"Comments\"}, \n",
        "        \"end_date_date\": {\"new_name\": \"End_Date\"}, \n",
        "        \"end_session\": {\"new_name\": \"End_Session\"}, \n",
        "        \"id\": {\"new_name\": \"external_id\"}, \n",
        "        \"days\": {\"new_name\": \"Days\"}, \n",
        "        \"reason\": {\"new_name\": \"Reason\"}, \n",
        "        \"reason_code\": {\"new_name\": \"Reason_Code\"}, \n",
        "        \"sessions\": {\"new_name\": \"Sessions\"}, \n",
        "        \"start_date_date\": {\"new_name\": \"Start_Date\"}, \n",
        "        \"start_session\": {\"new_name\": \"Start_Session\"}, \n",
        "        \"student_data_id\": {\"new_name\": \"student_id\"},  \n",
        "        \"term\": {\"new_name\": \"Term\"}, \n",
        "        \"type\": {\"new_name\": \"Type\"}, \n",
        "        \"type_code\": {\"new_name\": \"Type_Code\"}, \n",
        "        \"updated_at\": {\"new_name\": \"last_updated\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"exclusionkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"achievements_students.json\": {\n",
        "\t\t# drops\n",
        "\t\t\"achievement_date_timezone\": \"drop\",\n",
        "\t\t\"achievement_date_timezone_type\": \"drop\",\n",
        "\t\t\"created_at_date\": \"drop\",\n",
        "\t\t\"created_at_timezone\": \"drop\",\n",
        "\t\t\"created_at_timezone_type\": \"drop\",\n",
        "\t\t\"mis_id\": \"drop\",\n",
        "\t\t\"parents_notified\": \"drop\",\n",
        "\t\t\"recorded_date_date\": \"drop\",\n",
        "\t\t\"recorded_date_timezone\": \"drop\",\n",
        "\t\t\"recorded_date_timezone_type\": \"drop\",\n",
        "\t\t\"student_data_created_at_date\": \"drop\",\n",
        "\t\t\"student_data_created_at_timezone\": \"drop\",\n",
        "\t\t\"student_data_created_at_timezone_type\": \"drop\",\n",
        "\t\t\"student_data_date_of_birth_date\": \"drop\",\n",
        "\t\t\"student_data_date_of_birth_timezone\": \"drop\",\n",
        "\t\t\"student_data_date_of_birth_timezone_type\": \"drop\",\n",
        "\t\t\"student_data_forename\": \"drop\",\n",
        "\t\t\"student_data_gender\": \"drop\",\n",
        "\t\t\"student_data_initials\": \"drop\",\n",
        "\t\t\"student_data_legal_forename\": \"drop\",\n",
        "\t\t\"student_data_legal_surname\": \"drop\",\n",
        "\t\t\"student_data_meta_points\": \"drop\",\n",
        "\t\t\"student_data_middle_names\": \"drop\",\n",
        "\t\t\"student_data_mis_id\": \"drop\",\n",
        "\t\t\"student_data_restored_at_date\": \"drop\",\n",
        "\t\t\"student_data_restored_at_timezone\": \"drop\",\n",
        "\t\t\"student_data_restored_at_timezone_type\": \"drop\",\n",
        "\t\t\"student_data_surname\": \"drop\",\n",
        "\t\t\"student_data_updated_at_date\": \"drop\",\n",
        "\t\t\"student_data_updated_at_timezone\": \"drop\",\n",
        "\t\t\"student_data_updated_at_timezone_type\": \"drop\",\n",
        "\t\t\"student_data_upi\": \"drop\",\n",
        "\t\t\"updated_at_timezone\": \"drop\",\n",
        "\t\t\"updated_at_timezone_type\": \"drop\",\n",
        "\t\t# Renames\n",
        "\t\t\"achievement_date_date\": {\"new_name\": \"Achievement_Date\"}, \n",
        "\t\t\"class\": {\"new_name\": \"Class\"}, \n",
        "\t\t\"comment\": {\"new_name\": \"Comment\"}, \n",
        "\t\t\"id\": {\"new_name\": \"external_id\"}, \n",
        "\t\t\"points\": {\"new_name\": \"Points\"}, \n",
        "\t\t\"student_data_id\": {\"new_name\": \"student_id\"}, \n",
        "\t\t\"subject\": {\"new_name\": \"Subject\"}, \n",
        "\t\t\"total_points\": {\"new_name\": \"Total_Points\"}, \n",
        "\t\t\"type\": {\"new_name\": \"Type\"}, \n",
        "\t\t\"updated_at_date\": {\"new_name\": \"last_updated\"}, \n",
        "\t\t# adds\n",
        "\t\t\"add_columns\": {\n",
        "\t\t\"organisationkey\": \"\",\n",
        "\t\t\"achievementkey\": \"\",\n",
        "        \"studentkey\": \"\",\n",
        "\t\t}\n",
        "\t},\n",
        "    \"subjects.json\": {\n",
        "        # drops\n",
        "        \"created_at\":\"drop\",\n",
        "        \"updated_at\":\"drop\",\n",
        "        \"mis_id\":\"drop\",\n",
        "        # Renames\n",
        "        \"code\": {\"new_name\": \"Subject_Code\"},\n",
        "        \"id\": {\"new_name\": \"external_id\"}, \n",
        "        \"name\": {\"new_name\": \"Subject_Name\"}, \n",
        "        \"subject\": {\"new_name\": \"Subject\"}, \n",
        "        \"active\": {\"new_name\": \"Active\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"subjectkey\":\"\"\n",
        "        }\n",
        "    },\n",
        "    \"classes.json\": {\n",
        "        #drop\n",
        "        \"mis_id\":\"drop\",\n",
        "        \"created_at\":\"drop\",\n",
        "        \"updated_at\":\"drop\",\n",
        "        \"restored_at_date\":\"drop\",\n",
        "        \"restored_at_timezone\":\"drop\",\n",
        "        \"restored_at_timezone_type\":\"drop\",\n",
        "\n",
        "        # Renames\n",
        "        \"name\": {\"new_name\": \"Group_Name\"}, \n",
        "        \"description\": {\"new_name\": \"Group_Description\"}, \n",
        "        \"id\": {\"new_name\": \"external_id\"},\n",
        "        \"subject\": {\"new_name\": \"Subject_Id\"},\n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "             \"studentgroupkey\": \"\",\n",
        "             \"Group_Type\":\"\",\n",
        "             \"Group_Code\":\"\"\n",
        "        }\n",
        "    },\n",
        "    \"groups.json\": {\n",
        "        #drop\n",
        "        \"mis_id\":\"drop\",\n",
        "        \"created_at\":\"drop\",\n",
        "        \"updated_at\":\"drop\",\n",
        "        \"restored_at_date\":\"drop\",\n",
        "        \"restored_at_timezone\":\"drop\",\n",
        "        \"restored_at_timezone_type\":\"drop\",\n",
        "\n",
        "        # Renames\n",
        "        \"name\": {\"new_name\": \"Group_Name\"}, \n",
        "        \"code\": {\"new_name\": \"Group_Code\"}, \n",
        "        \"id\": {\"new_name\": \"external_id\"},\n",
        "        \"description\": {\"new_name\": \"Group_Description\"}, \n",
        "        \"type\": {\"new_name\": \"Group_Type\"},\n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "             \"studentgroupkey\": \"\",\n",
        "             \"Subject_Id\":\"\"\n",
        "        }\n",
        "    },\n",
        "    \"attendance_session.json\": {\n",
        "        # drops\n",
        "        \"date_timezone\": \"drop\",\n",
        "        \"date_timezone_type\": \"drop\",\n",
        "        # Renames\n",
        "        \"comment\": {\"new_name\": \"Comment\"}, \n",
        "        \"date_date\": {\"new_name\": \"Date\"}, \n",
        "        \"employee\": {\"new_name\": \"staff_id\"}, \n",
        "        \"id\": {\"new_name\": \"external_id\"}, \n",
        "        \"session\": {\"new_name\": \"Session\"}, \n",
        "        \"student\": {\"new_name\": \"student_id\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"attendancesessionkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"attendance_codes.json\": {\n",
        "        # drops\n",
        "        # Renames\n",
        "        \"code\": {\"new_name\": \"Mark\"}, \n",
        "        \"description\": {\"new_name\": \"Description\"}, \n",
        "        \"id\": {\"new_name\": \"attendance_code\"}, \n",
        "        \"type\": {\"new_name\": \"Type\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"attendancecodekey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"groups.json\": {\n",
        "        # drops\n",
        "        \"created_at\": \"drop\",\n",
        "        \"division\": \"drop\",\n",
        "        \"meta\": \"drop\",\n",
        "        \"notes\": \"drop\",\n",
        "        \"restored_at\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        # Renames\n",
        "        \"code\": {\"new_name\": \"Group_Code\"}, \n",
        "        \"name\": {\"new_name\": \"Group_Name\"}, \n",
        "        \"type\": {\"new_name\": \"Group_Type\"}, \n",
        "        \"description\": {\"new_name\": \"Group_Description\"}, \n",
        "        \"id\": {\"new_name\": \"Group_ID\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"groupkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "    \"group_membership.json\": {\n",
        "         # drops\n",
        "        \"created_at_date\": \"drop\",\n",
        "        \"created_at_timezone\": \"drop\",\n",
        "        \"created_at_timezone_type\": \"drop\",\n",
        "        \"division\": \"drop\",\n",
        "        \"meta\": \"drop\",\n",
        "        \"notes\": \"drop\",\n",
        "        \"restored_at\": \"drop\",\n",
        "        \"restored_at_date\": \"drop\",\n",
        "        \"restored_at_timezone\": \"drop\",\n",
        "        \"restored_at_timezone_type\": \"drop\",\n",
        "        \"updated_at\": \"drop\",\n",
        "        \"updated_at_timezone\": \"drop\",\n",
        "        \"updated_at_timezone_type\": \"drop\",\n",
        "        \"student_data_forename\": \"drop\",\n",
        "        \"student_data_gender\": \"drop\",\n",
        "        \"student_data_gender_identity\": \"drop\",\n",
        "        \"student_data_initials\": \"drop\",\n",
        "        \"student_data_legal_forename\": \"drop\",\n",
        "        \"student_data_legal_surname\": \"drop\",\n",
        "        \"student_data_middle_names\": \"drop\",\n",
        "        \"student_data_restored_at\": \"drop\",\n",
        "        \"student_data_surname\": \"drop\",\n",
        "        \"student_data_timezone\": \"drop\",\n",
        "        \"student_data_timezone_type\": \"drop\",\n",
        "        \"student_data_title\": \"drop\",\n",
        "        \"student_data_upi\": \"drop\",\n",
        "        \"updated_at_date\": \"drop\",\n",
        "        # Renames\n",
        "        \"code\": {\"new_name\": \"Group_Code\"}, \n",
        "        \"name\": {\"new_name\": \"Group_Name\"}, \n",
        "        \"type\": {\"new_name\": \"Group_Type\"}, \n",
        "        \"description\": {\"new_name\": \"Group_Description\"}, \n",
        "        \"id\": {\"new_name\": \"Group_ID\"}, \n",
        "        \"student_data_id\": {\"new_name\": \"student_id\"}, \n",
        "        \"student_data_date\": {\"new_name\": \"Student_Date\"}, \n",
        "        \"student_data_mis_id\": {\"new_name\": \"student_mis_id\"}, \n",
        "        # adds\n",
        "        \"add_columns\": {\n",
        "            \"organisationkey\": \"\",\n",
        "            \"groupmembershipkey\": \"\",\n",
        "            \"groupkey\": \"\",\n",
        "            \"studentkey\": \"\",\n",
        "        }\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to hold dataframes for each json file\n",
        "json_dfs = {}\n",
        "temp_dfs = {}\n",
        "all_columns = {}\n",
        "'''\n",
        "    This code is to loop through each directory and compile all the individual schools jsons into\n",
        "    a single json per endpoint.\n",
        "\n",
        "    It creates json_dfs - a dictionary of the aggregated json files\n",
        "'''\n",
        "for subdir in subdirectories:\n",
        "    school_dir = f\"{bronze_path}{subdir}/\"\n",
        "\n",
        "    # Consider only JSON files that are in your mapping\n",
        "    json_dirs = list(delta_table_name_mapping.keys())\n",
        "\n",
        "    #print(list(delta_table_name_mapping.keys()))\n",
        "    for json_dir in json_dirs:\n",
        "        json_dir_path = f\"{school_dir}{json_dir}/\"\n",
        "        try:\n",
        "            temp_df = spark.read.json(json_dir_path)\n",
        "            temp_df = temp_df.withColumn(\"school_id\", lit(subdir))\n",
        "\n",
        "            # Update the set of columns for the json_dir\n",
        "            all_columns.setdefault(json_dir, set()).update(temp_df.columns)\n",
        "\n",
        "            # Check if json_dir already exists in temp_dfs dictionary\n",
        "            if json_dir in temp_dfs:\n",
        "                # Align the schema of temp_df with existing DataFrame in temp_dfs\n",
        "                existing_columns = all_columns[json_dir]\n",
        "                temp_df = oeai.add_missing_columns(temp_df, existing_columns)\n",
        "                existing_df = oeai.add_missing_columns(temp_dfs[json_dir], temp_df.columns)\n",
        "                # Perform the union operation\n",
        "                try:\n",
        "                    temp_df = oeai.match_column_types(existing_df, temp_df)\n",
        "                    temp_dfs[json_dir] = existing_df[sorted(existing_df.columns)].unionByName(temp_df[sorted(temp_df.columns)])\n",
        "                except Exception as e:\n",
        "                    print(\"An unexpected error occurred:\", e)\n",
        "            else:\n",
        "                # If not, simply assign temp_df to temp_dfs[json_dir]\n",
        "                temp_dfs[json_dir] = temp_df\n",
        "        except AnalysisException as e:\n",
        "            print(f\"Path does not exist: {json_dir_path}, skipping...\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while processing {json_dir_path}: {e}\")\n",
        "            continue\n",
        "# Assign the final json_dfs outside the loops\n",
        "json_dfs = temp_dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_column_mappings(df, mappings):\n",
        "        \"\"\"\n",
        "        Applies various column mappings to a DataFrame such as dropping, renaming, \n",
        "        and adding columns with default values.\n",
        "\n",
        "        Args:\n",
        "            df (DataFrame): The DataFrame to be modified.\n",
        "            mappings (dict): A dictionary containing the mapping instructions. \n",
        "                             Keys are column names and values are actions or new names.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: The modified DataFrame after applying the mappings.\n",
        "        \"\"\"\n",
        "        # Drop columns\n",
        "        drop_cols = [col for col, action in mappings.items() if action == \"drop\"]\n",
        "        df = df.drop(*drop_cols)\n",
        "\n",
        "        # Rename columns or add new ones if they don't exist\n",
        "        #print(\"Existing columns before renaming:\", df.columns)\n",
        "        rename_mappings = {col: details['new_name'] for col, details in mappings.items()\n",
        "                        if isinstance(details, dict) and 'new_name' in details}\n",
        "        existing_columns = df.columns\n",
        "        for old_col, new_col in rename_mappings.items():\n",
        "            if old_col in existing_columns:\n",
        "                #print(f\"Renaming {old_col} to {new_col}\")\n",
        "                df = df.withColumnRenamed(old_col, new_col)\n",
        "            else:\n",
        "                #print(f\"Column {old_col} not found, adding {new_col} with None values\")\n",
        "                df = df.withColumn(new_col, lit(None))\n",
        "\n",
        "        # Add new columns with default values\n",
        "        add_columns = mappings.get(\"add_columns\", {})\n",
        "        for new_col, default_value in add_columns.items():\n",
        "            df = df.withColumn(new_col, lit(default_value))\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Assessment, Results and Resultset column processing\n",
        "\n",
        "#Refine columns in aspects json file\n",
        "\n",
        "# Assuming you have the DataFrame `json_dfs['aspects.json']`\n",
        "df = json_dfs['aspects.json']\n",
        "\n",
        "# Select the required columns and rename 'id' to 'aspect_id'\n",
        "try:\n",
        "    json_dfs['aspects.json'] = df.select(\n",
        "        df.created_at_date,\n",
        "        df.description,\n",
        "        df.id.alias(\"aspect_id\"),\n",
        "        df.max_value,\n",
        "        df.min_value,\n",
        "        df.mis_id,\n",
        "        df.name,\n",
        "        df.school_id,\n",
        "        df.type,\n",
        "        df.unique_key,\n",
        "        df.updated_at_date\n",
        "    )\n",
        "except AnalysisException as e:\n",
        "    # Handle the case where the table does not exist or other AnalysisExceptions\n",
        "    print(f\"AnalysisException: {e}\")\n",
        "    print(\"Table is empty, skipping\")\n",
        "except Exception as e:\n",
        "    # Handle other exceptions\n",
        "    print(f\"Exception: {e}\")\n",
        "    print(\"Table is empty, skipping\")\n",
        "\n",
        "#Refine columns in results json file\n",
        "\n",
        "\n",
        "# Assuming you have the DataFrame `json_dfs['results.json']`\n",
        "df_results = json_dfs['results.json']\n",
        "\n",
        "# Select the required columns and rename specified columns\n",
        "try:\n",
        "    json_dfs['results.json'] = df_results.select(\n",
        "        df_results.aspect.alias(\"aspect_id\"),\n",
        "        df_results.collection_date_date,\n",
        "        df_results.created_at_date,\n",
        "        df_results.grade_value,\n",
        "        df_results.id.alias(\"result_id\"),\n",
        "        df_results.mis_id,\n",
        "        df_results.result,\n",
        "        df_results.result_date_date.alias(\"result_date\"),\n",
        "        df_results.resultset.alias(\"resultset_id\"),\n",
        "        df_results.school_id,\n",
        "        df_results.student.alias(\"student_id\"),\n",
        "        df_results.unique_key,\n",
        "        df_results.updated_at_date\n",
        "    ).withColumn(\"organisationkey\", lit(\"\")).withColumn(\"studentkey\", lit(\"\"))\n",
        "\n",
        "except AnalysisException as e:\n",
        "    # Handle the case where the table does not exist or other AnalysisExceptions\n",
        "    print(f\"AnalysisException: {e}\")\n",
        "    print(\"Table is empty, skipping\")\n",
        "except Exception as e:\n",
        "    # Handle other exceptions\n",
        "    print(f\"Exception: {e}\")\n",
        "    print(\"Table is empty, skipping\")\n",
        "\n",
        "# Assuming you have the DataFrame `json_dfs['resultsets.json']`\n",
        "df_resultsets = json_dfs['resultsets.json']\n",
        "\n",
        "# Select the required columns and rename 'id' to 'resultset_id'\n",
        "try:\n",
        "    json_dfs['resultsets.json'] = df_resultsets.select(\n",
        "        df_resultsets.created_at_date,\n",
        "        df_resultsets.end_date,\n",
        "        df_resultsets.end_date_date,\n",
        "        df_resultsets.external_id,\n",
        "        df_resultsets.id.alias(\"resultset_id\"),\n",
        "        df_resultsets.locked,\n",
        "        df_resultsets.mis_id,\n",
        "        df_resultsets.module,\n",
        "        df_resultsets.name,\n",
        "        df_resultsets.school_id,\n",
        "        df_resultsets.source,\n",
        "        df_resultsets.start_date,\n",
        "        df_resultsets.start_date_date,\n",
        "        df_resultsets.supplier,\n",
        "        df_resultsets.unique_key,\n",
        "        df_resultsets.updated_at_date\n",
        "    )\n",
        "except AnalysisException as e:\n",
        "    # Handle the case where the table does not exist or other AnalysisExceptions\n",
        "    print(f\"AnalysisException: {e}\")\n",
        "    print(\"Table is empty, skipping\")\n",
        "except Exception as e:\n",
        "    # Handle other exceptions\n",
        "    print(f\"Exception: {e}\")\n",
        "    print(\"Table is empty, skipping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for json_name, df in json_dfs.items():\n",
        "    if json_name in column_mappings:\n",
        "        df = apply_column_mappings(df, column_mappings[json_name])\n",
        "        json_dfs[json_name] = df  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of jobs next to get the dimensions in the correct schema.\n",
        "\n",
        "'''\n",
        "***********************************************\n",
        "  STUDENTS\n",
        "**********************************************\n",
        "'''\n",
        "try:\n",
        "    if json_dfs['students.json'].count() > 0:\n",
        "        try:\n",
        "            df_student = json_dfs['students.json']\n",
        "            df_student_education = json_dfs['students_education.json']\n",
        "\n",
        "            # Join students and student_education dataframes\n",
        "            df_joined = df_student.join(\n",
        "                df_student_education.select('unique_key', 'UPN', 'Current_Year'),\n",
        "                on='unique_key',  \n",
        "                how='inner'  \n",
        "            )\n",
        "            json_dfs['students.json'] = df_joined\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        try:\n",
        "            df_student_extended = json_dfs['students_extended.json']\n",
        "            df_student_education = json_dfs['students_education.json']\n",
        "            df_student_contact_details = json_dfs['students_contact_details.json']\n",
        "\n",
        "            # Join students_extended and student_education dataframes\n",
        "            df_joined = df_student_extended.join(\n",
        "                df_student_education.select('unique_key', 'Admission_Date', 'Leaving_Date', 'Admission_Number'),\n",
        "                on='unique_key',\n",
        "                how='inner'\n",
        "            )\n",
        "            \n",
        "            if 'Leaving_Date' not in df_joined.columns:\n",
        "                df_joined = df_joined.withColumn('Leaving_Date', lit(None))\n",
        "\n",
        "            json_dfs['students_extended.json'] = df_joined\n",
        "\n",
        "            try:\n",
        "                # Join df_joined with students_contact_details dataframe\n",
        "                df_joined = df_joined.join(\n",
        "                    df_student_contact_details.select('unique_key', 'Postcode_Postal', 'Postcode_Home', 'Email'),\n",
        "                    on='unique_key',\n",
        "                    how='left'\n",
        "                )\n",
        "            \n",
        "                if 'Postcode' not in df_joined.columns:\n",
        "                    df_joined = df_joined.withColumn('Postcode', lit(None))\n",
        "\n",
        "                if 'Postcode_Home' not in df_joined.columns:\n",
        "                    df_joined = df_joined.withColumn('Postcode_Home', lit(None))\n",
        "\n",
        "                json_dfs['students_extended.json'] = df_joined\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"DataFrame is empty, skipping the operation.\")\n",
        "\n",
        "    # get the Year_Group\n",
        "    try:\n",
        "    # Assuming data is loaded correctly into these dataframes\n",
        "        df_student = json_dfs['students.json']\n",
        "        df_groupmembership = json_dfs['group_membership.json']\n",
        "\n",
        "        # Join operation using both school_id and student_id, adding Year_Group\n",
        "        df_joined = df_student.alias(\"student\").join(\n",
        "            df_groupmembership.alias(\"groupmembership\"),\n",
        "            (col(\"student.school_id\") == col(\"groupmembership.school_id\")) &\n",
        "            (col(\"student.student_id\") == col(\"groupmembership.student_id\")) &\n",
        "            (col(\"groupmembership.Group_Type\") == \"YEAR\"),\n",
        "            \"left\"\n",
        "        ).select(\n",
        "            \"student.*\",  # Select all columns from df_student\n",
        "            col(\"groupmembership.Group_Name\").alias(\"Year_Group\")  # Rename and select the Group_Name as Year_Group\n",
        "        )\n",
        "\n",
        "        # Update the original dataframe in your dictionary\n",
        "        json_dfs['students.json'] = df_joined\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\") \n",
        "\n",
        "'''\n",
        "***********************************************\n",
        "  STUDENTS_LEAVER\n",
        "**********************************************\n",
        "'''\n",
        "try:\n",
        "    if json_dfs['students_leaver.json'].count() > 0:\n",
        "        try:\n",
        "            df_student = json_dfs['students_leaver.json']\n",
        "            df_student_education = json_dfs['students_leaver_education.json']\n",
        "\n",
        "            # Join students and student_education dataframes\n",
        "            df_joined = df_student.join(\n",
        "                df_student_education.select('unique_key', 'UPN', 'Current_Year'),\n",
        "                on='unique_key',  \n",
        "                how='inner'  \n",
        "            )\n",
        "            json_dfs['students_leaver.json'] = df_joined\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        try:\n",
        "            df_student_leaver_extended = json_dfs['students_leaver_extended.json']\n",
        "            df_student_leaver_education = json_dfs['students_leaver_education.json']\n",
        "\n",
        "            # Join students_extended and student_education dataframes\n",
        "            df_joined = df_student_leaver_extended.join(\n",
        "                df_student_leaver_education.select('unique_key', 'Admission_Date', 'Leaving_Date', 'Admission_Number'),\n",
        "                on='unique_key',\n",
        "                how='inner'\n",
        "            )\n",
        "            \n",
        "            if 'Leaving_Date' not in df_joined.columns:\n",
        "                df_joined = df_joined.withColumn('Leaving_Date', lit(None))\n",
        "\n",
        "            json_dfs['students_leaver_extended.json'] = df_joined\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"DataFrame is empty, skipping the operation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\") \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "***********************************************\n",
        "  ATTENDANCE SUMMARY\n",
        "***********************************************\n",
        "'''\n",
        "try:\n",
        "    from pyspark.sql import functions as F\n",
        "    if json_dfs['attendance_summaries.json'].count() > 0:\n",
        "        try:\n",
        "            df_attendance_summary = json_dfs['attendance_summaries.json']\n",
        "\n",
        "            calculated_sum = (\n",
        "                coalesce(F.col('Authorised_Absences'), F.lit(0)) +\n",
        "                coalesce(F.col('Unauthorised_Absences'), F.lit(0)) +\n",
        "                coalesce(F.col('Attendance_Not_Required'), F.lit(0)) +\n",
        "                coalesce(F.col('Present'), F.lit(0)) +\n",
        "                coalesce(F.col('Approved_Education_Activity'), F.lit(0))\n",
        "            )\n",
        "\n",
        "            # Replace 'null' in Possible_marks with the sum of the other columns\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"Possible_marks\",\n",
        "                when((col(\"Possible_marks\").isNull()) | (col(\"Possible_marks\") == \"null\") | (col(\"Possible_marks\") == \"None\"), calculated_sum)\n",
        "                .otherwise(col(\"Possible_marks\"))\n",
        "            )\n",
        "\n",
        "            # Replace string 'null' with 0 in Possible_marks\n",
        "            df_attendance_summary = df_attendance_summary.fillna({'Possible_marks': 0})\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"Possible_marks\",\n",
        "                when(df_attendance_summary[\"Possible_marks\"] == \"null\", 0)\n",
        "                .otherwise(df_attendance_summary[\"Possible_marks\"])\n",
        "            )\n",
        "\n",
        "            # Convert string columns to integer\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Present\", col(\"Present\").cast(\"int\"))\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Approved_Education_Activity\", col(\"Approved_Education_Activity\").cast(\"int\"))\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Possible_marks\", col(\"Possible_marks\").cast(\"int\"))\n",
        "\n",
        "            # Calculate Percentage_Attendance and format to two decimal places\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Percentage_Attendance\", \n",
        "                            ((col(\"Present\") + col(\"Approved_Education_Activity\")) / col(\"Possible_marks\")).cast(DecimalType(10, 4)))\n",
        "            # Calculate Percentage_Auth and format to two decimal places\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Percentage_Authorised_Absence\", \n",
        "                            ((col(\"Authorised_Absences\")) / col(\"Possible_marks\")).cast(DecimalType(10, 4)))\n",
        "            # Calculate Percentage_UnAuth and format to two decimal places\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Percentage_Unauthorised_Absence\", \n",
        "                            ((col(\"Unauthorised_Absences\")) / col(\"Possible_marks\")).cast(DecimalType(10, 4)))\n",
        "            # Calculate Percentage_Unexp and format to two decimal places\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\"Percentage_Unexplained_Absence\", \n",
        "                            ((col(\"Unexplained_Absences\")) / col(\"Possible_marks\")).cast(DecimalType(10, 4)))\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"Is_Persistently_Absent\",\n",
        "                when(col(\"Percentage_Attendance\") < 0.9, 1).otherwise(0)\n",
        "            )                   \n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"Is_Severely_Absent\",\n",
        "                when(col(\"Percentage_Attendance\") < 0.5, 1).otherwise(0)\n",
        "            )      \n",
        "\n",
        "            #  attendance bands\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"under_50\",\n",
        "                when(col(\"Percentage_Attendance\") < 0.5, 1).otherwise(0)\n",
        "            ) \n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"50_to_70\",\n",
        "                when((col(\"Percentage_Attendance\") >= 0.5) & (col(\"Percentage_Attendance\") < 0.7), 1).otherwise(0)\n",
        "            )\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"70_to_80\",\n",
        "                when((col(\"Percentage_Attendance\") >= 0.7) & (col(\"Percentage_Attendance\") < 0.8), 1).otherwise(0)\n",
        "            )\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"80_to_90\",\n",
        "                when((col(\"Percentage_Attendance\") >= 0.8) & (col(\"Percentage_Attendance\") < 0.9), 1).otherwise(0)\n",
        "            )\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"90_to_92\",\n",
        "                when((col(\"Percentage_Attendance\") >= 0.9) & (col(\"Percentage_Attendance\") < 0.92), 1).otherwise(0)\n",
        "            )\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"92_to_95\",\n",
        "                when((col(\"Percentage_Attendance\") >= 0.92) & (col(\"Percentage_Attendance\") < 0.95), 1).otherwise(0)\n",
        "            )\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"95_to_98\",\n",
        "                when((col(\"Percentage_Attendance\") >= 0.95) & (col(\"Percentage_Attendance\") < 0.98), 1).otherwise(0)\n",
        "            )\n",
        "\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"above_98\",\n",
        "                when(col(\"Percentage_Attendance\") >= 0.98, 1).otherwise(0)\n",
        "            )             \n",
        "\n",
        "            # Create a single 'Attendance_Bin' column\n",
        "            df_attendance_summary = df_attendance_summary.withColumn(\n",
        "                \"Attendance_Bin\",\n",
        "                when(col(\"Percentage_Attendance\") < 0.5, \"under 50%\")\n",
        "                .when((col(\"Percentage_Attendance\") >= 0.5) & (col(\"Percentage_Attendance\") < 0.7), \"50% to 70%\")\n",
        "                .when((col(\"Percentage_Attendance\") >= 0.7) & (col(\"Percentage_Attendance\") < 0.8), \"70% to 80%\")\n",
        "                .when((col(\"Percentage_Attendance\") >= 0.8) & (col(\"Percentage_Attendance\") < 0.9), \"80% to 90%\")\n",
        "                .when((col(\"Percentage_Attendance\") >= 0.9) & (col(\"Percentage_Attendance\") < 0.92), \"90% to 92%\")\n",
        "                .when((col(\"Percentage_Attendance\") >= 0.92) & (col(\"Percentage_Attendance\") < 0.95), \"92% to 95%\")\n",
        "                .when((col(\"Percentage_Attendance\") >= 0.95) & (col(\"Percentage_Attendance\") < 0.98), \"95% to 98%\")\n",
        "                .when(col(\"Percentage_Attendance\") >= 0.98, \"above 98%\")\n",
        "                .otherwise(\"Unspecified\")  # This is optional and can handle any data outside the expected ranges\n",
        "            )\n",
        "\n",
        "\n",
        "            json_dfs['attendance_summaries.json'] = df_attendance_summary\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")  \n",
        "    else:\n",
        "        print(\"DataFrame is empty, skipping the operation.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\") \n",
        "\n",
        "\n",
        "'''\n",
        "**************************************************\n",
        "    dim_Address\n",
        "**************************************************\n",
        "'''\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "try:\n",
        "\n",
        "    def filter_address(*args):\n",
        "        return ', '.join(filter(lambda x: x and x != \"None\", args))\n",
        "\n",
        "    # Register UDF\n",
        "    filter_address_udf = udf(filter_address, StringType())\n",
        "\n",
        "    if json_dfs['students_contact_details.json'].count() > 0:\n",
        "        try:\n",
        "            df = json_dfs['students_contact_details.json']\n",
        "\n",
        "            # Apply UDF to create the full address column\n",
        "            df = df.withColumn(\"Address_Block\", filter_address_udf(\n",
        "                col(\"House_Name\"),\n",
        "                col(\"House_Number\"),\n",
        "                col(\"Apartment\"),\n",
        "                col(\"Street\"),\n",
        "                col(\"District\"),\n",
        "                col(\"Town\"),\n",
        "                col(\"County\"),\n",
        "                col(\"Country\"),\n",
        "                col(\"Postcode_Home\")\n",
        "            ))\n",
        "\n",
        "            if 'Postcode' not in df.columns:\n",
        "                df = df.withColumn('Postcode', lit(None))\n",
        "\n",
        "            json_dfs['students_contact_details.json'] = df\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")  \n",
        "    else:\n",
        "        print(\"DataFrame is empty, skipping the operation.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")  \n",
        "\n",
        "\n",
        "'''\n",
        "**************************************************\n",
        "    attendance_session\n",
        "**************************************************\n",
        "'''\n",
        "from pyspark.sql.functions import to_date\n",
        "try:\n",
        "    if json_dfs['attendance_session.json'].count() > 0:\n",
        "        try:\n",
        "            df_joined = []\n",
        "            df_session = json_dfs['attendance_session.json']\n",
        "            df_codes = json_dfs['attendance_codes.json']\n",
        "\n",
        "            df_joined = df_session.join(\n",
        "                df_codes.select('attendance_code', 'school_id', 'Mark'),\n",
        "                on=['attendance_code', 'school_id'],  # column name to join on, which must be present in both DataFrames\n",
        "                how='inner'  # you can also use 'left', 'right', or 'outer' as needed\n",
        "            )\n",
        "\n",
        "            df_joined = df_joined.drop('attendance_code')\n",
        "            #df_joined = df_joined.drop('school_id')\n",
        "            df_joined = df_joined.drop('Comment')\n",
        "            df_joined = df_joined.drop('staff_id')\n",
        "            df_joined = df_joined.drop('external_id')\n",
        "            #df_joined = df_joined.drop('student_id')\n",
        "            #df_joined = df_joined.drop('unique_key')\n",
        "            df_joined = df_joined.drop('organisationkey')\n",
        "\n",
        "            df_joined = df_joined.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss.SSSSSS\"))\n",
        "\n",
        "            json_dfs['attendance_session.json'] = df_joined\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")  \n",
        "    else:\n",
        "        print(\"DataFrame is empty, skipping the operation.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "***  student union with leavers ***\n",
        "'''\n",
        "from pyspark.sql.functions import col\n",
        "# Identify columns in source not in target\n",
        "df = json_dfs['students.json']\n",
        "target_df = json_dfs['students_leaver.json']\n",
        "\n",
        "# New columns in source that are not in target\n",
        "new_columns_in_source = set(df.columns) - set(target_df.columns)\n",
        "# New columns in target that are not in source\n",
        "new_columns_in_target = set(target_df.columns) - set(df.columns)\n",
        "\n",
        "# Add new columns with nulls to the target DataFrame\n",
        "for col in new_columns_in_source:\n",
        "    target_df = target_df.withColumn(col, lit(None).cast(df.schema[col].dataType))\n",
        "\n",
        "# Add new columns with nulls to the source DataFrame\n",
        "for col in new_columns_in_target:\n",
        "    df = df.withColumn(col, lit(None).cast(target_df.schema[col].dataType))\n",
        "\n",
        "# Ensure the columns are in the same order for both DataFrames\n",
        "df = df.select(sorted(df.columns))\n",
        "target_df = target_df.select(sorted(target_df.columns))\n",
        "\n",
        "# Now perform the union operation\n",
        "combined_df_by_name = df.unionByName(target_df)\n",
        "\n",
        "# Update the dictionary with the combined DataFrame\n",
        "json_dfs['students.json'] = combined_df_by_name\n",
        "\n",
        "json_dfs['students.json'] = json_dfs['students.json'].dropDuplicates(['unique_key'])\n",
        "\n",
        "'''\n",
        "***  student_extended ***\n",
        "'''\n",
        "\n",
        "# Accessing the dataframes\n",
        "df2 = json_dfs['students_extended.json']\n",
        "target_df2 = json_dfs['students_leaver_extended.json']\n",
        "\n",
        "# New columns in source that are not in target\n",
        "new_columns2 = set(df2.columns) - set(target_df2.columns)\n",
        "# New columns in target that are not in source\n",
        "new_columns_in_target2 = set(target_df2.columns) - set(df2.columns)\n",
        "\n",
        "# Drop the 'date_of_birth' column from the target DataFrame if it exists\n",
        "#if 'date_of_birth' in target_df2.columns:\n",
        "#    target_df2 = target_df2.drop('date_of_birth')\n",
        "\n",
        "# Add new columns with nulls to the target DataFrame\n",
        "for col in new_columns2:\n",
        "    target_df2 = target_df2.withColumn(col, lit(None).cast(df2.schema[col].dataType))\n",
        "\n",
        "# Add new columns with nulls to the source DataFrame\n",
        "for col in new_columns_in_target2:\n",
        "    df2 = df2.withColumn(col, lit(None).cast(target_df2.schema[col].dataType))\n",
        "\n",
        "# Ensure the columns are in the same order for both DataFrames\n",
        "df2 = df2.select(sorted(df2.columns))\n",
        "target_df2 = target_df2.select(sorted(target_df2.columns))\n",
        "\n",
        "# Now perform the union operation\n",
        "combined_df_by_name2 = df2.unionByName(target_df2)\n",
        "\n",
        "# Update the dictionary with the combined DataFrame\n",
        "json_dfs['students_extended.json'] = combined_df_by_name2\n",
        "\n",
        "json_dfs['students_extended.json'] = json_dfs['students_extended.json'].dropDuplicates(['unique_key'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_missing_columns(df_to_adjust, df_reference):\n",
        "    missing_columns = set(df_reference.columns) - set(df_to_adjust.columns)\n",
        "    for column in missing_columns:\n",
        "        df_to_adjust = df_to_adjust.withColumn(column, lit(None).cast(df_reference.schema[column].dataType))\n",
        "    return df_to_adjust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "# Process each DataFrame and upsert it to the silver_path\n",
        "for json_name, df in json_dfs.items():\n",
        "    \n",
        "    if json_name in delta_table_name_mapping and delta_table_name_mapping[json_name] != \"\":\n",
        "        if df.count() > 0:\n",
        "            # Get the Delta table name from the mapping\n",
        "            delta_table_name = delta_table_name_mapping[json_name]\n",
        "            silver_table_path = f\"{silver_path}/{delta_table_name}\"\n",
        "            uuid_column_name = oeai.get_uuid_column_name(delta_table_name)\n",
        "            print(delta_table_name)\n",
        "            # Define the unique key column name\n",
        "            unique_key_column = \"unique_key\"  \n",
        "\n",
        "            if delta_table_name == \"dim_Organisation\":\n",
        "                if DeltaTable.isDeltaTable(spark, silver_table_path):\n",
        "                    delta_table = DeltaTable.forPath(spark, silver_table_path)\n",
        "                    \n",
        "                    # Alias the Delta table as 'target' and rename 'organisationkey' to 'target_organisationkey'\n",
        "                    target_df = delta_table.toDF().select(unique_key_column, col(\"organisationkey\").alias(\"target_organisationkey\"))\n",
        "                    \n",
        "                    # Alias the source DataFrame as 'source'\n",
        "                    source_df = df.alias(\"source\")\n",
        "                    \n",
        "                    # Perform a left join to find non-matched records\n",
        "                    df_with_keys = source_df.join(\n",
        "                        target_df,\n",
        "                        source_df[unique_key_column] == target_df[unique_key_column],\n",
        "                        how=\"left\"\n",
        "                    ).select(\n",
        "                        # Select all columns from 'source' EXCEPT 'organisationkey' if it exists\n",
        "                        *[source_df[col].alias(col) for col in source_df.columns if col != \"organisationkey\"],\n",
        "                        # Coalesce to get 'organisationkey' from 'target' if it exists, or generate a new one\n",
        "                        coalesce(col(\"target_organisationkey\"), expr(\"uuid()\")).alias(\"organisationkey\")\n",
        "                    )\n",
        "                    \n",
        "                    # Now perform the merge operation\n",
        "                    delta_table.alias(\"target\").merge(\n",
        "                        df_with_keys.alias(\"source\"),\n",
        "                        f\"target.{unique_key_column} = source.{unique_key_column}\"\n",
        "                    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "                    \n",
        "                else:\n",
        "                    # If the table does not exist, create it by writing the current DataFrame\n",
        "                    # First, add a column for the organisationkey for all records since this is a new table\n",
        "                    df = df.withColumn(\"organisationkey\", expr(\"uuid()\"))\n",
        "                    df.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)\n",
        "            else:\n",
        "                # Process student table before all others:\n",
        "                if delta_table_name == \"dim_Student\":\n",
        "                    if DeltaTable.isDeltaTable(spark, silver_table_path):\n",
        "                        delta_table = DeltaTable.forPath(spark, silver_table_path)\n",
        "                        try:\n",
        "                            # ------------------------------\n",
        "                            # First, get the organisationkey\n",
        "                            # ------------------------------\n",
        "                            # Read the dim_Organisation table\n",
        "                            dim_org_df = spark.read.format(\"delta\").load(f\"{silver_path}/dim_Organisation\").select(\"external_id\", \"organisationkey\")\n",
        "                            \n",
        "                            df = df.drop(\"organisationkey\")\n",
        "\n",
        "                            # Perform a left join to find existing organisation keys\n",
        "                            df_joined = df.alias(\"source\").join(\n",
        "                                dim_org_df.alias(\"dim\"),\n",
        "                                col(\"source.school_id\") == col(\"dim.external_id\"),\n",
        "                                \"left\"\n",
        "                            )\n",
        "\n",
        "                            # df_joined = df.drop(\"external_id\")\n",
        "\n",
        "                            # Select all columns from df (source) and only the 'organisationkey' from dim_Organisation (dim)\n",
        "                            # Alias the dim_Organisation's organisationkey to avoid ambiguity\n",
        "                            \n",
        "                            df_with_orgkey = df_joined.select(\n",
        "                                *[col(f\"source.{col_name}\") for col_name in df.columns],\n",
        "                                col(\"dim.organisationkey\")\n",
        "                            )\n",
        "\n",
        "                            # ------------------------------\n",
        "                            # Second, update or insert the record\n",
        "                            # ------------------------------\n",
        "                            # generate a studentkey, this will be ignored if the record matches on the uniquekey column but inserted if it is a new record\n",
        "                            df_with_orgkey = df_with_orgkey.withColumn(uuid_column_name, expr(\"uuid()\"))\n",
        "\n",
        "                            target_columns = DeltaTable.forPath(spark, silver_table_path).toDF().columns\n",
        "                            # Filter source DataFrame columns based on target DataFrame columns\n",
        "                            source_columns = df_with_orgkey.columns\n",
        "                            columns_to_keep = [col for col in source_columns if col in target_columns]\n",
        "                            # Select only the matching columns from the source DataFrame\n",
        "                            df_adjusted = df_with_orgkey.select(*columns_to_keep)\n",
        "\n",
        "                            # Ensure update_columns only contains columns present in both DataFrames and not in the exclusion list\n",
        "                            update_columns = {col: f\"source.{col}\" for col in df_adjusted.columns if col not in ['organisationkey', uuid_column_name]}\n",
        "                            \n",
        "                            delta_table.alias(\"target\").merge(\n",
        "                                df_adjusted.alias(\"source\"),\n",
        "                                f\"target.{unique_key_column} = source.{unique_key_column}\"\n",
        "                            ).whenMatchedUpdate(set=update_columns\n",
        "                            ).whenNotMatchedInsertAll().execute()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(delta_table_name)\n",
        "                            df.printSchema()\n",
        "                            print(e)\n",
        "                    else:\n",
        "                        # If the table does not exist, create it by writing the current DataFrame\n",
        "                        df = df.withColumn(uuid_column_name, expr(\"uuid()\"))\n",
        "                        #df.printSchema()\n",
        "                        # Load the dim_Organisation table to get the existing mappings\n",
        "                        dim_org_df = spark.read.format(\"delta\").load(f\"{silver_path}/dim_Organisation\").select(\"external_id\", \"organisationkey\")\n",
        "                        # Perform a left join to find existing organisation keys\n",
        "                        df_joined = df.alias(\"source\").join(\n",
        "                            dim_org_df.alias(\"dim\"),\n",
        "                            col(\"source.school_id\") == col(\"dim.external_id\"),\n",
        "                            \"left\"\n",
        "                        )\n",
        "                        # Select all columns from df and only the 'organisationkey' from the dim_Organisation\n",
        "                        df_with_keys = df_joined.select(\"source.*\", col(\"dim.organisationkey\").alias(\"dim_organisationkey\"))\n",
        "                        #df_with_keys.printSchema()\n",
        "                        # Fill in the missing keys with UUIDs\n",
        "                        df_complete = df_with_keys.withColumn(\n",
        "                            \"organisationkey\",\n",
        "                            when(col(\"dim_organisationkey\").isNull(), expr(\"uuid()\")).otherwise(col(\"dim_organisationkey\"))\n",
        "                        )\n",
        "                        # Drop the 'dim_organisationkey' as it is no longer needed\n",
        "                        df_final = df_complete.drop(\"dim_organisationkey\")\n",
        "                        #df_final.printSchema()\n",
        "                        df_final.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)\n",
        "                else: # if not the organisation or student table         \n",
        "                    \n",
        "                    if ('studentkey' in df.columns) and (delta_table_name != \"dim_Student\"):\n",
        "                        # ------------------------------\n",
        "                        # First, get the organisationkey\n",
        "                        # ------------------------------\n",
        "                        # Read the dim_Organisation table\n",
        "                        dim_org_df = spark.read.format(\"delta\").load(f\"{silver_path}/dim_Organisation\").select(\"external_id\", \"organisationkey\")\n",
        "                        \n",
        "                        df = df.drop(\"organisationkey\")\n",
        "\n",
        "                        # Perform a left join to find existing organisation keys\n",
        "                        from pyspark.sql.functions import col\n",
        "                        \n",
        "                        df_joined = df.alias(\"source\").join(\n",
        "                            dim_org_df.alias(\"dim\"),\n",
        "                            col(\"source.school_id\") == col(\"dim.external_id\"),\n",
        "                            \"left\"\n",
        "                        )\n",
        "\n",
        "                        # df_joined = df.drop(\"external_id\")\n",
        "\n",
        "                        # Select all columns from df (source) and only the 'organisationkey' from dim_Organisation (dim)\n",
        "                        # Alias the dim_Organisation's organisationkey to avoid ambiguity\n",
        "                        \n",
        "                        df_with_orgkey = df_joined.select(\n",
        "                            *[col(f\"source.{col_name}\") for col_name in df.columns],\n",
        "                            col(\"dim.organisationkey\")\n",
        "                        )\n",
        "\n",
        "                        # --------------------------\n",
        "                        # second, get the studentkey\n",
        "                        # --------------------------\n",
        "                        \n",
        "                        dim_student_df = spark.read.format(\"delta\").load(f\"{silver_path}/dim_Student\").select(\"student_id\", \"organisationkey\", \"studentkey\")\n",
        "                        # Rename the 'studentkey' column from dim_student_df to avoid ambiguity\n",
        "                        dim_student_df = dim_student_df.withColumnRenamed(\"studentkey\", \"dim_studentkey\")\n",
        "                        \n",
        "                        # Perform a left join\n",
        "                        df_studjoined = df_with_orgkey.alias(\"source\").join(\n",
        "                            dim_student_df.alias(\"dim\"),\n",
        "                            (trim(lower(col(\"source.student_id\"))) == trim(lower(col(\"dim.student_id\")))) &\n",
        "                            (trim(lower(col(\"source.organisationkey\"))) == trim(lower(col(\"dim.organisationkey\")))),\n",
        "                            \"left\"\n",
        "                        )\n",
        "                        \n",
        "                        # Use when() to decide which studentkey to keep\n",
        "                        df_both_keys = df_studjoined.withColumn(\"studentkey\", \n",
        "                                                when(col(\"dim.dim_studentkey\").isNull(), col(\"source.studentkey\"))\n",
        "                                                .otherwise(col(\"dim.dim_studentkey\"))\n",
        "                                                ) \\\n",
        "                                    .drop(\"dim.dim_studentkey\") \\\n",
        "                                    .select(\"source.*\", \"studentkey\")\n",
        "\n",
        "                        df = df_both_keys\n",
        "                        # if the studentkey lookup has failed to find a student record then remove the related record for referential integrity\n",
        "                        df = df.filter((col(\"studentkey\").isNotNull()) & (col(\"studentkey\") != \"\"))\n",
        "                        \n",
        "                        # Wonde occasionally passes 2 records fo a sumamry attendance for a single student which would break\n",
        "                        if delta_table_name == \"fact_AttendanceSummary\":\n",
        "                            df = df.dropDuplicates(['studentkey'])\n",
        "                        \n",
        "                    # -------------------------------------------------------------------\n",
        "                    # Now that any table with student_id in it has studentkey continue...\n",
        "                    # -------------------------------------------------------------------\n",
        "\n",
        "                    # Set the update columns to update everything other than organisationkey and the unique_key\n",
        "                    update_columns = {col: f\"source.{col}\" for col in df.columns if col not in ['organisationkey', uuid_column_name]}\n",
        "                    # print(update_columns)\n",
        "\n",
        "                    if DeltaTable.isDeltaTable(spark, silver_table_path):\n",
        "                        delta_table = DeltaTable.forPath(spark, silver_table_path)\n",
        "                        target_df = delta_table.toDF()\n",
        "                        \n",
        "                        # Identify columns in source not in target\n",
        "                        new_columns = set(df.columns) - set(target_df.columns)\n",
        "                        \n",
        "                        if new_columns:\n",
        "                            # Add new columns with nulls to the target DataFrame\n",
        "                            for new_col in new_columns:\n",
        "                                target_df = target_df.withColumn(new_col, lit(None).cast(df.schema[new_col].dataType))\n",
        "                            \n",
        "                            # Create a new Delta table with the updated schema from the target DataFrame\n",
        "                            new_table_path = silver_table_path + \"_new\"\n",
        "                            \n",
        "                            if DeltaTable.isDeltaTable(spark, new_table_path):\n",
        "                                print(f\"Table at {new_table_path} exists. Deleting...\")\n",
        "                                deltaTable = DeltaTable.forPath(spark, new_table_path)\n",
        "                                deltaTable.delete()\n",
        "                            \n",
        "                            target_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(new_table_path)\n",
        "                            \n",
        "                            # Prepare the new Delta table for merging\n",
        "                            new_delta_table = DeltaTable.forPath(spark, new_table_path)\n",
        "                            \n",
        "                            # Adjust the source DataFrame to match the target schema, including any new columns\n",
        "                            df_adjusted = add_missing_columns(df, target_df)\n",
        "                            \n",
        "                            # Perform the merge operation\n",
        "                            new_delta_table.alias(\"target\").merge(\n",
        "                                df_adjusted.alias(\"source\"),\n",
        "                                f\"target.unique_key = source.unique_key\"\n",
        "                            ).whenMatchedUpdate(set=update_columns\n",
        "                            ).whenNotMatchedInsertAll().execute()\n",
        "                            \n",
        "                            # Overwrite the old table with the new table's data\n",
        "                            spark.read.format(\"delta\").load(new_table_path).write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").save(silver_table_path)\n",
        "                            \n",
        "                            # Consider cleaning up the new_table_path if necessary\n",
        "\n",
        "                        else:\n",
        "                            # Retrieve the schema of both DataFrames\n",
        "                            target_columns = delta_table.toDF().columns\n",
        "                            source_columns = df.columns\n",
        "                            \n",
        "                            # Identify columns that are in the target but not in the source\n",
        "                            missing_columns = [col for col in target_columns if col not in source_columns]\n",
        "\n",
        "                            # Add missing columns to the source DataFrame with default values (e.g., None)\n",
        "                            for col in missing_columns:\n",
        "                                df = df.withColumn(col, F.lit(None))\n",
        "\n",
        "                            # Perform your normal merge operation here since there are no new columns\n",
        "                            delta_table.alias(\"target\").merge(\n",
        "                                df.alias(\"source\"),\n",
        "                                f\"target.unique_key = source.unique_key\"\n",
        "                            ).whenMatchedUpdate(set=update_columns\n",
        "                            ).whenNotMatchedInsertAll().execute()\n",
        "\n",
        "                    else:\n",
        "                        # If the table does not exist, create it by writing the current DataFrame\n",
        "                        # First, generate a UUID for all records in the new UUID column\n",
        "                        df = df.withColumn(uuid_column_name, expr(\"uuid()\"))\n",
        "\n",
        "                        if ('studentkey' not in df.columns): # because we have already added organisationkey to that\n",
        "                    \n",
        "                            # Load the dim_Organisation table to get the existing mappings\n",
        "                            dim_org_df = spark.read.format(\"delta\").load(f\"{silver_path}/dim_Organisation\").select(\"external_id\", \"organisationkey\")\n",
        "\n",
        "                            # Perform a left join to find existing organisation keys\n",
        "                            df_joined = df.alias(\"source\").join(\n",
        "                                dim_org_df.alias(\"dim\"),\n",
        "                                col(\"source.school_id\") == col(\"dim.external_id\"),\n",
        "                                \"left\"\n",
        "                            )\n",
        "\n",
        "                            # Select all columns from df and only the 'organisationkey' from the dim_Organisation\n",
        "                            # Alias the dim_Organisation's organisationkey to avoid ambiguity\n",
        "                            df_with_keys = df_joined.select(\"source.*\", col(\"dim.organisationkey\").alias(\"dim_organisationkey\"))\n",
        "                            #df_with_keys.printSchema()\n",
        "\n",
        "                            # Fill in the missing keys with UUIDs\n",
        "                            # Ensure to use the aliased column name 'dim_organisationkey' to avoid ambiguity\n",
        "                            df_complete = df_with_keys.withColumn(\n",
        "                                \"organisationkey\",\n",
        "                                when(col(\"dim_organisationkey\").isNull(), expr(\"uuid()\")).otherwise(col(\"dim_organisationkey\"))\n",
        "                            )\n",
        "\n",
        "                            # Drop the 'dim_organisationkey' as it is no longer needed\n",
        "                            df = df_complete.drop(\"dim_organisationkey\")\n",
        "\n",
        "\n",
        "                        # debug, show 20 records\n",
        "                        df.show(n=20, truncate=False)\n",
        "\n",
        "                        df.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)\n",
        "        else:\n",
        "            print(f\"DataFrame for {json_name} is empty. Skipping processing.\")             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import coalesce, col, when, isnan, lit\n",
        "\n",
        "# Update dim_StudentExtended for leavers\n",
        "try:\n",
        "    deltaTablePath = silver_path + \"dim_StudentExtended\"\n",
        "    df = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "\n",
        "    df_students_leaver = json_dfs['students_leaver.json']\n",
        "    leaver_count = df_students_leaver.count()\n",
        "\n",
        "    if leaver_count > 0:\n",
        "        # Check if 'Leaving_Date' column exists, and if not, add it with null values\n",
        "        if 'Leaving_Date' not in df.columns:\n",
        "            df = df.withColumn('Leaving_Date', lit(None))\n",
        "\n",
        "        if 'Leaving_Date' not in df_students_leaver.columns:\n",
        "            df_students_leaver = df_students_leaver.withColumn('Leaving_Date', lit(None))\n",
        "\n",
        "        #print(f\"Number of records in 'students_leaver.json': {leaver_count}\")\n",
        "\n",
        "        # Join df_joined with df_students_leaver to update 'Leaving_Date' where matches are found\n",
        "        df_joined = df.join(\n",
        "                df_students_leaver.select('unique_key', 'Leaving_Date').withColumnRenamed('Leaving_Date', 'new_leaving_date'),\n",
        "                on='unique_key',\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "        # Modify the DataFrame to handle 'None' and 'nan'\n",
        "        df_joined = df_joined.withColumn(\n",
        "            'new_leaving_date', \n",
        "            when((col('new_leaving_date') != 'None') & (~isnan(col('new_leaving_date'))), col('new_leaving_date'))\n",
        "            .otherwise(None)\n",
        "        )\n",
        "\n",
        "        # Apply coalesce to update 'Leaving_Date' only if 'new_leaving_date' has a valid (non-null) value\n",
        "        df_joined = df_joined.withColumn(\n",
        "            'Leaving_Date', \n",
        "            coalesce('new_leaving_date', 'Leaving_Date')\n",
        "        ).drop('new_leaving_date')\n",
        "\n",
        "        # Print distinct Leaving Dates and their counts\n",
        "        distinct_leaving_dates = df_joined.select('Leaving_Date').distinct()\n",
        "        #distinct_leaving_dates.show()\n",
        "        counts_leaving_dates = df_joined.groupBy('Leaving_Date').count()\n",
        "        #counts_leaving_dates.show()\n",
        "\n",
        "        df_joined.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(deltaTablePath)\n",
        "    else:\n",
        "        # If df_students_leaver is empty, log a message or perform alternative actions\n",
        "        print(\"No records in 'students_leaver.json', skipping update.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the update operation: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# update dim_GroupMembership for groupkey\n",
        "try:\n",
        "    deltaTablePath = silver_path + \"dim_Group\"\n",
        "    df = spark.read.format(\"delta\").load(deltaTablePath).withColumnRenamed('unique_key', 'unique_group_id').withColumnRenamed('groupkey', 'new_groupkey')\n",
        "\n",
        "    membershipTablePath = silver_path + \"dim_GroupMembership\"\n",
        "    df_groupmembership = spark.read.format(\"delta\").load(membershipTablePath)\n",
        "    df_groupmembership = df_groupmembership.withColumn(\"unique_group_id\", F.concat(F.col(\"school_id\"), F.col(\"Group_ID\")))\n",
        "\n",
        "    # Perform the join\n",
        "    df_joined = df_groupmembership.join(df.select('unique_group_id', 'new_groupkey'), on='unique_group_id', how='left')\n",
        "\n",
        "    # Selecting columns from df_groupmembership and replacing 'groupkey' from the joined data\n",
        "    columns_to_select = [F.col(c) for c in df_groupmembership.columns if c != 'groupkey'] + [F.col('new_groupkey').alias('groupkey')]\n",
        "    df_updated = df_joined.select(*columns_to_select)\n",
        "\n",
        "    # Drop the 'unique_group_id' if you no longer need it\n",
        "    df_updated = df_updated.drop('unique_group_id')\n",
        "\n",
        "    df_updated.write.format(\"delta\").mode(\"overwrite\").save(membershipTablePath)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the update operation: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    deltaTablePath = silver_path + \"dim_Group\"\n",
        "    df = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "    from pyspark.sql.functions import col\n",
        "    df = df.filter(\n",
        "        ~(\n",
        "            col('groupkey').isNull() |  # Checks for null\n",
        "            (col('groupkey') == \"\") |  # Checks for empty strings (valid for string types)\n",
        "            (col('groupkey').cast(\"string\") == \"0\")  # Casts to string to safely check for \"0\"\n",
        "        )\n",
        "    )\n",
        "    df = df.dropDuplicates(['groupkey'])\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the update operation: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the Year_Group\n",
        "\n",
        "# Assuming data is loaded correctly into these dataframes\n",
        "deltaTablePath = silver_path + \"dim_Student\"\n",
        "df_student = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "deltaTablePath = silver_path + \"dim_GroupMembership\"\n",
        "df_groupmembership = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "\n",
        "# Drop existing Year_Group column\n",
        "df_student = df_student.drop(\"Year_Group\")\n",
        "\n",
        "# Join operation using both school_id and student_id, adding Year_Group\n",
        "df_joined = df_student.alias(\"student\").join(\n",
        "df_groupmembership.alias(\"groupmembership\"),\n",
        "(col(\"student.school_id\") == col(\"groupmembership.school_id\")) &\n",
        "(col(\"student.student_id\") == col(\"groupmembership.student_id\")) &\n",
        "(col(\"groupmembership.Group_Type\") == \"YEAR\"),\n",
        "\"left\"\n",
        ").select(\n",
        "\"student.*\",  # Select all columns from df_student\n",
        "col(\"groupmembership.Group_Name\").alias(\"Year_Group\")  # Rename and select the Group_Name as Year_Group\n",
        ")\n",
        "\n",
        "# Count the number of rows in the DataFrame\n",
        "row_count = df_joined.count()\n",
        "\n",
        "print(f\"Number of rows in the DataFrame: {row_count}\")\n",
        "\n",
        "# Count the number of non-empty entries in the 'Year_Group' column\n",
        "non_empty_year_group_count = df_joined.filter((col(\"Year_Group\").isNotNull()) & (col(\"Year_Group\") != \"\")).count()\n",
        "\n",
        "print(f\"Number of non-empty entries in the 'Year_Group' column: {non_empty_year_group_count}\")\n",
        "\n",
        "deltaTablePath = silver_path + \"dim_Student\"\n",
        "df_joined.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Drop duplicates based on studentkey in StudentExtended\n",
        "\n",
        "deltaTablePath = silver_path + \"dim_StudentExtended\"\n",
        "df = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "\n",
        "df = df.dropDuplicates([\"studentkey\"])\n",
        "\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Drop duplicates based on studentkey in Student\n",
        "\n",
        "deltaTablePath = silver_path + \"dim_Student\"\n",
        "df = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "\n",
        "df = df.dropDuplicates([\"studentkey\"])\n",
        "\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Drop duplicates in attendancesummary\n",
        "\n",
        "try:\n",
        "    deltaTablePath = silver_path + \"fact_AttendanceSummary\"\n",
        "    df_attendancesummary = spark.read.format(\"delta\").load(deltaTablePath)\n",
        "    df_attendancesummary = df_attendancesummary.dropDuplicates(['attendancesummarykey'])\n",
        "    df_attendancesummary = df_attendancesummary.dropDuplicates(['studentkey'])\n",
        "    df_attendancesummary.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the update operation: {e}\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
