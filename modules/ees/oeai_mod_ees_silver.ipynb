{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579d5d8-9b0e-42ee-a32d-2de1958c7e6f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lit, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acf5f9-8511-407a-91ba-0c2bea5ffd4d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run ees_env_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe60875-d522-445b-ad78-31a0e7ab0fb3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    'ees_daily_22_23' : None,\n",
    "    'ees_daily_23_24' : None,\n",
    "    'ees_daily' : None,\n",
    "    'ees_weekly' : None,\n",
    "    'ees_ytd' : None,\n",
    "}\n",
    "\n",
    "if env_debug: print(f\"{dfs.keys()}\\n\") #DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331827c0-1579-4117-bd63-409ef33b104d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Function: check_path_exists_databricks\n",
    "# Description:\n",
    "#     This function checks whether a given file or directory path exists in the Databricks filesystem.\n",
    "#     It utilizes Databricks-specific utilities (`dbutils.fs.ls`) to list the contents of the specified path.\n",
    "#     If the path exists, the function returns `True`; otherwise, it catches the exception and returns `False`.\n",
    "#\n",
    "# Parameters:\n",
    "#     path_to_check (str): The file or directory path in the Databricks filesystem to check for existence.\n",
    "#\n",
    "# Returns:\n",
    "#     bool: Returns `True` if the path exists, otherwise returns `False`.\n",
    "#\n",
    "# Example usage:\n",
    "#     exists = check_path_exists_databricks(\"/mnt/data/my_folder\")\n",
    "#\n",
    "# Notes:\n",
    "#     - This function is designed specifically for Databricks and relies on the `dbutils` utility.\n",
    "#     - If an exception occurs during the `dbutils.fs.ls` operation (e.g., path does not exist), \n",
    "#       it is caught and the function returns `False`.\n",
    "#     - Use this function to verify the existence of directories or files before performing operations on them.\n",
    "\n",
    "def check_path_exists_databricks(path_to_check):\n",
    "    try:\n",
    "        dbutils.fs.ls(path_to_check)  # For Databricks\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526152f-71d6-49db-90d5-5fbaf9e48687",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Function: check_path_exists_datalake\n",
    "# Description:\n",
    "#     This function checks whether a specified path exists in an Azure Data Lake or OneLake storage system\n",
    "#     using PySpark. It interacts with the Hadoop FileSystem API to determine if the path exists. If the \n",
    "#     path is found, it returns `True`; otherwise, it returns `False`. In case of any exception during the \n",
    "#     operation, the function catches it and returns `False`.\n",
    "#\n",
    "# Parameters:\n",
    "#     path_to_check (str): The file or directory path in the Azure Data Lake or OneLake storage to check.\n",
    "#\n",
    "# Returns:\n",
    "#     bool: Returns `True` if the specified path exists in the Data Lake or OneLake, otherwise returns `False`.\n",
    "#\n",
    "# Example usage:\n",
    "#     exists = check_path_exists_datalake(\"abfss://container@account.dfs.core.windows.net/folder\")\n",
    "#\n",
    "# Notes:\n",
    "#     - This function uses the Hadoop FileSystem API via PySpark to interact with Azure Data Lake or OneLake storage.\n",
    "#     - It catches any exception (e.g., permission issues, incorrect path) and returns `False` for robustness.\n",
    "#     - Ensure that the Spark session is properly configured and has access to the required storage system.\n",
    "#     - Use this function to safely check for the existence of paths before performing operations in Data Lake storage.\n",
    "\n",
    "def check_path_exists_datalake(path_to_check):\n",
    "    try:\n",
    "        hadoop_fs = spark._jvm.org.apache.hadoop.fs.FileSystem\n",
    "        hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "        path = spark._jvm.org.apache.hadoop.fs.Path(path_to_check)\n",
    "\n",
    "        # Check if the path exists\n",
    "        fs = hadoop_fs.get(hadoop_conf)\n",
    "        if fs.exists(path):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc36ad2-541a-46b8-a39e-0578ff437226",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Function: add_missing_columns\n",
    "# Description:\n",
    "#     This function ensures that all columns defined in a given schema are present in the provided DataFrame. \n",
    "#     If a column from the defined schema is missing in the DataFrame, the function adds that column with \n",
    "#     null values and casts it to the appropriate data type as defined in the schema.\n",
    "#\n",
    "# Parameters:\n",
    "#     df (DataFrame): The input DataFrame that may have missing columns.\n",
    "#     defined_schema (StructType): The schema that defines the expected structure of the DataFrame, \n",
    "#                                  including column names and data types.\n",
    "#\n",
    "# Returns:\n",
    "#     DataFrame: A new DataFrame that includes all the columns from the defined schema. Missing columns \n",
    "#                are added with null values and cast to the corresponding data type.\n",
    "#\n",
    "# Example usage:\n",
    "#     df = add_missing_columns(df, defined_schema)\n",
    "#\n",
    "# Notes:\n",
    "#     - This function is useful when the input DataFrame may have a dynamic or incomplete set of columns\n",
    "#       compared to the defined schema.\n",
    "#     - It adds any missing columns as nulls, which can help maintain consistency when processing data\n",
    "#       with varying schemas.\n",
    "\n",
    "def add_missing_columns(df, defined_schema):\n",
    "\n",
    "    for field in defined_schema:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0516d3-137b-4ac2-a486-7c30ed7dedbe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Function: conform_to_schema\n",
    "# Description:\n",
    "#     This function modifies the input DataFrame to conform to a given schema. It performs two main actions:\n",
    "#     1. It removes any columns from the DataFrame that are not part of the defined schema.\n",
    "#     2. It casts the remaining columns in the DataFrame to match the data types specified in the defined schema.\n",
    "#\n",
    "# Parameters:\n",
    "#     df (DataFrame): The input DataFrame that needs to be conformed to the defined schema.\n",
    "#     defined_schema (StructType): The schema that defines the expected column names and data types.\n",
    "#\n",
    "# Returns:\n",
    "#     DataFrame: A new DataFrame that matches the defined schema by containing only the required columns,\n",
    "#                with their data types cast appropriately.\n",
    "#\n",
    "# Example usage:\n",
    "#     df = conform_to_schema(df, defined_schema)\n",
    "#\n",
    "# Notes:\n",
    "#     - This function is helpful when ensuring that a DataFrame conforms to a specific schema, such as\n",
    "#       when dealing with data from diverse sources or inconsistent formats.\n",
    "#     - Columns not defined in the schema are dropped, and the remaining columns are cast to the data\n",
    "#       types specified in the schema.\n",
    "#     - Ensure that the input schema accurately reflects the desired structure to prevent data loss \n",
    "#       from dropped columns.\n",
    "\n",
    "def conform_to_schema(df, defined_schema):\n",
    "    # Drop any extra columns\n",
    "    df = df.select([field.name for field in defined_schema if field.name in df.columns])\n",
    "\n",
    "    # Cast the columns to match the defined schema\n",
    "    for field in defined_schema:\n",
    "        df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194dbb42-6c01-44a3-9e01-cf8ae83be8d2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "defined_schema = StructType([\n",
    "    StructField(\"time_period\", IntegerType(), True),\n",
    "    StructField(\"time_identifier\", StringType(), True),\n",
    "    StructField(\"region_name\", StringType(), True),\n",
    "    StructField(\"la_name\", StringType(), True),\n",
    "    StructField(\"old_la_code\", StringType(), True),\n",
    "    StructField(\"attendance_date\", DateType(), True),\n",
    "    StructField(\"school_type\", StringType(), True),\n",
    "    StructField(\"attendance_perc\", FloatType(), True),\n",
    "    StructField(\"authorised_absence_perc\", FloatType(), True),\n",
    "    StructField(\"unauthorised_absence_perc\", FloatType(), True),\n",
    "    StructField(\"illness_perc\", FloatType(), True),\n",
    "    StructField(\"appointments_perc\", FloatType(), True),\n",
    "    StructField(\"unauth_hol_perc\", FloatType(), True),\n",
    "    StructField(\"unauth_oth_perc\", FloatType(), True),\n",
    "    StructField(\"unauth_late_registers_closed_perc\", FloatType(), True),\n",
    "    StructField(\"unauth_not_yet_perc\", FloatType(), True),\n",
    "    StructField(\"auth_religious_perc\", FloatType(), True),\n",
    "    StructField(\"auth_study_perc\", FloatType(), True),\n",
    "    StructField(\"auth_grt_perc\", FloatType(), True),\n",
    "    StructField(\"auth_holiday_perc\", FloatType(), True),\n",
    "    StructField(\"auth_excluded_perc\", FloatType(), True),\n",
    "    StructField(\"auth_other_perc\", FloatType(), True),\n",
    "    StructField(\"pa_perc\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304f606-61ac-46dc-8e49-91ff5c4e4b5d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Read Bronze files into Dataframes and conform to schema (drop extra, add new, apply data types)\n",
    "for key, df in dfs.items():\n",
    "    print(f\"Loading [{key}] ...\")\n",
    "    \n",
    "    # Read Bronze files into Dataframe\n",
    "    df = spark.read.csv(os.path.join(env_paths[\"bronze\"], key), header=True, inferSchema=True)\n",
    "    # df.printSchema()\n",
    "\n",
    "    # Transforms go here\n",
    "    if \"attendance_date\" in df.columns:\n",
    "        if isinstance(df.schema[\"attendance_date\"].dataType, StringType):\n",
    "            print(f\"\\tTransforming [{key}].[attendance_date]\")\n",
    "            df = df.withColumn(\"attendance_date\", to_date(col(\"attendance_date\"), \"dd/MM/yyyy\"))\n",
    "    \n",
    "    # Add missing columns from Schema\n",
    "    df = add_missing_columns(df, defined_schema)\n",
    "\n",
    "    # Apply schema to Dataframe (drop extra columns, Cast to datatype)\n",
    "    df = conform_to_schema(df, defined_schema)\n",
    "\n",
    "    # Write to Data dictionary\n",
    "    dfs[key] = df\n",
    "    print(f\"\\trow count:\\t{dfs[key].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c66aa-e355-4cc3-83d7-9abaa018fae3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Combine Daily with 23/24\n",
    "dfs['ees_daily'] = dfs['ees_daily'] \\\n",
    "    .unionByName(dfs['ees_daily_22_23'], allowMissingColumns=True) \\\n",
    "    .unionByName(dfs['ees_daily_23_24'], allowMissingColumns=True) \\\n",
    "\n",
    "del dfs['ees_daily_22_23']\n",
    "del dfs['ees_daily_23_24']\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    print(f\"{key}:\\t{dfs[key].count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc067bf-bd23-4884-b65e-2a9f65d1e095",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Write Dataframes to Silver\n",
    "for key, df in dfs.items():\n",
    "    print(f\"{key}:\\t{dfs[key].count()}\")\n",
    "    # print(f\"{key}:\")\n",
    "\n",
    "    delta_path = os.path.join(env_paths['silver'], \"fact_\" + key)\n",
    "    if env_debug: print(f\"\\t{delta_path}\") #DEBUG\n",
    "\n",
    "    if check_path_exists_databricks(delta_path) or check_path_exists_datalake(delta_path):\n",
    "        ees_existing = spark.read.format(\"delta\").load(delta_path)\n",
    "        \n",
    "        # Print row count before union\n",
    "        print(f\"\\tExisting:\\t{ees_existing.count()}\")\n",
    "      \n",
    "        ees_combined = ees_existing.unionByName(dfs[key], allowMissingColumns=True).dropDuplicates()\n",
    "\n",
    "    else:\n",
    "        ees_combined = dfs[key].dropDuplicates()\n",
    "        print(f\"\\tNo Previous Data\")\n",
    "\n",
    "    # Write the updated DataFrames back to the Delta tables\n",
    "    print(f\"\\tWriting:\\t{ees_combined.count()}\")\n",
    "    ees_combined.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(delta_path)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
