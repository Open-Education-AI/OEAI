{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\"  # fully qualified for Fabric\n",
        "keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME_HERE\" # not required for Fabric  \n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"cpoms-bronze\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"cpoms-ids\", keyvault_linked_service, keyvault)\n",
        "school_ids = school_ids_secret.split(\",\")  # Assuming the IDs are comma-separated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the spark session and initialise the audit log\n",
        "audit_log = oeai.load_audit_log(spark, bronze_path + \"audit_log.json\")\n",
        "audit_logs = []\n",
        "error_log_path = bronze_path + \"error_log.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def update_query_with_chunks_cpoms(original_query, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Updates a query string by replacing or adding 'updated_after' and 'updated_before' parameters \n",
        "        with the provided start and end dates.\n",
        "\n",
        "        Args:\n",
        "            original_query (str): The original query string.\n",
        "            start_date (datetime): The start date for the 'updated_after' parameter.\n",
        "            end_date (datetime): The end date for the 'updated_before' parameter.\n",
        "\n",
        "        Returns:\n",
        "            str: The updated query string.\n",
        "        \"\"\"\n",
        "        # Remove existing 'updated_after' parameter using regex\n",
        "        query_without_updated_after = re.sub(r'filters[updated_at.gt]=[^&]*', '', original_query)\n",
        "\n",
        "        # Trim any trailing '&' characters\n",
        "        query_without_updated_after = query_without_updated_after.rstrip('&')\n",
        "\n",
        "        # Ensure the query starts correctly with '?' or '&' based on existing content\n",
        "        if query_without_updated_after and not query_without_updated_after.startswith('?'):\n",
        "            query_prefix = '&' if '?' in query_without_updated_after else '?'\n",
        "            query_without_updated_after = query_prefix + query_without_updated_after\n",
        "\n",
        "        # Format the new 'updated_after' and 'updated_before' parameters\n",
        "        formatted_start_date = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        formatted_end_date = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        chunk_query = f\"{query_without_updated_after}&filters[updated_at.gt]={formatted_start_date}&filters[updated_at.lt]={formatted_end_date}\"\n",
        "        \n",
        "        return chunk_query    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_school_data(token: str, school_id: str, endpoint: str, query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get the data for a school from the CPOMS API.\n",
        "\n",
        "    Args:\n",
        "        token (str): The token to use for the Wonde API.\n",
        "        school_id (str): The ID of the school to get data for.\n",
        "        endpoint (str): The endpoint to get data from.\n",
        "        query (str): Additional query parameters.\n",
        "        pagination_type (str): Type of pagination ('cursor' or 'offset').\n",
        "\n",
        "    Returns:\n",
        "        dict: The data for the school from the Wonde API.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the query string\n",
        "    query = f\"?{query.lstrip('&?')}\"\n",
        "\n",
        "    # Construct the initial URL\n",
        "    base_url = f\"https://{school_id}.cpoms.net/api/v1/{endpoint}\"\n",
        "    url = base_url\n",
        "    \n",
        "    # set the request headers\n",
        "    headers = {\n",
        "        'Authorization': 'Token ' + token,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    \n",
        "    all_data = []\n",
        "    url = url + query\n",
        "    page = 1\n",
        "    # per_page_limit = 50  # Default limit per page\n",
        "\n",
        "    while True:\n",
        "        print(url + \"page \" + str(page))\n",
        "        response = requests.get(url, headers=headers, params={\"page\": page})\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code != 200:\n",
        "            # Create a list of Row objects with your data\n",
        "            data = [\n",
        "                Row(Description=\"Error fetching data from URL\", Value=url),\n",
        "                Row(Description=\"Response Text\", Value=response.text),\n",
        "                Row(Description=\"School ID\", Value=school_id),\n",
        "                Row(Description=\"Token\", Value=token)\n",
        "            ]\n",
        "\n",
        "            # Create a DataFrame from the data\n",
        "            df = spark.createDataFrame(data)\n",
        "\n",
        "            # Specify the filename\n",
        "            filename = bronze_path + \"debug_info.csv\"\n",
        "\n",
        "            # Write the DataFrame to a CSV file\n",
        "            try:\n",
        "                df.write.csv(filename, header=True, mode=\"overwrite\")\n",
        "                print(\"Written the data to\", filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred: {e}\")\n",
        "                print(f\"Error fetching data from {paginated_url}. Stopping.\")\n",
        "            break\n",
        "\n",
        "        response_data = response.json()\n",
        "        page += 1\n",
        "\n",
        "        # Check if data is a list or dictionary\n",
        "        data_from_response = response_data.get(\"data\", [])\n",
        "        if isinstance(data_from_response, dict):\n",
        "            all_data.append(data_from_response)\n",
        "        else:\n",
        "            all_data.extend(data_from_response)\n",
        "    \n",
        "        if len(response_data[\"data\"]) < 30:\n",
        "            break\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def load_bronze(spark, endpoint: str, school_id: str, urn: str, token: str, limit=None, query=None, use_date_chunk=False, audit_log_file=\"audit_log.json\", override_date=None):\n",
        "    global audit_log\n",
        "    df = pd.DataFrame()\n",
        "    data_list = []  # Default empty list\n",
        "    full_data_list = []  # Default empty list\n",
        "    \n",
        "    # Calculate the duration of the API call\n",
        "    start_time = datetime.now()\n",
        "    now = datetime.now()\n",
        "\n",
        "    if override_date:\n",
        "        last_updated_str = oeai.safe_get_or_create(LastUpdated, override_date, school_id, endpoint)\n",
        "        last_updated_time = datetime.strptime(override_date, \"%Y-%m-%d %H:%M:%S\")\n",
        "    else:\n",
        "        last_updated_str = oeai.safe_get_or_create(LastUpdated, \"2023-09-01 00:00:00\", school_id, endpoint)\n",
        "        if last_updated_str is None:\n",
        "            last_updated_time = now - timedelta(weeks=2)\n",
        "        elif isinstance(last_updated_str, str):\n",
        "            last_updated_time = datetime.strptime(last_updated_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "        elif isinstance(last_updated_str, datetime):\n",
        "            last_updated_time = last_updated_str\n",
        "        else:\n",
        "            last_updated_time = now - timedelta(weeks=2)\n",
        "\n",
        "    # If last_updated_time is more than two weeks ago, chunk the requests\n",
        "    if use_date_chunk and (now - last_updated_time).days > 7:\n",
        "        for start_date, end_date in oeai.generate_date_chunks(last_updated_time, now, chunk_size=timedelta(days=7)):\n",
        "            \n",
        "            chunk_query = update_query_with_chunks_cpoms(query, start_date, end_date)\n",
        "            r = get_school_data(token, school_id, endpoint, chunk_query)\n",
        "\n",
        "            # Check if the response is not None and not empty before processing\n",
        "            if r:\n",
        "                if isinstance(r, dict) and 'data' in r:\n",
        "                    data_list.append(r['data'])\n",
        "                elif isinstance(r, list):\n",
        "                    data_list.extend(r)\n",
        "            else:\n",
        "                error_message = f\"Empty response, not adding to data_list: {traceback.format_exc()}\"\n",
        "                oeai.log_error(spark, error_message, error_log_path)\n",
        "    else:\n",
        "        \n",
        "        #if not override_date and last_updated_str is not None:\n",
        "        formatted_date = last_updated_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "        query += \"&filters[updated_at.gt]=\" + formatted_date\n",
        "        \n",
        "        r = get_school_data(token, school_id, endpoint, query)\n",
        "\n",
        "        # Ensure the data is always a list\n",
        "        if isinstance(r, dict) and 'data' in r:\n",
        "            data_list = [r['data']]\n",
        "        elif isinstance(r, list):\n",
        "            data_list = r\n",
        "\n",
        "    # Construct the directory path\n",
        "    school_folder = os.path.join(bronze_path, urn)\n",
        "\n",
        "    # Check and create directory if it doesn't exist\n",
        "    if not os.path.exists(school_folder):\n",
        "        os.makedirs(school_folder)\n",
        "\n",
        "    #if not override_date:\n",
        "    LastUpdated[school_id][endpoint] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    json_str = json.dumps(LastUpdated)\n",
        "    last_updated_df = spark.createDataFrame([LastUpdated])\n",
        "    last_updated_df.repartition(1).write.mode(\"overwrite\").json(bronze_path + 'last_run')\n",
        "\n",
        "    if not data_list:\n",
        "        oeai.save_empty_json(spark, school_folder + \"/\" + endpoint + \".json\")\n",
        "    else:\n",
        "        try:\n",
        "            nested = ['categories.json']\n",
        "            # Flatten each item in data_list\n",
        "            if endpoint in nested:\n",
        "                flattened_data_list = oeai.flatten_nested_json(json.dumps(data_list))\n",
        "            else:\n",
        "                flattened_data_list = [oeai.flatten_json(item) for item in data_list]\n",
        "\n",
        "            # Convert the list of dictionaries to a Pandas DataFrame\n",
        "            pandas_df = pd.DataFrame(flattened_data_list)\n",
        "\n",
        "            # Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "            r_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "            # Add school_id and unique_key to the DataFrame\n",
        "            r_df = r_df.withColumn(\"school_id\", lit(urn))\n",
        "            if \"student_data_id\" in r_df.columns:\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(urn),r_df[\"student_data_id\"].cast(\"string\"), r_df[\"id\"].cast(\"string\")))\n",
        "            else:\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(urn), r_df[\"id\"].cast(\"string\")))\n",
        "            \n",
        "            # Save the DataFrame to a JSON file\n",
        "            r_df.write.mode(\"overwrite\").json(school_folder + \"/\" + endpoint + \".json\")\n",
        "      \n",
        "        # if the key doesn't exist, skip it    \n",
        "        except KeyError as e:\n",
        "            print(f\"KeyError for key: {e}\")\n",
        "            #print(r_df)  # If you suspect the error is due to r_df\n",
        "            pass\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    \n",
        "    # Convert duration to a string representation\n",
        "    duration_str = str(duration)\n",
        "\n",
        "    # Update the audit log\n",
        "    audit_data = {\n",
        "        \"school_id\": school_id,\n",
        "        \"endpoint\": endpoint,\n",
        "        \"query\": query,\n",
        "        \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"duration\": duration_str,\n",
        "        \"records_returned\": str(len(data_list)),\n",
        "    }\n",
        "\n",
        "    # Append the audit log to the audit_logs list\n",
        "    audit_log.append(audit_data)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# introduce a limit for testing or leave as None for Live\n",
        "Limit = None\n",
        "\n",
        "schools_list = []\n",
        "for school_id in school_ids:\n",
        "    name, urn = school_id.split('-', 1)\n",
        "    secret_name = f\"cpoms-{name}\"\n",
        "    try:\n",
        "        token = oeai.get_secret(spark, secret_name, keyvault_linked_service, keyvault)\n",
        "        schools_list.append({\"school_id\": name, \"urn\": urn, \"token\": token})\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving secret for {school_id}: {e}\")\n",
        "\n",
        "# Define the path\n",
        "json_file_path = bronze_path + 'last_run'\n",
        "\n",
        "# Reset LastUpdated \n",
        "LastUpdated = {}\n",
        "\n",
        "# Read the JSON file \n",
        "try:\n",
        "    df = spark.read.json(bronze_path + 'last_run')\n",
        "    rows = df.collect()\n",
        "    if rows:\n",
        "        LastUpdated = rows[0].asDict()\n",
        "    else:\n",
        "        # Handle the case where the JSON file might be empty or not read correctly\n",
        "        LastUpdated = {}\n",
        "except:\n",
        "    LastUpdated = {}\n",
        "\n",
        "# Convert 'LastUpdated' Row objects to dictionaries\n",
        "for key, value in LastUpdated.items():\n",
        "    if isinstance(value, Row):\n",
        "        LastUpdated[key] = oeai.row_to_dict(value)\n",
        "\n",
        "for school in schools_list:\n",
        "    school_id = school[\"school_id\"]\n",
        "    token = school[\"token\"]  \n",
        "    urn = school[\"urn\"]\n",
        "\n",
        "    daily_jobs = [\n",
        "        (\"students\", Limit, \"?\", False), \n",
        "        (\"categories\", Limit, \"?\", False), \n",
        "        (\"incidents\", Limit, \"?\", True), \n",
        "        (\"actions\", Limit, \"?\", True), \n",
        "        (\"links\", Limit, \"?\", False), \n",
        "        ]\n",
        "\n",
        "    # call load bronze for each of the daily jobs\n",
        "    for job in daily_jobs:\n",
        "        #spark, endpoint: str, school_id: str, urn: str, token: str, limit=None, query=None, use_date_chunk=False, audit_log_file=\"audit_log.json\"):\n",
        "        #load_bronze(spark, job[0], school[\"school_id\"], school[\"urn\"], school[\"token\"], job[1], job[2], job[3])\n",
        "        # to override the lastupdated:\n",
        "        load_bronze(spark, job[0], school[\"school_id\"], school[\"urn\"], school[\"token\"], job[1], job[2], job[3], override_date=\"2024-01-01 00:00:00\")\n",
        "\n",
        "    # Save the audit log\n",
        "    oeai.save_audit_log(spark, audit_log, bronze_path + \"audit_log.json\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
