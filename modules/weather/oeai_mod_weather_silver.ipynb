{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\" # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n",
        "keyvault_linked_service = \"INSERT_YOUR_LINKED_SERVICE_NAME_HERE\"  # Not required for Fabric.\n",
        "\n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"wonde-bronze\", keyvault_linked_service, keyvault)\n",
        "silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"wonde-school-ids\", keyvault_linked_service, keyvault)\n",
        "school_ids = school_ids_secret.split(\",\")\n",
        "APIkey = oeai.get_secret(spark, \"weather-apikey\", keyvault_linked_service, keyvault)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# read bronze file into a data frame\n",
        "json_dir_path = f\"{bronze_path}/{school_id}.json\"\n",
        "df = spark.read.json(json_dir_path)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# List of weather IDs considered as bad weather\n",
        "bad_weather_ids = [200, 201, 202, 210, 211, 212, 221, 230, 231, 232, 302, 501,\n",
        "                   502, 503, 504, 511, 522, 531, 601, 602, 611, 616,\n",
        "                   622, 771, 781]\n",
        "\n",
        "# Create a new column based on the condition\n",
        "df = df.withColumn(\"Is_bad_weather\", when(col(\"weather_id\").isin(bad_weather_ids), 1).otherwise(0))\n",
        "\n",
        "# Show the DataFrame to verify the new column\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "bad_weather_df = df.filter(col(\"Is_bad_weather\") == 1)\n",
        "\n",
        "# Show the filtered DataFrame\n",
        "bad_weather_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the count of records in the DataFrame\n",
        "record_count = df.count()\n",
        "\n",
        "# Print the count\n",
        "print(f\"Total number of records in the DataFrame: {record_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Group by 'weather_id' and count each occurrence\n",
        "weather_id_counts = df.groupBy(\"weather_id\").count()\n",
        "\n",
        "# Show the weather IDs along with their counts\n",
        "weather_id_counts.show(truncate=False)\n",
        "\n",
        "# Optionally, sort the output by count in descending order to see the most common weather IDs first\n",
        "weather_id_counts.orderBy(col(\"count\").desc()).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_unixtime, col, date_format\n",
        "\n",
        "# Assuming 'timestamp' is your column with Unix timestamp values\n",
        "# Create a column for the calendar date\n",
        "df = df.withColumn(\"calendar_date\", date_format(from_unixtime(col(\"timestamp\")), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Create a column for the time\n",
        "df = df.withColumn(\"time\", date_format(from_unixtime(col(\"timestamp\")), \"HH:mm:ss\"))\n",
        "\n",
        "# Show the DataFrame to verify the new columns\n",
        "df.select(\"timestamp\", \"calendar_date\", \"time\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Assuming 'temperature' is the column with temperature values in Kelvin\n",
        "# Create a new column 'temp_celsius' with temperature converted to Celsius\n",
        "df = df.withColumn(\"temp_celsius\", col(\"temperature\") - 273.15)\n",
        "\n",
        "# Show the DataFrame to verify the new column\n",
        "df.select(\"temperature\", \"temp_celsius\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Path where the Delta table will be stored\n",
        "delta_table_path = f\"{silver_path}/dim_weather\"\n",
        "\n",
        "# Save the DataFrame to the Delta table\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
