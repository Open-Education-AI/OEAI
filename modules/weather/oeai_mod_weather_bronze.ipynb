{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\" # Fabric requires full URL eg \"https://key_vault_name.vault.azure.net/\"\n",
        "keyvault_linked_service = \"INSERT_YOUR_LINKED_SERVICE_NAME_HERE\"  # Not required for Fabric.\n",
        "\n",
        "\n",
        "# Synapse OEA environment paths\n",
        "bronze_path = oeai.get_secret(spark, \"wonde-bronze\", keyvault_linked_service, keyvault)\n",
        "silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\n",
        "school_ids_secret = oeai.get_secret(spark, \"wonde-school-ids\", keyvault_linked_service, keyvault)\n",
        "school_ids = school_ids_secret.split(\",\")\n",
        "APIkey = oeai.get_secret(spark, \"weather-apikey\", keyvault_linked_service, keyvault)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_weather_data(lat: str, long: str, part: str, APIkey: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get the data for a school from the Weather API.\n",
        "\n",
        "    Args:\n",
        "        token (str): The token to use for the Bromcom API.\n",
        "        estab (str): The estab of the school to get data for.\n",
        "        endpoint (str): The endpoint to get data from.\n",
        "\n",
        "    Returns:\n",
        "        dict: The data for the school from the Weather API.\n",
        "    \"\"\"\n",
        "    #Uses openweathermap - replace as required\n",
        "    url = f\"https://api.openweathermap.org/data/3.0/onecall?lat={lat}&lon={lon}&exclude={part}&appid={APIkey}\"\n",
        "    all_data = []\n",
        "    next_url = url\n",
        "\n",
        "    while next_url:\n",
        "        response = requests.get(next_url)\n",
        "        print(response)\n",
        "        # Check if the request was successful\n",
        "        if response.status_code != 200:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            break\n",
        "\n",
        "        response_data = response.json()\n",
        "        \n",
        "        # Check if there are more pages\n",
        "        next_url = response_data.get(\"meta\", {}).get(\"pagination\", {}).get(\"next\")\n",
        "    print(response_data)\n",
        "    return response_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def load_bronze(spark, lat: str, long: str, part: str, APIkey: str, limit=None, audit_log_file=\"audit_log.json\"):\n",
        "    \"\"\"\n",
        "    Loads data from an API into a Bronze layer, handling pagination, date chunking, and audit logging.\n",
        "\n",
        "    Args:\n",
        "        spark (SparkSession): Active SparkSession for DataFrame operations.\n",
        "        endpoint (str): API endpoint to retrieve data from.\n",
        "        subkey (str): Subkey for identifying the specific data.\n",
        "        school_id (str): Unique identifier for the school.\n",
        "        token (str): Authentication token for API access.\n",
        "        pagination_type (str): Type of pagination used by the API ('cursor' or 'offset').\n",
        "        limit (int, optional): Limit for the number of records to retrieve. Defaults to None.\n",
        "        query (str, optional): Additional query parameters for the API call. Defaults to None.\n",
        "        audit_log_file (str, optional): Filename for the audit log. Defaults to \"audit_log.json\".\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: A PySpark DataFrame with the loaded data.\n",
        "    \"\"\"\n",
        "    global audit_log\n",
        "    df = pd.DataFrame()\n",
        "    data_list = []  # Default empty list\n",
        "    full_data_list = []  # Default empty list\n",
        "\n",
        "    r = get_weather_data(lat,lon,part,APIkey)\n",
        "    \n",
        "    # Construct the directory path\n",
        "    school_folder = bronze_path + school_id\n",
        "\n",
        "    # Check and create directory if it doesn't exist\n",
        "    if not os.path.exists(school_folder):\n",
        "        os.makedirs(school_folder)\n",
        "\n",
        "    # Calculate the duration of the API call\n",
        "    #start_time = datetime.now()\n",
        "\n",
        "    if not data_list:\n",
        "        oeai.save_empty_json(spark, school_folder + \".json\")\n",
        "    else:\n",
        "        try:\n",
        "            # Update the last called date in the JSON file\n",
        "            #LastUpdated[subkey] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            r_df = pd.DataFrame.from_records(data_list)\n",
        "            flattened_data_list = [oeai.flatten_json(item) for item in data_list]\n",
        "\n",
        "            # Convert the list of dictionaries to a Pandas DataFrame\n",
        "            pandas_df = pd.DataFrame(flattened_data_list)\n",
        "\n",
        "            # Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "            r_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "            # Add school_id and unique_key to the DataFrame\n",
        "            r_df = r_df.withColumn(\"school_id\", lit(school_id))\n",
        "            r_df = r_df.withColumn(\"unique_key\", lit(school_id).cast(\"string\"))\n",
        "\n",
        "            # Save the DataFrame to a JSON file\n",
        "            r_df.write.mode(\"overwrite\").json(school_folder + \".json\")\n",
        "\n",
        "            #with open(bronze_path+'last_run.json', 'w') as f:\n",
        "            #    json.dump(LastUpdated, f)\n",
        "            \n",
        "        # if the key doesn't exist, skip it    \n",
        "        except Exception as e:\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\n",
        "            oeai.log_error(spark, error_message, error_log_path)\n",
        "            pass\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  BRONZE PROCESS\n",
        "'''\n",
        "# introduce a limit for testing or leave as None for Live\n",
        "Limit = None\n",
        "query = \"\"\n",
        "\n",
        "#Introduce parameters for your school\n",
        "\n",
        "lat = \"\" \n",
        "lon = \"\" \n",
        "part = \"\"\n",
        "\n",
        "\n",
        "\n",
        "# Set up the daily Jobs list with required endpoints\n",
        "for school_id in school_ids:\n",
        "    daily_jobs = [\n",
        "        (lat, lon, part, APIkey),\n",
        "        ]\n",
        "\n",
        "    # call load bronze for each of the daily jobs\n",
        "    for job in daily_jobs:\n",
        "        load_bronze(spark, job[0], job[1], job[2], job[3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_weather_data_for_timestamp(lat: str, lon: str, dt: int, APIkey: str, school_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetch the weather data for a specific timestamp and add school ID to the data.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.openweathermap.org/data/3.0/onecall/timemachine?lat={lat}&lon={lon}&dt={dt}&appid={APIkey}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error retrieving data: {response.text}\")\n",
        "        return {}\n",
        "\n",
        "    response_data = response.json()\n",
        "    response_data['school_id'] = school_id  # Append the school_id to the response data\n",
        "    return response_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import calendar\n",
        "\n",
        "def generate_daily_timestamps(start_date: str, end_date: str, hour_utc: int):\n",
        "    \"\"\"\n",
        "    Generate Unix timestamps for every day at a specific hour in UTC between two dates.\n",
        "    \"\"\"\n",
        "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    delta = datetime.timedelta(days=1)\n",
        "\n",
        "    timestamps = []\n",
        "    while start <= end:\n",
        "        # Ensure the datetime object is set to the specified hour in UTC\n",
        "        daily_time = start.replace(hour=hour_utc, minute=0, second=0, microsecond=0)\n",
        "        # Use calendar.timegm() to correctly handle UTC\n",
        "        timestamp = int(calendar.timegm(daily_time.timetuple()))\n",
        "        timestamps.append(timestamp)\n",
        "        start += delta\n",
        "\n",
        "    return timestamps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Introduce parameters for youe school\n",
        "lat = \"\"  \n",
        "lon = \"\"  \n",
        "\n",
        "#Define start and end date in format: YYYY-MM-DD\n",
        "start_date = \"\"\n",
        "end_date = \"\"\n",
        "hour_utc = 8  # 8 AM UTC\n",
        "\n",
        "# Generate timestamps\n",
        "timestamps = generate_daily_timestamps(start_date, end_date, hour_utc)\n",
        "\n",
        "# Collect data\n",
        "weather_data = []\n",
        "for timestamp in timestamps:\n",
        "    data = get_weather_data_for_timestamp(lat, lon, timestamp, APIkey, school_id)\n",
        "    print(data)  # See exactly what's being returned\n",
        "    weather_data.append(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, DoubleType, LongType, IntegerType, StructType, FloatType\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Define the schema corresponding to the JSON structure\n",
        "schema = StructType([\n",
        "    StructField(\"lat\", FloatType()),\n",
        "    StructField(\"lon\", FloatType()),\n",
        "    StructField(\"timezone\", StringType()),\n",
        "    StructField(\"timezone_offset\", LongType()),\n",
        "    StructField(\"data\", ArrayType(StructType([\n",
        "        StructField(\"dt\", LongType()),\n",
        "        StructField(\"sunrise\", LongType()),\n",
        "        StructField(\"sunset\", LongType()),\n",
        "        StructField(\"temp\", FloatType()),\n",
        "        StructField(\"feels_like\", FloatType()),\n",
        "        StructField(\"pressure\", IntegerType()),\n",
        "        StructField(\"humidity\", IntegerType()),\n",
        "        StructField(\"dew_point\", FloatType()),\n",
        "        StructField(\"clouds\", IntegerType()),\n",
        "        StructField(\"visibility\", IntegerType()),\n",
        "        StructField(\"wind_speed\", FloatType()),\n",
        "        StructField(\"wind_deg\", IntegerType()),\n",
        "        StructField(\"weather\", ArrayType(StructType([\n",
        "            StructField(\"id\", IntegerType()),\n",
        "            StructField(\"main\", StringType()),\n",
        "            StructField(\"description\", StringType()),\n",
        "            StructField(\"icon\", StringType())\n",
        "        ])))\n",
        "    ]))),\n",
        "    StructField(\"school_id\", StringType())\n",
        "])\n",
        "\n",
        "for record in weather_data:\n",
        "    for item in record['data']:\n",
        "        # Convert integer temperature and other related fields to float\n",
        "        item['temp'] = float(item['temp']) if item['temp'] is not None else None\n",
        "        item['feels_like'] = float(item['feels_like']) if item['feels_like'] is not None else None\n",
        "        item['dew_point'] = float(item['dew_point']) if item['dew_point'] is not None else None\n",
        "        item['wind_speed'] = float(item['wind_speed']) if item['wind_speed'] is not None else None\n",
        "\n",
        "# Create DataFrame using the defined schema\n",
        "df = spark.createDataFrame(weather_data, schema=schema)\n",
        "\n",
        "# Show the DataFrame to verify the correct loading of data\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Optionally, explode the data array to flatten it for further analysis or storage\n",
        "df = df.withColumn(\"data_exploded\", explode(\"data\"))\n",
        "df.select(\"school_id\", \"data_exploded.*\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# Assuming `df` already has the necessary data with the exploded 'data' array\n",
        "# Now explode the 'weather' array to transform each dictionary in the array into a separate row\n",
        "df = df.withColumn(\"weather_exploded\", explode(\"data_exploded.weather\"))\n",
        "\n",
        "# Flatten the structure by selecting the fields, including weather details\n",
        "flattened_df = df.select(\n",
        "    col(\"school_id\"),\n",
        "    col(\"data_exploded.dt\").alias(\"timestamp\"),\n",
        "    col(\"data_exploded.sunrise\").alias(\"sunrise\"),\n",
        "    col(\"data_exploded.sunset\").alias(\"sunset\"),\n",
        "    col(\"data_exploded.temp\").alias(\"temperature\"),\n",
        "    col(\"data_exploded.feels_like\").alias(\"feels_like\"),\n",
        "    col(\"data_exploded.pressure\").alias(\"pressure\"),\n",
        "    col(\"data_exploded.humidity\").alias(\"humidity\"),\n",
        "    col(\"data_exploded.dew_point\").alias(\"dew_point\"),\n",
        "    col(\"data_exploded.clouds\").alias(\"clouds\"),\n",
        "    col(\"data_exploded.visibility\").alias(\"visibility\"),\n",
        "    col(\"data_exploded.wind_speed\").alias(\"wind_speed\"),\n",
        "    col(\"data_exploded.wind_deg\").alias(\"wind_deg\"),\n",
        "    col(\"weather_exploded.id\").alias(\"weather_id\"),\n",
        "    col(\"weather_exploded.main\").alias(\"weather_main\"),\n",
        "    col(\"weather_exploded.description\").alias(\"weather_description\"),\n",
        "    col(\"weather_exploded.icon\").alias(\"weather_icon\")\n",
        ")\n",
        "\n",
        "# Print the schema to verify the new structure\n",
        "flattened_df.printSchema()\n",
        "\n",
        "# Show some data to ensure correctness\n",
        "flattened_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write to JSON file named after school_id\n",
        "output_path = f\"{bronze_path}/{school_id}\"\n",
        "flattened_df.write.mode('overwrite').json(output_path)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
