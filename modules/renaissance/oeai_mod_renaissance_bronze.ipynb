{"cells":[{"cell_type":"code","execution_count":null,"id":"d43ace8a-4ecf-4cad-a071-158d9ebdd1c7","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["%run oeai_py"]},{"cell_type":"code","execution_count":null,"id":"44ec292b-764d-4ef8-ae1f-62e654656dce","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n","oeai = OEAI(\"Fabric\")"]},{"cell_type":"code","execution_count":null,"id":"222de937-0918-4740-96c8-1750cc01a0bb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["pip install paramiko"]},{"cell_type":"code","execution_count":null,"id":"b9a0ed3c-5234-4892-8130-bfd96e5a4dea","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["pip install pysftp"]},{"cell_type":"code","execution_count":null,"id":"481f73ad-66be-45eb-942c-01e29994a3a6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import pysftp\n","import paramiko\n","import os"]},{"cell_type":"code","execution_count":null,"id":"b36f6b17-86d2-433c-bb51-1997d86c0f29","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# CHANGE VALUES FOR YOUR KEY VAULT\n","keyvault = \"INSERT_YOUR_KEYVAULT_NAME\" # fully qualified for Fabric \n","keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME\" # linked service name for Synapse\n","\n","# Synapse OEA environment path & secrets\n","bronze_path = oeai.get_secret(spark, \"renaissance-bronze\", keyvault_linked_service, keyvault)"]},{"cell_type":"code","execution_count":null,"id":"66ec9f7f-0576-40f9-a02c-5402c2de3b1d","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"6daab4fe-366f-4089-b172-d3f69d937ea5\",\"activityId\":\"3c84eccf-54e5-4ddc-9d6e-21ca6f5e3d2e\",\"applicationId\":\"application_1718379090020_0001\",\"jobGroupId\":\"10\",\"advices\":{\"error\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# SFTP details\n","\n","#https://transfer.renaissance.com/ThinClient/WTM/public/index.html#/login\n","\n","\n","host = \"sftp.renaissance.com\"\n","port = 22  # SFTP usually runs on port 22\n","username = \"YOUR_USERNAME\"\n","password = \"YOUR_PASSWORD\"\n","remote_path = \"\"\n","local_path = \"YOUR_LOCAL_PATH\"\n","\n","# Ensure the local directory exists\n","os.makedirs(local_path, exist_ok=True)\n","\n","# Print the local directory path for debugging\n","print(f\"Local directory path: {local_path}\")\n","\n","# Function to list and download files using paramiko\n","def download_files():\n","    transport = paramiko.Transport((host, port))\n","    transport.connect(username=username, password=password)\n","    sftp = paramiko.SFTPClient.from_transport(transport)\n","    \n","    try:\n","        sftp.chdir(remote_path)\n","        print(\"Files in the remote directory:\")\n","        for filename in sftp.listdir():\n","            print(filename)  # Print the list of files in the remote directory\n","            if filename.endswith(\".zip\"):\n","                local_file_path = os.path.join(local_path, filename)\n","                print(f\"Downloading {filename} to {local_file_path}\")\n","                sftp.get(filename, local_file_path)\n","    finally:\n","        sftp.close()\n","        transport.close()\n","\n","# Call the download function\n","download_files()\n","\n","# List files in the local directory to verify download\n","print(\"Files in the local directory:\")\n","local_files = [os.path.join(local_path, file) for file in os.listdir(local_path)]\n","for file in local_files:\n","    print(file)\n","\n","# Check if the paths actually exist and construct the list of zip files\n","zip_files = []\n","for file in local_files:\n","    if file.endswith('.zip'):\n","        if os.path.exists(file):\n","            print(f\"File exists: {file}\")\n","            zip_files.append(file)\n","        else:\n","            print(f\"File does not exist: {file}\")\n","\n","# Debug: print zip files to be loaded\n","print(f\"ZIP files to be loaded: {zip_files}\")\n","\n","# Load the downloaded files into a DataFrame\n","if zip_files:\n","    df = spark.read.format(\"binaryFile\").load(zip_files)\n","    # Show the DataFrame\n","    df.show()\n","else:\n","    print(\"No valid ZIP files found to load.\")"]},{"cell_type":"code","execution_count":null,"id":"091c5db3-3d71-405f-804e-7b50eea71885","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","\n","local_path = bronze_path  # Ensure this is defined with the correct path\n","\n","# Ensure the local directory exists\n","if not os.path.exists(local_path):\n","    print(f\"Directory {local_path} does not exist.\")\n","else:\n","    print(f\"Files in the local directory {local_path}:\")\n","    local_files = os.listdir(local_path)\n","    for file in local_files:\n","        print(file)\n"]},{"cell_type":"code","execution_count":null,"id":"7a951f9d-20c7-4658-9c1c-23024a2fe6e2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","import zipfile\n","import shutil\n","\n","# Define the local path and the specific file to load\n","local_path = bronze_path  # Ensure this is defined with the correct path\n","zip_file = os.path.join(local_path, 'YOUR_SCHOOL.zip')\n","\n","# Ensure the local directory exists\n","if not os.path.exists(local_path):\n","    print(f\"Directory {local_path} does not exist.\")\n","else:\n","    print(f\"Local directory path: {local_path}\")\n","\n","# Check if the ZIP file exists\n","if os.path.exists(zip_file):\n","    print(f\"ZIP file exists: {zip_file}\")\n","    \n","    # Create a directory for the extracted contents\n","    extracted_path = os.path.join(local_path, 'extracted')\n","    if os.path.exists(extracted_path):\n","        shutil.rmtree(extracted_path)\n","    os.makedirs(extracted_path)\n","    \n","    # Extract the ZIP file\n","    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n","        zip_ref.extractall(extracted_path)\n","    \n","    print(f\"Extracted files to: {extracted_path}\")\n","    \n","    # List the extracted files\n","    extracted_files = [os.path.join(extracted_path, file) for file in os.listdir(extracted_path)]\n","    for file in extracted_files:\n","        print(f\"Extracted file: {file}\")\n","    \n","    # Load the extracted CSV files into a Spark DataFrame\n","    for csv_file in extracted_files:\n","        if csv_file.endswith('.csv'):\n","            absolute_path = os.path.abspath(csv_file)\n","            file_url = f\"file://{absolute_path}\"\n","            print(f\"Loading CSV file: {file_url}\")\n","            \n","            # Confirm the file exists\n","            if os.path.exists(absolute_path):\n","                print(f\"File exists: {file_url}\")\n","                \n","                # Load the file using Spark\n","                df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_url)\n","                \n","                # Show the DataFrame\n","                df.show()\n","            else:\n","                print(f\"File does not exist: {file_url}\")\n","else:\n","    print(f\"ZIP file does not exist: {zip_file}\")\n"]},{"cell_type":"code","execution_count":null,"id":"8a8d70aa-ec26-40bb-a0c5-3157ad7c56c3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","import zipfile\n","import shutil\n","\n","# Define the local path and the specific file to load\n","local_path = bronze_path  # Ensure this is defined with the correct path\n","zip_file = os.path.join(local_path, 'YOUR_SCHOOL.zip')\n","output_path = os.path.join(local_path, 'output')  # Define the output path for saving the DataFrame\n","\n","# Ensure the local directory exists\n","if not os.path.exists(local_path):\n","    print(f\"Directory {local_path} does not exist.\")\n","else:\n","    print(f\"Local directory path: {local_path}\")\n","\n","# Check if the ZIP file exists\n","if os.path.exists(zip_file):\n","    print(f\"ZIP file exists: {zip_file}\")\n","    \n","    # Create a directory for the extracted contents\n","    extracted_path = os.path.join(local_path, 'extracted')\n","    if os.path.exists(extracted_path):\n","        shutil.rmtree(extracted_path)\n","    os.makedirs(extracted_path)\n","    \n","    # Extract the ZIP file\n","    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n","        zip_ref.extractall(extracted_path)\n","    \n","    print(f\"Extracted files to: {extracted_path}\")\n","    \n","    # List the extracted files\n","    extracted_files = [os.path.join(extracted_path, file) for file in os.listdir(extracted_path)]\n","    for file in extracted_files:\n","        print(f\"Extracted file: {file}\")\n","    \n","    # Load the extracted CSV files into a Spark DataFrame and write to output path\n","    for csv_file in extracted_files:\n","        if csv_file.endswith('.csv'):\n","            absolute_path = os.path.abspath(csv_file)\n","            file_url = f\"file://{absolute_path}\"\n","            print(f\"Loading CSV file: {file_url}\")\n","            \n","            # Confirm the file exists\n","            if os.path.exists(absolute_path):\n","                print(f\"File exists: {file_url}\")\n","                \n","                # Load the file using Spark\n","                df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_url)\n","                \n","                # Show the DataFrame\n","                df.show()\n","                \n","                # Write the DataFrame to the output directory\n","                output_csv_path = os.path.join(output_path, os.path.basename(csv_file))\n","                df.write.format(\"csv\").option(\"header\", \"true\").save(output_csv_path)\n","                print(f\"DataFrame written to: {output_csv_path}\")\n","            else:\n","                print(f\"File does not exist: {file_url}\")\n","else:\n","    print(f\"ZIP file does not exist: {zip_file}\")\n"]},{"cell_type":"code","execution_count":null,"id":"2e5a5585-6622-42eb-a4e3-3f68ba63248b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","import zipfile\n","import shutil\n","\n","# Define the local path where ZIP files are located\n","local_path = bronze_path  # Ensure this is defined with the correct path\n","\n","# Ensure the local directory exists\n","if not os.path.exists(local_path):\n","    print(f\"Directory {local_path} does not exist.\")\n","else:\n","    print(f\"Local directory path: {local_path}\")\n","\n","# Function to extract ZIP files into folders named after the ZIP file\n","def extract_zip_files():\n","    # List all ZIP files in the local directory\n","    zip_files = [f for f in os.listdir(local_path) if f.endswith('.zip')]\n","    \n","    for zip_file in zip_files:\n","        zip_file_path = os.path.join(local_path, zip_file)\n","        \n","        # Create a directory named after the ZIP file (excluding the .zip extension)\n","        folder_name = os.path.splitext(zip_file)[0]\n","        extracted_path = os.path.join(local_path, folder_name)\n","        \n","        # Remove existing directory if it exists\n","        if os.path.exists(extracted_path):\n","            shutil.rmtree(extracted_path)\n","        \n","        # Create the new directory\n","        os.makedirs(extracted_path)\n","        \n","        # Extract the ZIP file into the created directory\n","        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","            zip_ref.extractall(extracted_path)\n","        \n","        print(f\"Extracted {zip_file} to {extracted_path}\")\n","        \n","        # List the extracted files\n","        extracted_files = [os.path.join(extracted_path, file) for file in os.listdir(extracted_path)]\n","        for file in extracted_files:\n","            print(f\"Extracted file: {file}\")\n","\n","# Call the function to extract ZIP files\n","extract_zip_files()\n","\n","# Optional: Load the extracted CSV files into a Spark DataFrame (example for one CSV file)\n","folder_name = 'YOUR_SCHOOL'  # Example folder name\n","csv_file = 'NAME.csv'  # Example CSV file name\n","csv_file_path = os.path.join(local_path, folder_name, csv_file)\n","\n","# Check if the CSV file exists and load it into Spark DataFrame\n","if os.path.exists(csv_file_path):\n","    file_url = f\"file://{os.path.abspath(csv_file_path)}\"\n","    print(f\"Loading CSV file: {file_url}\")\n","    \n","    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_url)\n","    \n","    # Show the DataFrame\n","    df.show()\n","else:\n","    print(f\"CSV file does not exist: {csv_file_path}\")\n"]},{"cell_type":"code","execution_count":null,"id":"a9612df5-1a63-45e6-bb91-33ef3f511bad","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","import zipfile\n","import shutil\n","\n","# Define the local path where ZIP files are located\n","local_path = bronze_path  # Ensure this is defined with the correct path\n","\n","# Ensure the local directory exists\n","if not os.path.exists(local_path):\n","    print(f\"Directory {local_path} does not exist.\")\n","else:\n","    print(f\"Local directory path: {local_path}\")\n","\n","# Function to extract ZIP files into folders named after the ZIP file\n","def extract_zip_files():\n","    # List all ZIP files in the local directory\n","    zip_files = [f for f in os.listdir(local_path) if f.endswith('.zip')]\n","    \n","    for zip_file in zip_files:\n","        zip_file_path = os.path.join(local_path, zip_file)\n","        \n","        # Create a directory named after the ZIP file (excluding the .zip extension)\n","        folder_name = os.path.splitext(zip_file)[0]\n","        extracted_path = os.path.join(local_path, folder_name)\n","        \n","        # Remove existing directory if it exists\n","        if os.path.exists(extracted_path):\n","            shutil.rmtree(extracted_path)\n","        \n","        # Create the new directory\n","        os.makedirs(extracted_path)\n","        \n","        # Extract the ZIP file into the created directory\n","        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","            zip_ref.extractall(extracted_path)\n","        \n","        print(f\"Extracted {zip_file} to {extracted_path}\")\n","        \n","        # List the extracted files\n","        extracted_files = [os.path.join(extracted_path, file) for file in os.listdir(extracted_path)]\n","        for file in extracted_files:\n","            print(f\"Extracted file: {file}\")\n","\n","# Call the function to extract ZIP files\n","extract_zip_files()\n","\n","# Load the extracted CSV files into a Spark DataFrame (example for one CSV file)\n","folder_name = 'YOUR_SCHOOL'  # Example folder name\n","csv_file = 'AR_UK.csv'  # Example CSV file name\n","csv_file_path = os.path.join(local_path, folder_name, csv_file)\n","\n","# Ensure the path is correctly formatted and check existence\n","absolute_csv_path = os.path.abspath(csv_file_path)\n","\n","print(f\"Loading CSV file: {absolute_csv_path}\")\n","\n","# Check if the CSV file exists and load it into Spark DataFrame\n","if os.path.exists(absolute_csv_path):\n","    file_url = f\"file://{absolute_csv_path}\"\n","    print(f\"File exists: {file_url}\")\n","    \n","    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_url)\n","    \n","    # Show the DataFrame\n","    df.show()\n","else:\n","    print(f\"CSV file does not exist: {absolute_csv_path}\")\n","\n","# Save the combined DataFrame as a Delta table\n","csv_path = bronze_path + \"AR_UK.csv\"\n","df.write.format(\"CSV\").mode(\"overwrite\").save(csv_path)\n","\n"]}],"metadata":{"dependencies":{},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
