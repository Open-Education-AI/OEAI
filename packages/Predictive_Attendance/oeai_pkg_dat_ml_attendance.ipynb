{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\"  \n",
        "keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME_HERE\"  \n",
        "\n",
        "silver_path = oeai.get_secret(spark, \"bromcom-silver\", keyvault_linked_service, keyvault)\n",
        "gold_path = oeai.get_secret(spark, \"gold-path\", keyvault_linked_service, keyvault)\n",
        "\n",
        "# Define the paths to your Delta tables\n",
        "path_dim_student = silver_path + 'dim_Student'\n",
        "path_dim_studentextended = silver_path + 'dim_StudentExtended'\n",
        "path_attendancesummary = silver_path + 'fact_AttendanceSummary'\n",
        "\n",
        "# Read the Delta tables into DataFrames\n",
        "df_dim_student = spark.read.format(\"delta\").load(path_dim_student)\n",
        "df_dim_studentextended = spark.read.format(\"delta\").load(path_dim_studentextended)\n",
        "df_attendancesummary = spark.read.format(\"delta\").load(path_attendancesummary)\n",
        "\n",
        "# Alias each DataFrame\n",
        "df_student = df_dim_student.alias(\"student\")\n",
        "df_extended = df_dim_studentextended.alias(\"extended\")\n",
        "df_attendance = df_attendancesummary.alias(\"attendance\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This code is for the first part of the notebook, where we create the DataFrame for prediction. \n",
        "The DataFrame is saved as a Delta table in the Gold zone.\n",
        "'''\n",
        "# Select columns explicitly to avoid duplicates, using DataFrame alias\n",
        "combined_df = df_student.join(df_extended, col(\"student.unique_key\") == col(\"extended.unique_key\"), \"inner\") \\\n",
        "                        .join(df_attendance, col(\"student.unique_key\") == col(\"attendance.unique_key\"), \"inner\") \\\n",
        "                        .select(\n",
        "                            col(\"student.unique_key\"),  \n",
        "                            col(\"student.Gender\"),\n",
        "                            col(\"extended.Pupil_Premium_Indicator\"), \n",
        "                            col(\"extended.SEN_Status\"),\n",
        "                            col(\"extended.Current_Year\"),\n",
        "                            col(\"extended.English_As_Additional_Language\"),\n",
        "                            col(\"attendance.Attendance_Mark_String\"),\n",
        "                        )\n",
        "\n",
        "# Convert 'Pupil_Premium_Indicator' to numeric (1 for True, 0 for False)\n",
        "combined_df = combined_df.withColumn(\"Pupil_Premium_Indicator_Num\", col(\"Pupil_Premium_Indicator\").cast(\"integer\"))\n",
        "\n",
        "\n",
        "# Define the UDF to calculate risk based on the last 20 marks of the attendance string\n",
        "# StringIndexer for categorical columns\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_Index\"),\n",
        "    StringIndexer(inputCol=\"SEN_Status\", outputCol=\"SEN_Status_Index\"),\n",
        "    StringIndexer(inputCol=\"Current_Year\", outputCol=\"Current_Year_Index\"),  # Optional: treat as categorical\n",
        "    StringIndexer(inputCol=\"English_As_Additional_Language\", outputCol=\"EAL_Index\"),\n",
        "]\n",
        "\n",
        "# UDF to count absences in the last 30 characters of a string, ignoring '#'\n",
        "def count_absences_last_30_chars(mark_string):\n",
        "    filtered_string = mark_string.replace(\"#\", \"\")[-30:]  # Remove '#' and consider the last 30 characters\n",
        "    count = 0\n",
        "    for char in filtered_string:\n",
        "        if char not in ['/', '\\\\', 'Z', 'L', 'B', 'D', 'J', 'P', 'V', 'W', 'X', 'Y']:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# Register UDF\n",
        "count_absences_last_30_chars_udf = udf(count_absences_last_30_chars, IntegerType())\n",
        "\n",
        "# Apply the UDF to create a new column for absences\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"count_last_30_absences\", count_absences_last_30_chars_udf(F.col(\"Attendance_Mark_String\")))\n",
        "\n",
        "# Example: Viewing the new column\n",
        "combined_df_transformed.select(\"unique_key\", \"count_last_30_absences\").show()\n",
        "\n",
        "# we are later going to predict the risk of absence for each student, so we are preparing the data for that\n",
        "df_for_prediction = combined_df_transformed\n",
        "\n",
        "# Define the UDF to calculate risk based on the last 20 marks of the attendance string\n",
        "def calculate_risk(mark_string):\n",
        "    # Get the last 20 characters (10 days) of the string\n",
        "    future_marks = mark_string.replace(\"#\", \"\")[-20:]  # Remove '#' and consider the last 20 characters\n",
        "    # Count non-attendance marks\n",
        "    non_attendance_count = 0\n",
        "    for mark in future_marks:\n",
        "        if mark not in ['/', '\\\\', 'Z', 'L', 'B', 'D', 'J', 'P', 'V', 'W', 'X', 'Y']:\n",
        "            non_attendance_count += 1\n",
        "    # Determine if the student is at risk\n",
        "    return 1 if non_attendance_count >= 2 else 0\n",
        "\n",
        "calculate_risk_udf = udf(calculate_risk, IntegerType())\n",
        "\n",
        "# Apply the UDF to create the label\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"label\", calculate_risk_udf(col(\"Attendance_Mark_String\")))\n",
        "\n",
        "# next we are going to train a model to predict the risk of absence for each student\n",
        "\n",
        "# List of input columns for the VectorAssembler\n",
        "feature_columns = [\"Gender_Index\", \"SEN_Status_Index\", \"Current_Year_Index\", \"EAL_Index\", \"Pupil_Premium_Indicator_Num\", \"count_last_30_absences\"]\n",
        "\n",
        "# Drop the existing 'features' column, because we are going to create a new one\n",
        "combined_df_transformed = combined_df_transformed.drop('features')\n",
        "\n",
        "# we are going to use a random forest classifier because it is a good classifier for binary classification problems \n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# Pipeline with assembler and classifier.  We use a pipeline because it allows us to reuse the same steps for the training and prediction data\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Split the data because we want to evaluate the model on unseen data\n",
        "(train_data, test_data) = combined_df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Model training and evaluation\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Model evaluation\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# the ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) for the different possible cutpoints of a diagnostic test.\n",
        "print(\"Test Area Under ROC: \", auc)\n",
        "\n",
        "# Extract Best Model Parameters\n",
        "bestPipeline = cvModel.bestModel\n",
        "bestRFModel = bestPipeline.stages[-1]  # The last stage in the pipeline is the RandomForest model\n",
        "print(\"Best Max Depth: \", bestRFModel.getMaxDepth())\n",
        "print(\"Best Num Trees: \", bestRFModel.numTrees)  # Access as attribute\n",
        "\n",
        "# Evaluate Best Model Performance\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "bestModelPerformance = evaluator.evaluate(bestPipeline.transform(test_data))\n",
        "print(\"Best Model Test Area Under ROC: \", bestModelPerformance)\n",
        "\n",
        "# specify the feature columns\n",
        "feature_columns = [\n",
        "    \"Pupil_Premium_Indicator_Num\", \"Gender_Index\", \"SEN_Status_Index\",\n",
        "    \"Current_Year_Index\", \"EAL_Index\", \"count_last_30_absences\"\n",
        "]\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = model.stages[-1].featureImportances.toArray()\n",
        "\n",
        "# Map feature names to their importance scores\n",
        "importances = {feature: score for feature, score in zip(feature_columns, feature_importances)}\n",
        "\n",
        "# Sort and print feature importances\n",
        "sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
        "for feature, importance in sorted_importances:\n",
        "    print(f\"{feature}: {importance}\")\n",
        "\n",
        "\n",
        "# next we are going to make predictions for the risk of absence for each student and save the results in a parquet file in the Gold zone\n",
        "# Make predictions\n",
        "predictions = model.transform(df_for_prediction) #df_for_prediction is the dataframe we created earlier\n",
        "\n",
        "# Select the relevant columns (student key and prediction)\n",
        "output_data = predictions.select(col(\"unique_key\"), col(\"prediction\").alias(\"predicted_label\"))\n",
        "\n",
        "# Write to Parquet file\n",
        "output_data.write.parquet(gold_path + \"ml_fact_AttendanceRisk/\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
