{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of OEAI class and set the platform (\"Synapse\" or \"Fabric\")\n",
        "oeai = OEAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql.functions import lit, current_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\n",
        "keyvault = \"INSERT_YOUR_KEYVAULT_NAME_HERE\"  \n",
        "keyvault_linked_service = \"INSERT_YOUR_KEYVAULT_LINKED_SERVICE_NAME_HERE\" # Linked service name for Synapse  \n",
        "\n",
        "# Synapse OEA environment paths\n",
        "silver_path = oeai.get_secret(spark, \"bromcom-silver\", keyvault_linked_service, keyvault)\n",
        "gold_path = oeai.get_secret(spark, \"gold-path\", keyvault_linked_service, keyvault)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define the paths to your Delta tables\n",
        "path_dim_student = silver_path + 'dim_Student'\n",
        "path_dim_studentextended = silver_path + 'dim_StudentExtended'\n",
        "path_attendancesummary = silver_path + 'fact_AttendanceSummary'\n",
        "\n",
        "# Read the Delta tables into DataFrames\n",
        "df_dim_student = spark.read.format(\"delta\").load(path_dim_student)\n",
        "df_dim_studentextended = spark.read.format(\"delta\").load(path_dim_studentextended)\n",
        "df_attendancesummary = spark.read.format(\"delta\").load(path_attendancesummary)\n",
        "\n",
        "# Alias each DataFrame\n",
        "df_student = df_dim_student.alias(\"student\")\n",
        "df_extended = df_dim_studentextended.alias(\"extended\")\n",
        "df_attendance = df_attendancesummary.alias(\"attendance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Select columns explicitly to avoid duplicates, using DataFrame alias\n",
        "combined_df = df_student.join(df_extended, col(\"student.unique_key\") == col(\"extended.unique_key\"), \"inner\") \\\n",
        "                        .join(df_attendance, col(\"student.unique_key\") == col(\"attendance.unique_key\"), \"inner\") \\\n",
        "                        .select(\n",
        "                            col(\"student.unique_key\"),  \n",
        "                            col(\"student.Gender\"),\n",
        "                            col(\"extended.Pupil_Premium_Indicator\"), \n",
        "                            col(\"extended.SEN_Status\"),\n",
        "                            col(\"extended.Current_Year\"),\n",
        "                            col(\"extended.English_As_Additional_Language\"),\n",
        "                            col(\"attendance.Attendance_Mark_String\"),\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Convert 'Pupil_Premium_Indicator' to numeric (1 for True, 0 for False)\n",
        "combined_df = combined_df.withColumn(\"Pupil_Premium_Indicator_Num\", col(\"Pupil_Premium_Indicator\").cast(\"integer\"))\n",
        "\n",
        "#Convert 'English_As_An_Additional_Language' to numeric (1 for True, 0 for False)\n",
        "combined_df = combined_df.withColumn(\"English_As_Additional_Language_Num\", col(\"English_As_Additional_Language\").cast(\"integer\"))\n",
        "\n",
        "\n",
        "# StringIndexer for categorical columns\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_Index\"),\n",
        "    StringIndexer(inputCol=\"SEN_Status\", outputCol=\"SEN_Status_Index\"),\n",
        "    StringIndexer(inputCol=\"Current_Year\", outputCol=\"Current_Year_Index\"),  \n",
        "    #StringIndexer(inputCol=\"English_As_Additional_Language\", outputCol=\"EAL_Index\"),\n",
        "]\n",
        "\n",
        "# Assembling all features into one vector column\n",
        "# Include \"English_As_Additional_Language_Num\" directly in the inputCols for VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[indexer.getOutputCol() for indexer in indexers] + [\"Pupil_Premium_Indicator_Num\", \"English_As_Additional_Language_Num\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "\n",
        "# Fit and transform the pipeline on the combined_df\n",
        "combined_df_transformed = pipeline.fit(combined_df).transform(combined_df)\n",
        "\n",
        "# Now, combined_df_transformed has a 'features' column with all the standardized features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define a schema for the UDF return type\n",
        "schema = StructType([\n",
        "    StructField(\"count\", IntegerType(), False),\n",
        "    StructField(\"filtered_string\", StringType(), False)\n",
        "])\n",
        "\n",
        "# Modify the UDF to return both count and filtered_string\n",
        "def count_absences_last_40_chars(mark_string):\n",
        "    filtered_string = mark_string.replace(\"#\", \"\")[-40:]  # Remove '#' and consider the last 40 characters\n",
        "    count = 0\n",
        "    for char in filtered_string:\n",
        "        if char not in ['/', '\\\\', 'Z', 'L', 'B', 'D', 'J', 'P', 'V', 'W', 'X', 'Y']:\n",
        "            count += 1\n",
        "    \n",
        "    return (count, filtered_string)\n",
        "\n",
        "# Register the UDF with the new schema\n",
        "count_absences_last_40_chars_udf = udf(count_absences_last_40_chars, schema)\n",
        "\n",
        "# Apply the UDF and create two new columns\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"absence_info\", count_absences_last_40_chars_udf(combined_df_transformed[\"Attendance_Mark_String\"]))\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"count_last_40_absences\", combined_df_transformed[\"absence_info\"][\"count\"])\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"last_40_mark_string\", combined_df_transformed[\"absence_info\"][\"filtered_string\"])\n",
        "\n",
        "# Drop the struct column as it's no longer needed\n",
        "combined_df_transformed = combined_df_transformed.drop(\"absence_info\")\n",
        "\n",
        "# Example: Viewing the new columns\n",
        "combined_df_transformed.select(\"unique_key\", \"count_last_40_absences\", \"last_40_mark_string\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_for_prediction = combined_df_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define a schema for the UDF return type\n",
        "schema = StructType([\n",
        "    StructField(\"risk\", IntegerType(), False),\n",
        "    StructField(\"future_marks\", StringType(), False)\n",
        "])\n",
        "\n",
        "# Modify the UDF to return both risk and future_marks\n",
        "def calculate_risk(mark_string):\n",
        "    future_marks = mark_string.replace(\"#\", \"\")[-10:]  # Remove '#' and consider the last 10 characters\n",
        "    non_attendance_count = 0\n",
        "    for mark in future_marks:\n",
        "        if mark not in ['/', '\\\\', 'Z', 'L', 'B', 'D', 'J', 'P', 'V', 'W', 'X', 'Y']:\n",
        "            non_attendance_count += 1\n",
        "    risk = 1 if non_attendance_count >= 2 else 0\n",
        "    return (risk, future_marks)\n",
        "\n",
        "# Register the UDF with the new schema\n",
        "calculate_risk_udf = udf(calculate_risk, schema)\n",
        "\n",
        "# Apply the UDF to create a struct column containing both risk and future_marks, then extract these into separate columns\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"risk_info\", calculate_risk_udf(col(\"Attendance_Mark_String\")))\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"label\", combined_df_transformed[\"risk_info\"][\"risk\"])\n",
        "combined_df_transformed = combined_df_transformed.withColumn(\"training_future_marks\", combined_df_transformed[\"risk_info\"][\"future_marks\"])\n",
        "\n",
        "# Drop the struct column as it's no longer needed\n",
        "combined_df_transformed = combined_df_transformed.drop(\"risk_info\")\n",
        "\n",
        "# Now the DataFrame has 'label' and 'training_future_marks' columns for each student\n",
        "combined_df_transformed.show(50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# List of feature columns\n",
        "feature_columns = [\n",
        "    \"Pupil_Premium_Indicator_Num\", \"Gender_Index\", \"SEN_Status_Index\",\n",
        "    \"Current_Year_Index\", \"English_As_Additional_Language_Num\",\n",
        "    \"count_last_40_absences\"\n",
        "]\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"assembled_features\")  # Replace 'feature_columns' with your feature column names\n",
        "\n",
        "# Initialize the classifier\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"assembled_features\")\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, rf])\n",
        "\n",
        "# Split the data\n",
        "(train_data, test_data) = combined_df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Model training\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Model evaluation\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(\"Test Area Under ROC: \", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Hyperparameter tuning (optional)\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=5)\n",
        "\n",
        "cvModel = crossval.fit(train_data)\n",
        "bestModel = cvModel.bestModel\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Extract Best Model Parameters\n",
        "bestPipeline = cvModel.bestModel\n",
        "bestRFModel = bestPipeline.stages[-1]  # The last stage in the pipeline is the RandomForest model\n",
        "print(\"Best Max Depth: \", bestRFModel.getMaxDepth())\n",
        "print(\"Best Num Trees: \", bestRFModel.numTrees)  # Access as attribute\n",
        "\n",
        "# Evaluate Best Model Performance\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "bestModelPerformance = evaluator.evaluate(bestPipeline.transform(test_data))\n",
        "print(\"Best Model Test Area Under ROC: \", bestModelPerformance)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Ensure this list exactly matches the inputCols of the VectorAssembler used in the model\n",
        "feature_columns = [\n",
        "    \"Pupil_Premium_Indicator_Num\", \"Gender_Index\", \"SEN_Status_Index\",\n",
        "    \"Current_Year_Index\", \"English_As_Additional_Language_Num\",\n",
        "    \"count_last_40_absences\"\n",
        "]\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = model.stages[-1].featureImportances.toArray()\n",
        "\n",
        "# Check if lengths match\n",
        "if len(feature_columns) != len(feature_importances):\n",
        "    print(\"Warning: Mismatch in the number of features\")\n",
        "\n",
        "# Map feature names to their importance scores\n",
        "importances = {feature: score for feature, score in zip(feature_columns, feature_importances)}\n",
        "\n",
        "# Sort and print feature importances\n",
        "sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
        "for feature, importance in sorted_importances:\n",
        "    print(f\"{feature}: {importance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the dataset size\n",
        "dataset_size = combined_df_transformed.count()\n",
        "\n",
        "# Define a simpler schema without TrainingDateTime and DatasetSize\n",
        "schema = StructType([\n",
        "    StructField(\"Feature\", StringType(), True),\n",
        "    StructField(\"Importance\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Prepare rows with Python floats for the importances\n",
        "rows = [(feature, float(importance)) for feature, importance in sorted_importances]\n",
        "\n",
        "# Create a DataFrame from rows\n",
        "importances_df = spark.createDataFrame(rows, schema)\n",
        "\n",
        "# Add TrainingDateTime and DatasetSize columns\n",
        "importances_df = importances_df.withColumn(\"TrainingDateTime\", current_timestamp())\\\n",
        "                               .withColumn(\"DatasetSize\", lit(dataset_size))\n",
        "\n",
        "# Specify the path to save the feature importances\n",
        "output_path = gold_path + \"ml_fact_AttendanceRisk_explanation/\"\n",
        "\n",
        "# Save the DataFrame as a Parquet file, overwriting any existing data\n",
        "importances_df.write.mode(\"overwrite\").parquet(output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(df_for_prediction)\n",
        "#predictions.printSchema()\n",
        "# Write to Parquet file\n",
        "predictions_path = gold_path + \"ml_fact_AttendanceRisk/\"\n",
        "predictions.write.mode(\"overwrite\").parquet(predictions_path)\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
